<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="https://www.ink8s.com">
  <title>kubernetes 资源对象 | INK8S</title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="description" content="kubernets 资源对象 k8s 基础应用 Pod 生命周期 Pod 资源限制 deployment daemonset job、cronjob service ingress configmap secret statefulset 授权与认证（RBAC） 准入控制（ResoucesQuta、LimitRange）">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes 资源对象">
<meta property="og:url" content="https://www.ink8s.com/2025/07/17/k8s-%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/index.html">
<meta property="og:site_name" content="INK8S">
<meta property="og:description" content="kubernets 资源对象 k8s 基础应用 Pod 生命周期 Pod 资源限制 deployment daemonset job、cronjob service ingress configmap secret statefulset 授权与认证（RBAC） 准入控制（ResoucesQuta、LimitRange）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-338.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-339.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-340.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-341.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-342.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-343.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-352.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-353.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-354.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-355.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-356.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-357.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-358.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-359.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-360.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-361.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-362.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-363.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-364.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-365.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-366.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-367.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-368.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-369.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-370.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-371.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-372.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-373.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-374.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-375.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-377.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-379.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-380.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-381.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-383.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-384.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-386.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-387.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-389.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-390.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-391.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-392.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-393.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-394.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-395.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-396.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-397.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-398.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-399.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-400.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-401.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-402.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-403.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-406.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-404.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-405.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-407.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-409.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-410.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-411.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-412.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-421.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-422.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-423.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-424.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-425.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-426.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-427.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-428.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-429.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-430.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-435.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-433.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-434.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-436.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-437.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-438.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-439.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-440.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-441.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-442.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-443.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-444.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-446.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-445.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-447.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-448.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-450.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-451.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-452.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-453.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-454.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-468.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-469.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-455.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-456.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-458.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-459.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-460.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-461.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-462.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-463.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-464.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-465.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-466.png">
<meta property="og:image" content="https://www.ink8s.com/images/pasted-467.png">
<meta property="article:published_time" content="2025-07-17T13:36:49.000Z">
<meta property="article:modified_time" content="2025-07-24T08:02:37.302Z">
<meta property="article:author" content="weixj@inadm.com">
<meta property="article:tag" content="Linux-k8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.ink8s.com/images/pasted-338.png">
  
    <link rel="alternative" href="/atom.xml" title="INK8S" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  
    <link rel="apple-touch-icon" href="/apple-touch-icon-180x180.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.a5fda8.css">
  <style type="text/css">
    
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

  
  

<script>
  (function () {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();

</script>



  

</head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      


<div class="overlay" style="background: #4d4d4d;background: url('/img/biubiubiu.gif') no-repeat ;"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/head.jpg" class="js-avatar">
		</a>
		<hgroup>
			<h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/" >主页</a></li>
			
				<li><a href="/tags/Linux-storage" >Linux_存储类</a></li>
			
				<li><a href="/tags/Linux-base" >Linux_系统类</a></li>
			
				<li><a href="/tags/Server-tool" >Linux_工具类</a></li>
			
				<li><a href="/tags/Linux-svc" >Linux_服务部署</a></li>
			
				<li><a href="/tags/Linux-db" >Linux_数据库类</a></li>
			
				<li><a href="/tags/Linux-k8s" >Linux_Kubernetes</a></li>
			
				<li><a href="/tags/Windows-svc" >Windows_服务配置</a></li>
			
			</ul>
		</nav>
		<nav class="header-smart-menu">
			
				
					<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
				
			
				
					<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
				
			
		</nav>
                <nav>
                   <a>总文章数 37</a>
                </nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" href="#" title="GitHub" ><i class="icon-github"></i></a>
				
					<a class="gitee" href="#" title="gitee" ><i class="icon-gitee"></i></a>
				
					<a class="jianshu" href="#" title="jianshu" ><i class="icon-jianshu"></i></a>
				
					<a class="cnblog" href="#" title="cnblog" ><i class="icon-cnblog"></i></a>
				
			</div>
		
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
      
      <!-- a class="forkMe" style="position:absolute;z-index:999;top:0;right:0.5em;" 
        href="https://github.com/JoeyBling/hexo-theme-yilia-plus" target="_blank">
        <img src="/img/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a> -->
      
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<a href="/">
					<img src="/img/head.jpg" class="js-avatar">
				</a>
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="GitHub"><i class="icon-github"></i></a>
			        
						<a class="gitee" target="_blank" href="#" title="gitee"><i class="icon-gitee"></i></a>
			        
						<a class="jianshu" target="_blank" href="#" title="jianshu"><i class="icon-jianshu"></i></a>
			        
						<a class="cnblog" target="_blank" href="#" title="cnblog"><i class="icon-cnblog"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 12.5%"><a href="/">主页</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Linux-storage">Linux_存储类</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Linux-base">Linux_系统类</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Server-tool">Linux_工具类</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Linux-svc">Linux_服务部署</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Linux-db">Linux_数据库类</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Linux-k8s">Linux_Kubernetes</a></li>
		        
					<li style="width: 12.5%"><a href="/tags/Windows-svc">Windows_服务配置</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1"
              class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-k8s-资源对象" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
  
  
    <h1 class="article-title" itemprop="name">
      kubernetes 资源对象
    </h1>
  


  
  
        


<a href="/2025/07/17/k8s-%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/" class="archive-article-date">
        <time datetime="2025-07-17T13:36:49.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2025-07-17</time>
</a>


  
  
  </header>
  
  <div class="article-entry" itemprop="articleBody">
    
    <ul>
<li><strong>kubernets 资源对象</strong><ul>
<li>k8s 基础应用</li>
<li>Pod 生命周期</li>
<li>Pod 资源限制</li>
<li>deployment</li>
<li>daemonset</li>
<li>job、cronjob</li>
<li>service</li>
<li>ingress</li>
<li>configmap</li>
<li>secret</li>
<li>statefulset</li>
<li>授权与认证（RBAC）</li>
<li>准入控制（ResoucesQuta、LimitRange）</li>
</ul>
</li>
</ul>
<span id="more"></span>

<h1 id="1-0-k8s-基础应用"><a href="#1-0-k8s-基础应用" class="headerlink" title="1.0 k8s 基础应用"></a>1.0 <strong>k8s 基础应用</strong></h1><ul>
<li><a href="https://www.ink8s.com/2025/07/14/kubernets-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">kubernetes 集群搭建参考</a></li>
</ul>
<h2 id="1-1-deploy"><a href="#1-1-deploy" class="headerlink" title="1.1 deploy"></a>1.1 <strong>deploy</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 空跑 deploy def-www 资源来获取模版</span><br><span class="line"><span class="comment"># kubectl create deploy default-www --image=nginx --replicas=3 --dry-run=client -o yaml</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-338.png" alt="upload successful"></p>
<h2 id="1-2-pod-镜像拉取策略"><a href="#1-2-pod-镜像拉取策略" class="headerlink" title="1.2 pod 镜像拉取策略"></a>1.2 <strong>pod 镜像拉取策略</strong></h2><ul>
<li>imagePullPolicy: 容器的镜像拉取策略:<ul>
<li>IfNotPresent: 本地有镜像则使用本地镜像，本地不存在则拉取镜像 (默认)</li>
<li>Always: 每次都会尝试拉取镜像</li>
<li>Never: 永不拉取，如果镜像已经在本地，kubelet 会尝试使用镜像启动容器；否则,会启动失败</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: demo-container                           <span class="comment"># 容器的名称</span></span><br><span class="line">        image: harbor.inadm.com/demo/demoapp:v1.1</span><br><span class="line">        imagePullPolicy: Always                        <span class="comment"># 指定容器镜像拉取策略</span></span><br><span class="line"></span><br><span class="line">// 默认镜像拉取策略，当控制器向API服务器提交新的Pod时，集群会满足特定条件时设置 imagePullPolicy 字段</span><br><span class="line">    如果你省略了 imagePullPolicy 字段，并且容器镜像的标签是 :latest ，imagePullPolicy 会自动设置为 Always</span><br><span class="line">    如果你省略了 imagePullPolicy 字段，并且没有指定容器镜像的标签，imagePullPolicy 会自动设置为 Always</span><br><span class="line">    如果你省略了 imagePullPolicy 字段，并且为容器镜像指定了非 :latest 的标签，imagePullPolicy 就会自动设置为 IfNotPresent</span><br></pre></td></tr></table></figure>

<h2 id="1-3-获取私有仓库镜像"><a href="#1-3-获取私有仓库镜像" class="headerlink" title="1.3 获取私有仓库镜像"></a>1.3 <strong>获取私有仓库镜像</strong></h2><h3 id="1-3-1-创建-secret-认证"><a href="#1-3-1-创建-secret-认证" class="headerlink" title="1.3.1 创建 secret 认证"></a>1.3.1 <strong>创建 secret 认证</strong></h3><ul>
<li>ImagePullSecrets 拉取私有仓库中的镜像</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 将 harbor 的认证信息保存到 k8s secret 资源中</span><br><span class="line"><span class="comment"># kubectl create secret docker-registry harbor-registry-auth \</span></span><br><span class="line">  --docker-server=harbor.inadm.com \</span><br><span class="line">  --docker-username=admin \</span><br><span class="line">  --docker-password=ink8s.comA \</span><br><span class="line">  --docker-email=user01@inadm.com                            <span class="comment"># 可选</span></span><br><span class="line">// 查看 harbor-registry-auth 内容</span><br><span class="line"><span class="comment"># kubectl get secret harbor-registry-auth --output=yaml</span></span><br><span class="line">// harbor-registry-auth 数据转换为可读格式</span><br><span class="line"><span class="comment"># kubectl get secret harbor-registry-auth --output=&quot;jsonpath=&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-339.png" alt="upload successful"></p>
<h3 id="1-3-2-yaml-部署验证"><a href="#1-3-2-yaml-部署验证" class="headerlink" title="1.3.2 yaml 部署验证"></a>1.3.2 <strong>yaml 部署验证</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:                                    <span class="comment"># 指定 harbor 认证凭据</span></span><br><span class="line">      - name: harbor-registry-auth</span><br><span class="line">      containers:</span><br><span class="line">      - image: harbor.inadc.com/java-demo/java-demo:v1</span><br><span class="line">        name: java-demo</span><br><span class="line">        resources: &#123;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="1-4-传递环境变量"><a href="#1-4-传递环境变量" class="headerlink" title="1.4 传递环境变量"></a>1.4 <strong>传递环境变量</strong></h2><ul>
<li>使用 env 控制容器环境变量</li>
<li><a target="_blank" rel="noopener" href="https://hub.docker.com/_/mysql">mysql docker 镜像下载及变量等信息</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat mysql-demo-env.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-demo-env</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mysql-demo-env</span><br><span class="line">    image: mysql:5.7.37</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">      value: <span class="string">&quot;inadm.com&quot;</span></span><br><span class="line">    - name: MYSQL_DATABASE</span><br><span class="line">      value: <span class="string">&quot;testdb&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pod -o wide</span></span><br><span class="line"><span class="comment"># mysql -h 10.1.135.133 -uroot -pinadm.com</span></span><br><span class="line">MySQL [(none)]&gt; </span><br></pre></td></tr></table></figure>

<h2 id="1-5-自定义容器命令与参数"><a href="#1-5-自定义容器命令与参数" class="headerlink" title="1.5 自定义容器命令与参数"></a>1.5 <strong>自定义容器命令与参数</strong></h2><ul>
<li>command: 为容器指定启动命令，会覆盖容器启动的默认命令，不指定则默认容器的启动命令</li>
<li>args: 为命令提供选项或参数</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-busybox-command.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-busybox-command</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: busy</span><br><span class="line">    image: busybox</span><br><span class="line">    <span class="comment"># command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 999999&quot; ]</span></span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">    - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">    - <span class="string">&quot;-c&quot;</span></span><br><span class="line">    - <span class="string">&quot;sleep 999999&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-busybox-command.yaml </span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">// 重点在于颜色 args</span><br><span class="line"><span class="comment"># cat redis-leader.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-leader</span><br><span class="line">  labels:</span><br><span class="line">    role: leader</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: redis-leader</span><br><span class="line">    image: redis</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f redis-leader.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat redis-slave.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-slave</span><br><span class="line">  labels:</span><br><span class="line">    role: slave</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: redis-slave</span><br><span class="line">    image: redis</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;redis-server&quot;</span>]</span><br><span class="line">    args:</span><br><span class="line">    - --port 6380</span><br><span class="line">    - --slaveof  10.1.135.134 6379		<span class="comment"># redis_leader_pod_ip</span></span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 6380</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f redis-slave.yaml</span></span><br><span class="line"><span class="comment"># redis-cli -h redis_leader_pod_ip info</span></span><br></pre></td></tr></table></figure>
<h1 id="2-0-Pod-生命周期"><a href="#2-0-Pod-生命周期" class="headerlink" title="2.0 Pod 生命周期"></a>2.0 <strong>Pod 生命周期</strong></h1><h2 id="2-1-初始化容器"><a href="#2-1-初始化容器" class="headerlink" title="2.1 初始化容器"></a>2.1 <strong>初始化容器</strong></h2><ul>
<li>init container 是用来做初始化工作的容器，可以有一个或多个，如果多个按照定义的顺序依次执行，只有所有的执行完成后，主容器才启动，由于一个 pod 里的存储卷是共享的，所以 init container 里产生的数据可以被主容器使用到，但它仅仅是在启动时，在主容器启动前执行，做初始化工作。如果 pod 的 init 容器失败，kubernetes 会不断重启该 pod，直到 init 容器成功为止。如果 pod 对应的 restartPolicy 值为 Never， kubernetes 不会重新启动 pod。</li>
<li>应用场景:<ul>
<li>app 容器依赖 mysql 的数据交互，所以可以启动一个初始化容器检查 mysql 服务是否正常，如果正常则启动主容器</li>
<li>在启动主容器之前，使用初始化容器对系统内核参数进行调优，然后共享给主容器使用</li>
<li>获取集群成员节点地址，为主容器生成对应配置信息，这样主容器启动后，可以通过配置信息加入集群环境</li>
</ul>
</li>
</ul>
<h3 id="2-1-1-场景1-端口检查"><a href="#2-1-1-场景1-端口检查" class="headerlink" title="2.1.1 场景1-端口检查"></a>2.1.1 <strong>场景1-端口检查</strong></h3><ul>
<li>编写 yaml，使用初始化容器对 mysql 端口进行检查，如果存活则运行 pod，否则就一直重启尝试</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat init-check-mysql.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: init-check-mysql</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-registry-auth</span><br><span class="line">  initContainers:</span><br><span class="line">  - name: check-mysql</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;nc -z MYSQL_POD_IP 3306&quot;</span>]</span><br><span class="line">    securityContext:                                    <span class="comment"># 特权模式运行容器，否则无法修改内核参数</span></span><br><span class="line">      privileged: <span class="literal">true</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: app-mysql</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line"></span><br><span class="line">// 这里的 <span class="string">&quot;MYSQL_POD_IP&quot;</span> 是使用的 1.4 mysql-demo-env 容器 IP 地址,主要是为了演示初始化容器场景的实现</span><br><span class="line"><span class="comment"># kubectl apply -f init-check-mysql.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="2-1-2-场景2-内核参数优化"><a href="#2-1-2-场景2-内核参数优化" class="headerlink" title="2.1.2 场景2-内核参数优化"></a>2.1.2 <strong>场景2-内核参数优化</strong></h3><ul>
<li>使用初始化容器对内核参数进行优化</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat init-sysctl-nginx.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: init-sysctl-nginx</span><br><span class="line">spec:</span><br><span class="line">  initContainers:</span><br><span class="line">  - name: set-sysctl</span><br><span class="line">    image: alpine:3.13</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sysctl -w net.core.somaxconn=32769; sysctl -w net.ipv4.ip_local_port_range=&#x27;1024 65000&#x27;&quot;</span>]</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: <span class="literal">true</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: app-sysctl</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f init-sysctl-nginx.yaml </span></span><br><span class="line">// 检查验证</span><br><span class="line"><span class="comment"># kubectl exec -it init-sysctl-nginx -c &quot;app-sysctl&quot; -- /bin/bash -c &quot;cat /proc/sys/net/core/somaxconn&quot;		# 值为 &quot;32769&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="2-2-钩子函数"><a href="#2-2-钩子函数" class="headerlink" title="2.2 钩子函数"></a>2.2 <strong>钩子函数</strong></h2><ul>
<li>钩子函数用来监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数<ul>
<li>当一个容器启动后，kubernetes 将立即执行 postStart 事件关联的动作</li>
<li>在容器被终结之前，kubernetes 将立即执行 preStop 事件关联的动作</li>
</ul>
</li>
<li>两种钩子<ul>
<li>postStart: 容器创建后立即执行，由于是异步执行，它无法保证在容器之前运行。如果失败，容器会杀死，并根据 RestartPolicy 决定是否重启</li>
<li>preStop: 在容器终止前执行。用于: 释放占用的资源、清理注册过的信息、优雅的关闭进程。在其完成之前会阻塞删除容器的操作，默认等待时间为 30s，可以通过 terminationGracePeriodSeconds 宽限时间</li>
</ul>
</li>
</ul>
<h3 id="2-2-1-钩子示例"><a href="#2-2-1-钩子示例" class="headerlink" title="2.2.1 钩子示例"></a>2.2.1 <strong>钩子示例</strong></h3><figure class="highlight bash"><figcaption><span>postStart 示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 通过 postStart 设定端口重定向，将请求本机的 8080 调度到本机 80 端口</span><br><span class="line">lifecycle:</span><br><span class="line">  postStart:</span><br><span class="line">    <span class="built_in">exec</span>:</span><br><span class="line">      <span class="built_in">command</span>:</span><br><span class="line">      - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">      - <span class="string">&quot;-c&quot;</span></span><br><span class="line">      - <span class="string">&quot;iptables -t nat -A REROUTING -p tcp --dport 8080 -j REDIRECT --to-ports 80&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><figcaption><span>preStop 示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// runner 主要用来编译打包提高 CI 效率。启动后会注册到 gitlab 上，后续不需要可以删除 Pod，然后清理注册信息</span><br><span class="line">// 通过 preStop 清理 runner 注册信息</span><br><span class="line">lifecycle:</span><br><span class="line">  preStop:</span><br><span class="line">    <span class="built_in">exec</span>:</span><br><span class="line">      <span class="built_in">command</span>:</span><br><span class="line">      - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">      - <span class="string">&quot;-c&quot;</span></span><br><span class="line">      - <span class="string">&quot;/usr/bin/gitlab-runner unregister -n <span class="variable">$RUNNER_NAME</span>&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-钩子场景1"><a href="#2-2-2-钩子场景1" class="headerlink" title="2.2.2 钩子场景1"></a>2.2.2 <strong>钩子场景1</strong></h3><ul>
<li>postStart 命令在容器的 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html 自定义一段内容 </li>
<li>preStop 负责优雅地终止 nginx 服务</li>
<li>terminationGracePeriodSeconds 宽限期，如果超过宽限期 pod 还没有终止，则会由 SIGKLL 强制关闭信号介入。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat 1-pod-postStart.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: lifecycle-nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: lifecycle-demo-container</span><br><span class="line">    image: nginx</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        <span class="built_in">exec</span>:</span><br><span class="line">          <span class="built_in">command</span>:</span><br><span class="line">          - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">          - <span class="string">&quot;-c&quot;</span></span><br><span class="line">          - <span class="string">&quot;echo Hello from the postStart handler... &gt; /usr/share/nginx/html/index.html&quot;</span></span><br><span class="line">      preStop:</span><br><span class="line">        <span class="built_in">exec</span>:</span><br><span class="line">          <span class="built_in">command</span>:</span><br><span class="line">          - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">          - <span class="string">&quot;-c&quot;</span></span><br><span class="line">          - <span class="string">&quot;sleep 10; nginx -s quit&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f 1-pod-postStart.yaml</span></span><br><span class="line"><span class="comment"># kubectl exec -it lifecycle-nginx -- cat /usr/share/nginx/html/index.html	# postStart 演示结果</span></span><br><span class="line">// preStop 演示,在删除时使用 describe 可查看到 Killing 在 11s 时完全删除完毕,如图</span><br><span class="line">// <span class="built_in">sleep</span> 10s + pod 自身1s 时间</span><br><span class="line"><span class="comment"># kubectl describe pod lifecycle-nginx </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-340.png" alt="upload successful"></p>
<h3 id="2-2-3-钩子场景2"><a href="#2-2-3-钩子场景2" class="headerlink" title="2.2.3 钩子场景2"></a>2.2.3 <strong>钩子场景2</strong></h3><ul>
<li>postStart 命令负责将默认页面拷贝至 &#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps</li>
<li>preStop 负责给容器发送 SIGTERM 信号，从而优雅地终止 tomcat 服务</li>
<li>terminationGracePeriodSeconds 宽限期，如果超过宽限期 pod 还没有终止，则会由 SIGKILL 强制关闭信号介入</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat 2-pod-postStart.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-lifecycle-2</span><br><span class="line">  labels:</span><br><span class="line">    app: tomcat</span><br><span class="line">spec:</span><br><span class="line">  terminationGracePeriodSeconds: 120                        <span class="comment"># 超过宽限期,优雅关闭未执行完情况下，强制SIGKILL</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: tomcat-web</span><br><span class="line">    image: tomcat</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        <span class="built_in">exec</span>:</span><br><span class="line">          <span class="built_in">command</span>:</span><br><span class="line">          - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">          - <span class="string">&quot;-c&quot;</span></span><br><span class="line">          - <span class="string">&quot;cp -rp /usr/local/tomcat/webapps.dist/* /usr/local/tomcat/webapps/&quot;</span></span><br><span class="line">      preStop:</span><br><span class="line">        <span class="built_in">exec</span>:</span><br><span class="line">          <span class="built_in">command</span>:</span><br><span class="line">          - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">          - <span class="string">&quot;-c&quot;</span></span><br><span class="line">          - <span class="string">&quot;/usr/local/tomcat/bin/shutdown.sh&quot;</span>                <span class="comment"># 优雅关闭</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f 2-pod-postStart.yaml </span></span><br><span class="line"><span class="comment"># kubectl expose pod pod-lifecycle-2 --port=9999 --target-port=8080 --type=NodePort</span></span><br><span class="line">// 浏览器访问: http://10.16.41.24:30597/</span><br><span class="line">// 下图表示,删除 Pod 时,的优雅关闭</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-341.png" alt="upload successful"></p>
<h2 id="2-3-检测探针"><a href="#2-3-检测探针" class="headerlink" title="2.3 检测探针"></a>2.3 <strong>检测探针</strong></h2><ul>
<li>为何需要探针: 当容器进程运行时如出现异常退出，k8s 则会认为容器发生故障，会尝试进行重启解决该问题。但有些情况是发生了故障，但进程没有退出。比如访问 web 服务时出现 500 错误，可能是系统超载，也可能资源死锁，但 nginx 进程并没有异常退出，在这种情况下重启容器是最佳方法，如何来实现检测</li>
<li>kubernetes 使用探针(probe)方式来保障容器正常运行，实现零宕机。它通过 kubelet 定期对容器进行健康检查 (exec 、tpc、http)，当探针检测到容器状态异常时，会通过重启策略来进行重启或重建完成修复。修复后继续进行探针检测，以确保容器稳定运行</li>
</ul>
<h3 id="2-3-1-探针检测类型"><a href="#2-3-1-探针检测类型" class="headerlink" title="2.3.1 探针检测类型"></a>2.3.1 <strong>探针检测类型</strong></h3><ul>
<li>针对运行中的容器，kubelet 可以选择一下三种探针来探测容器的状态<ul>
<li><strong>startupProbe</strong> 启动探针: 用于检测容器中的应用是否已经正常启动。如果使用了<strong>启动探针</strong>，则所有其它探针都会被禁用，需要等待启动探针检测成功之后才可以执行。如果<strong>启动探针</strong>探测失败，则 kubelet 会将容器杀死，而容器以其<strong>重启策略</strong>进行重启。如果容器没有提供启动探测，则默认状态为 Success。</li>
<li><strong>livenessProbe</strong> 存活探针: 用于检测容器是否存活，如果存活探测检测失败， kubelet 会杀死容器，然后根据容器重启策略，决定是否重启该容器。如果容器不提供存活探针，则默认状态为 Success</li>
<li><strong>readinessProbe</strong> 就绪探针: 指容器是否准备好接收网络请求，如果<strong>就绪探测</strong>失败，则将容器设定为未就绪状态，然后将其从负载均衡列表移除，这样就不会有请求会调度到该 Pod 上。如果容器不提供就绪探针，则默认状态为 Success</li>
</ul>
</li>
</ul>
<h3 id="2-3-2-探针检查机制"><a href="#2-3-2-探针检查机制" class="headerlink" title="2.3.2 探针检查机制"></a>2.3.2 <strong>探针检查机制</strong></h3><ul>
<li>使用探针来检查容器 [只能任选其一]<ul>
<li>exec: 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功</li>
<li>httpGet: 对指定的 IP、端口，执行 HTTP 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的</li>
<li>tcpSocket: 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的</li>
</ul>
</li>
<li>每次探测都获得以下三种结果之一<ul>
<li>Success：容器通过诊断</li>
<li>Failure：容器未通过诊断，可能会触发重启操作</li>
<li>Unknown：诊断失败，因此不会采取任何行动</li>
</ul>
</li>
</ul>
<h3 id="2-3-3-startupProbe"><a href="#2-3-3-startupProbe" class="headerlink" title="2.3.3 startupProbe"></a>2.3.3 <strong>startupProbe</strong></h3><ul>
<li>有时会有一些应用在启动时需要较长的初始化时间。若要不影响对死锁做出快速响应的探测，设置<strong>存活探测</strong>参数是要技巧的。技巧就是使用相同的命令来设置<strong>启动探测</strong>，针对 HTTP 或 TCP 检测，可以通过将 failureThreshold * periodSeconds 参数设置为足够长的时间来应对最糟糕情况下的启动时间</li>
</ul>
<h4 id="2-3-3-1-exec"><a href="#2-3-3-1-exec" class="headerlink" title="2.3.3.1 exec"></a>2.3.3.1 <strong>exec</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create ns inadm</span></span><br><span class="line"><span class="comment"># kubectl create secret docker-registry harbor-auth \</span></span><br><span class="line"> --docker-server=harbor.inadm.com \</span><br><span class="line"> --docker-username=admin \</span><br><span class="line"> --docker-password=ink8s.comA \</span><br><span class="line"> -n inadm</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-startupProbe-exec.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-startupprobe</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-startupprobe-exec</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    startupProbe:</span><br><span class="line">      <span class="built_in">exec</span>:</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">        - <span class="string">&quot;-c&quot;</span></span><br><span class="line">        - <span class="string">&quot;netstat -nlput | grep 80&quot;</span></span><br><span class="line">      initialDelaySeconds: 10                    <span class="comment"># 容器启动多久后开始探测，默认0</span></span><br><span class="line">      periodSeconds: 5                           <span class="comment"># 探测频率</span></span><br><span class="line">      timeoutSeconds: 10                         <span class="comment"># 探测超时时长</span></span><br><span class="line">      successThreshold: 1                        <span class="comment"># 成功多少次则为成功</span></span><br><span class="line">      failureThreshold: 3                        <span class="comment"># 失败多少次则为失败</span></span><br><span class="line"></span><br><span class="line">// 第一次探测识别多久会重启 (对所有探测有用)</span><br><span class="line">initialDelaySeconds + (periodSeconds + timeoutSeconds) * failureThreshold</span><br><span class="line">10 + (5 + 10) * 3 = 55s</span><br><span class="line">// 程序启动完成后: 此时不需要计入 initialDelaySeconds (对 livenessprobe、readiness 有用)</span><br><span class="line">(periodSeconds + timeoutSeconds) * failureThreshold</span><br><span class="line">// 应用将会有(failureThreshold * periodSeconds = 15s)的时间来完成启动过程</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># kubectl apply -f pod-startupProbe-exec.yaml </span></span><br></pre></td></tr></table></figure>

<h4 id="2-3-3-2-httpGet"><a href="#2-3-3-2-httpGet" class="headerlink" title="2.3.3.2 httpGet"></a>2.3.3.2 <strong>httpGet</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-startprobe-httpget.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-startupprobe-httpget</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-startupprobe-httpget</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    startupProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: /</span><br><span class="line">        port: 80</span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-startprobe-httpget.yaml </span></span><br></pre></td></tr></table></figure>

<h4 id="2-3-3-3-tcpSocket"><a href="#2-3-3-3-tcpSocket" class="headerlink" title="2.3.3.3 tcpSocket"></a>2.3.3.3 <strong>tcpSocket</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-startupprobe-tcp.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-startupprobe-tcp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-startupprobe-tcp</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    startupProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 80</span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-startupprobe-tcp.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-4-livenessProbe"><a href="#2-3-4-livenessProbe" class="headerlink" title="2.3.4 livenessProbe"></a>2.3.4 <strong>livenessProbe</strong></h3><h4 id="2-3-4-1-exec"><a href="#2-3-4-1-exec" class="headerlink" title="2.3.4.1 exec"></a>2.3.4.1 <strong>exec</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-livenessprobe-exe.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-livenessprobe-exec</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-livenessprobe-exec</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    livenessProbe:</span><br><span class="line">      <span class="built_in">exec</span>:</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">        - <span class="string">&quot;-c&quot;</span></span><br><span class="line">        - <span class="string">&#x27;[ &quot;$(curl -s 127.0.0.1/aaaalivez)&quot; == &quot;OK&quot; ]&#x27;</span></span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-livenessprobe-exe.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -n inadm -it pod-livenessprobe-exec -- curl -s 127.0.0.1/livez</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-342.png" alt="upload successful"></p>
<h4 id="2-3-4-1-httpGet"><a href="#2-3-4-1-httpGet" class="headerlink" title="2.3.4.1 httpGet"></a>2.3.4.1 <strong>httpGet</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-livenessprobe-httpget.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-livenessprobe-httpget</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-livenessprobe-httpget</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        port: 80</span><br><span class="line">        scheme: HTTP</span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-livenessprobe-httpget.yaml </span></span><br><span class="line">// 检测存活状态检测效果，可以手动将 /livez 接口响应内容修改为任意值</span><br><span class="line"><span class="comment"># kubectl exec -n inadm -it pod-livenessprobe-httpget -- curl -s -X POST -d &#x27;livez=error&#x27; 127.0.0.1/livez</span></span><br><span class="line"></span><br><span class="line">// 程序启动完成后: 此时不需要计入 initialDelaySeconds (对 livenessprobe、readiness 有用)</span><br><span class="line">(periodSeconds + timeoutSeconds) * failureThreshold</span><br><span class="line">( 5 + 10 ) * 3 = 45s</span><br><span class="line"></span><br><span class="line">// 等待3个检测周期结束，容器会因健康检测失败而被重启，重启后 /livez 响应内容会被重置为 ok，后续存活状态检测不会出现错误</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-343.png" alt="upload successful"></p>
<h4 id="2-3-4-2-tcpSocket"><a href="#2-3-4-2-tcpSocket" class="headerlink" title="2.3.4.2 tcpSocket"></a>2.3.4.2 <strong>tcpSocket</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-livenessprobe-tcp.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-livenessprobe-tcp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-livenessprobe-tcp</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 80</span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-livenessprobe-tcp.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-5-readinessProbe"><a href="#2-3-5-readinessProbe" class="headerlink" title="2.3.5 readinessProbe"></a>2.3.5 <strong>readinessProbe</strong></h3><ul>
<li>有些程序启动需要加载配置或数据，甚至有些程序需要运行预热的过程，需要一定时间。所以需要避免 Pod 启动成功后立即让其处理客户端请求，而应该让其初始化完成后转为就绪状态，在对外提供服务。此类应用就需要使用  readinessProbe 探针</li>
</ul>
<h4 id="2-3-5-1-exec"><a href="#2-3-5-1-exec" class="headerlink" title="2.3.5.1 exec"></a>2.3.5.1 <strong>exec</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-readinessprobe-exec.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-readiness-exec</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: readiness</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-readiness-exec</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    readinessProbe:</span><br><span class="line">      <span class="built_in">exec</span>:</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">          - <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">          - <span class="string">&quot;-c&quot;</span></span><br><span class="line">          - <span class="string">&#x27;[ &quot;$(curl -s 127.0.0.1/readyz)&quot; == &quot;OK&quot; ]&#x27;</span></span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-readinessprobe-exec.yaml </span></span><br></pre></td></tr></table></figure>

<h4 id="2-3-5-2-httpGet"><a href="#2-3-5-2-httpGet" class="headerlink" title="2.3.5.2 httpGet"></a>2.3.5.2 <strong>httpGet</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// 需要有 svc 服务配合，此处简略不加入 svc 验证</span><br><span class="line"><span class="comment"># cat pod-readiness-httpget.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-readiness-httpget</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: readiness</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-readiness-httpget</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    readinessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">        port: 80</span><br><span class="line">        scheme: HTTP</span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-readiness-httpget.yaml </span></span><br><span class="line"></span><br><span class="line">// 将接口修改为 error ，状态会变为未就绪，节点就会被提除出 endpoints</span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-readiness-httpget -- curl -s -X POST -d &#x27;readyz=error&#x27; 127.0.0.1/ready</span></span><br></pre></td></tr></table></figure>

<h4 id="2-3-5-3-tcpSocket"><a href="#2-3-5-3-tcpSocket" class="headerlink" title="2.3.5.3 tcpSocket"></a>2.3.5.3 <strong>tcpSocket</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-readinessprobe-tcp.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-readiness-tcp</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: readiness</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-readiness-tcp</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">    readinessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 80</span><br><span class="line">      initialDelaySeconds: 10</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">      timeoutSeconds: 10</span><br><span class="line">      successThreshold: 1</span><br><span class="line">      failureThreshold: 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-readinessprobe-tcp.yaml </span></span><br></pre></td></tr></table></figure>

<h1 id="3-0-Pod-资源限制"><a href="#3-0-Pod-资源限制" class="headerlink" title="3.0 Pod 资源限制"></a>3.0 <strong>Pod 资源限制</strong></h1><ul>
<li>kubernetes 通过 Requests 和 Limits 字段来实现对 Pod 的资源限制<ul>
<li>Requests: 启动 Pod 时申请分配的资源大小 (Pod在调度时requests比较重要)</li>
<li>Limits: 限制 Pod 运行时最大可用的资源大小 (Pod在运行时limits比较重要)</li>
</ul>
</li>
<li>CPU 限制单位<ul>
<li>1 核 CPU 等于 1000 毫核，当定义容器为 0.5 时，所能用到的 CPU 资源是 1 核心 CPU 的一半，对于 CPU 资源单位，表达式 0.1 等价于表达式 100m，可以看做 100 millicpu</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1C = 1000 millicpu (1 coer = 1000m)</span><br><span class="line">0.5C = 500 millicpu (0.5 coer = 500m)</span><br></pre></td></tr></table></figure>

<ul>
<li>内存分配单位: 内存的基本单位是字节数(Bytes)，也可以加上国际单位，十进制的 E、P、T、G、M、K、m，或二进的 Ei、Pi、Ti、Gi、Mi、Ki</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1MB = 1000KB = 1000000 Bytes</span><br><span class="line">1Mi = 1024KB = 1048576 btyes</span><br></pre></td></tr></table></figure>

<h2 id="3-1-cpu-资源限制"><a href="#3-1-cpu-资源限制" class="headerlink" title="3.1 cpu 资源限制"></a>3.1 <strong>cpu 资源限制</strong></h2><ul>
<li><a href="https://www.ink8s.com/2025/07/14/kubernets-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">需要安装 metrics-server</a></li>
</ul>
<h3 id="3-1-1-cpu-请求和限制"><a href="#3-1-1-cpu-请求和限制" class="headerlink" title="3.1.1 cpu 请求和限制"></a>3.1.1 <strong>cpu 请求和限制</strong></h3><ul>
<li>创建一个 Pod，容器将请求 0.5 个 CPU，最多限制使用 1 个 CPU</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cpu-requests-limits.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: cpu-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: cpu-demo-ctr</span><br><span class="line">    image: vish/stress</span><br><span class="line">    args:</span><br><span class="line">    - -cpus                        <span class="comment"># 模拟压力测试，可以使用到2个cpu。但下面有设定限制，最多只能使用到1个</span></span><br><span class="line">    - <span class="string">&quot;2&quot;</span></span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;500m&quot;</span>                <span class="comment"># 限制启动pod时最多申请0.5核的cpu</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;1000m&quot;</span>               <span class="comment"># 限制pod最多使用1核cpu</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f cpu-requests-limits.yaml </span></span><br><span class="line"><span class="comment"># kubectl top pod cpu-demo -n inadm</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-352.png" alt="upload successful"></p>
<h3 id="3-1-2-超过节点-cpu-请求"><a href="#3-1-2-超过节点-cpu-请求" class="headerlink" title="3.1.2 超过节点 cpu 请求"></a>3.1.2 <strong>超过节点 cpu 请求</strong></h3><ul>
<li>创建一个pod，设置该pod中容器的请求为100核，这个值会大于集群中的任何一个节点</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-requests-limints.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  namespace: inadm</span><br><span class="line">  name: cpu-demo-2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: cpu-demo-ctr-2</span><br><span class="line">    image: vish/stress</span><br><span class="line">    args:</span><br><span class="line">    - -cpus</span><br><span class="line">    - <span class="string">&quot;2&quot;</span></span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;100&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;100&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-requests-limints.yaml </span></span><br><span class="line"><span class="comment"># kubectl describe pod -n inadm cpu-demo-2 </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-353.png" alt="upload successful"></p>
<h3 id="3-1-2-不指定cpu-limits"><a href="#3-1-2-不指定cpu-limits" class="headerlink" title="3.1.2 不指定cpu limits"></a>3.1.2 <strong>不指定cpu limits</strong></h3><ul>
<li>如果没有为容器指定cpu限制，那么容器在可以使用的cpu资源是没有上限。因而可以使用所在节点上所有的可用cpu资源，这样会造成一个pod占用大量cpu，可能会导致其它pod的正常运行，从而造成业务的不稳定</li>
<li>kubernetes中，可以通过 <strong>LimitRange</strong> 自动为容器设定，所使用的CPU资源和内存资源最大最小值</li>
</ul>
<h2 id="3-2-内存资源限制"><a href="#3-2-内存资源限制" class="headerlink" title="3.2 内存资源限制"></a>3.2 <strong>内存资源限制</strong></h2><h3 id="3-2-1-mem-请求和限制"><a href="#3-2-1-mem-请求和限制" class="headerlink" title="3.2.1 mem 请求和限制"></a>3.2.1 <strong>mem 请求和限制</strong></h3><ul>
<li>创建一个Pod，容器将会请求 100MiB 内存，并且内存会被限制在 200MiB 以内</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-mem-requests-limits.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-demo-1</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mem-demo-1</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;stress&quot;</span>]</span><br><span class="line">    args: [<span class="string">&quot;--vm&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;--vm-bytes&quot;</span>, <span class="string">&quot;150M&quot;</span>, <span class="string">&quot;--vm-hang&quot;</span>, <span class="string">&quot;1&quot;</span>]</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">       memory: <span class="string">&quot;100Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        memory: <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">    resources:</span><br><span class="line">      limits:</span><br><span class="line">        memory: 200Mi</span><br><span class="line">      requests:</span><br><span class="line">        memory: 100Mi</span><br><span class="line"></span><br><span class="line">// 获取该Pod指标数据，输出结果显示Pod正在使用的内存约为 150MiB，这大于Pod请求的100Mib，但又在Pod限制的200MiB之内</span><br><span class="line"><span class="comment"># kubectl apply -f pod-mem-requests-limits.yaml </span></span><br><span class="line"><span class="comment"># kubectl top pod -n inadm mem-demo-1 </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-354.png" alt="upload successful"></p>
<h3 id="3-2-2-超过容器mem限制的应用"><a href="#3-2-2-超过容器mem限制的应用" class="headerlink" title="3.2.2 超过容器mem限制的应用"></a>3.2.2 <strong>超过容器mem限制的应用</strong></h3><ul>
<li>当节点有足够的内存时，容器可以使用其请求的内存。但是，容器不允许使用超过其限制的内存。如果容器的内存超过其限制，该容器会成为被终止的候选容器。如果容器继续消耗超过其限制的内存，则终止容器。如果终止的容器可以被重启，则 kubelet 会重新启动它。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个pod，其拥有一个容器，该容器的内存请求为 100MiB，内存限制为200MiB，尝试分配超出其限制的内存</span><br><span class="line"><span class="comment"># cat pod-mem-requests-limist.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-demo-3</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mem-demo-3</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;stress&quot;</span>]                                            <span class="comment"># 容器会尝试分配 250MiB 内存，这远高于 100MiB 的限制</span></span><br><span class="line">    args: [<span class="string">&quot;--vm&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;--vm-bytes&quot;</span>, <span class="string">&quot;250M&quot;</span>, <span class="string">&quot;--vm-hang&quot;</span>, <span class="string">&quot;1&quot;</span>]    <span class="comment"># 模拟1个进程产生 250M 内存</span></span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">       memory: <span class="string">&quot;100Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        memory: <span class="string">&quot;200Mi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-mem-requests-limist.yaml </span></span><br><span class="line"><span class="comment"># kubectl get pod -n inadm mem-demo-3 -o yaml | grep reason        # 内存溢出 00M</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-355.png" alt="upload successful"></p>
<h3 id="3-2-3-超过节点mem分配"><a href="#3-2-3-超过节点mem分配" class="headerlink" title="3.2.3 超过节点mem分配"></a>3.2.3 <strong>超过节点mem分配</strong></h3><ul>
<li>pod 的调度基于请求。只有当节点拥有足够满足Pod内存请求的内存时，才会将pod调度至节点上运行</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个pod，其拥有一个请求 100GiB内存的容器，这应该超过了集群中任何一台节点所拥有的内存</span><br><span class="line"><span class="comment"># cat pod-requests-limits.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-demo-4</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mem-demo-4</span><br><span class="line">    image: polinux/stress</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;stress&quot;</span>]</span><br><span class="line">    args: [<span class="string">&quot;--vm&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;--vm-bytes&quot;</span>, <span class="string">&quot;250M&quot;</span>, <span class="string">&quot;--vm-hang&quot;</span>, <span class="string">&quot;1&quot;</span>]</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">       memory: <span class="string">&quot;100Gi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        memory: <span class="string">&quot;200Gi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-requests-limits.yaml </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-356.png" alt="upload successful"></p>
<h3 id="3-2-4-未指定内存限制"><a href="#3-2-4-未指定内存限制" class="headerlink" title="3.2.4 未指定内存限制"></a>3.2.4 <strong>未指定内存限制</strong></h3><ul>
<li>如果没有为容器指定内存限制，容器可无限制地使用其所在节点的所有可用内存，进而可能导致该节点调用 OOM Killer。此外，如果发生 OOM Kill，没有配置资源限制的容器将被杀掉的可行性更大</li>
<li>不用担心，在 kubernetes中，可以通过 LimitRange 自动为其容器设定，所使用的内存资源最大最小值</li>
</ul>
<h2 id="3-3-Qos-服务质量"><a href="#3-3-Qos-服务质量" class="headerlink" title="3.3 Qos 服务质量"></a>3.3 <strong>Qos 服务质量</strong></h2><ul>
<li><p>Qos [服务质量等级] or [服务质量保证]，是作用在pod上的一个配置，当kubernetes创建一个pod时，它就会给这个pod分配一个Qos等级</p>
</li>
<li><p>在k8s环境中，k8s允许节点的pod过载使用资源，这意味着节点无法同时满足所有pod以过载方式运行。因此在内存资源紧缺情况下，k8s需要借助pod对象的服务质量和优先级等完成判定，进而挑选对应的pod杀死。k8s根据pod的requests和limits属性，把pod对象归类为 BestEffort、BurStable、Guaranteed</p>
</li>
<li><p>Qos 类别:</p>
<ul>
<li>Guaranteed: pod对象为每个容器都设置了cpu资源需求和资源限制，且两者的值相同；还同时为每个容器设置了内存需求和内存限制，并且两者的值相同。这类pod对象具有最高级别服务质量</li>
<li>Burstable: 至少有一个容器设置了cpu或者内存资源requests属性，但不满足Guaranteed，这类pod具有中级服务质量</li>
<li>BestEffort: 没有为任何容器设置requests和limits属性，这类pod对象服务质量是最低级别</li>
</ul>
</li>
<li><p>当k8s集群内存资源紧缺，优先杀死 BestEffort 类别的容器，因为系统不为该资源提供任何服务保证，但此类资源最大的好处就是能够尽可能使用资源</p>
</li>
<li><p>当k8s集群内存资源紧缺，优先杀死 BestEffort 类别的容器，因为系统不为该资源提供任何服务保证，但此类资源最大的好处就是能够尽可能使用资源</p>
</li>
<li><p>对于 Guaranteed 类别容器拥有最高优先级，他们不会被杀死，除非其它内存资源需求超限，或者 OOM 时没有其它更低优先级的 pod 对象存在，才会干掉 Guaranteed 类容器</p>
</li>
</ul>
<h3 id="3-3-1-创建Guaranteed的pod"><a href="#3-3-1-创建Guaranteed的pod" class="headerlink" title="3.3.1 创建Guaranteed的pod"></a>3.3.1 <strong>创建Guaranteed的pod</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// pod中的每个容器都必须指定内存请求和内存限制，且pod中每个容器内存请求必须等于内存限制</span><br><span class="line">// pod中的每个容器都必须指定cpu请求和cpu限制，且pod中每个容器cpu请求必须等于内存限制</span><br><span class="line">// 创建一个pod，容器设置了内存请求和内存限制，值都是200MiB。容器设置了CPU请求和CPU限制，值都是700 milliCPU</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-qos-guaranteed.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-qos-guaranteed</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-qos-guaranteed</span><br><span class="line">    image: nginx</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;700m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;700m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;200Mi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-qos-guaranteed.yaml </span></span><br><span class="line"><span class="comment"># kubectl describe pod -n inadm pod-qos-guaranteed </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-357.png" alt="upload successful"></p>
<h3 id="3-3-2-创建Burstables的pod"><a href="#3-3-2-创建Burstables的pod" class="headerlink" title="3.3.2 创建Burstables的pod"></a>3.3.2 <strong>创建Burstables的pod</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">// 如果满足下面条件，将会指定pod的Qos类型为Burstable:</span><br><span class="line">    1. pod不符合Guaranteed Qos类型标准</span><br><span class="line">    2. pod中至少一个容器指定了，内存或cpu的请求或限制</span><br><span class="line">// 创建一个pod，容器设置了内存请求100MiB，以及内存限制200MiB</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-qos-burstable.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-qos-burstable</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-qos-burstable</span><br><span class="line">    image: nginx</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: <span class="string">&quot;100Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        memory: <span class="string">&quot;200Mi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-qos-burstable.yaml </span></span><br><span class="line"><span class="comment"># kubectl describe pod -n inadm pod-qos-burstable</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-358.png" alt="upload successful"></p>
<h3 id="3-3-2-创建BestEffort的pod"><a href="#3-3-2-创建BestEffort的pod" class="headerlink" title="3.3.2 创建BestEffort的pod"></a>3.3.2 <strong>创建BestEffort的pod</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 对于Qos类为 BestEffort 的 pod，pod 中的容器必须没有设置内存和cpu限制或请求</span><br><span class="line">// 创建一个pod，容器没有设置内存和cpu限制或请求</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-qos-besteffort.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-qos-besteffort</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-qos-besteffort</span><br><span class="line">    image: nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-qos-besteffort.yaml </span></span><br><span class="line"><span class="comment"># kubectl describe pod -n inadm pod-qos-besteffort </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-359.png" alt="upload successful"></p>
<h3 id="3-3-3-创建多个容器pod"><a href="#3-3-3-创建多个容器pod" class="headerlink" title="3.3.3 创建多个容器pod"></a>3.3.3 <strong>创建多个容器pod</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 创建一个pod，一个容器指定了内存请求 200MiB。另外一个容器没有指定任何请求和限制。此pod满足burstable Qos类的标准。但它不满足 Guaranteed Qos 类标准，因为它的一个容器没有内存请求</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-qos-mutil.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-qos-mutil</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-qos-mutil</span><br><span class="line">    image: nginx</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: <span class="string">&quot;100Mi&quot;</span></span><br><span class="line">  - name: redis-qos</span><br><span class="line">    image: redis</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-qos-mutil.yaml </span></span><br><span class="line"><span class="comment"># kubectl describe pod -n inadm pod-qos-mutil | grep QoS</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-360.png" alt="upload successful"></p>
<h2 id="3-4-downward-api"><a href="#3-4-downward-api" class="headerlink" title="3.4 downward api"></a>3.4 <strong>downward api</strong></h2><ul>
<li>什么是 DownwardAPI: DownwardAPI 可以让容器获取pod的相关元数据信息，比如pod名称、pod_ip、pod的资源限制等，获取后通过env、volume的方式将相关的环境信息注入到容器中，从而让容器通过这些信息，来设定容器的运行特性<ul>
<li>例如: nginx 进程根据节点的cpu核心数量自动设定要启动的worker进程数</li>
<li>例如: jvm虚拟根据pod的内存资源限制，来设定对应容器的堆内存大小</li>
<li>例如: 获取pod名称，以pod名称注册到某个服务，当pod结束后，调用prestop清理对应名称的注册信息</li>
</ul>
</li>
</ul>
<h3 id="3-4-1-可注入元数据信息"><a href="#3-4-1-可注入元数据信息" class="headerlink" title="3.4.1 可注入元数据信息"></a>3.4.1 <strong>可注入元数据信息</strong></h3><ul>
<li>使用 pod.spec.containers.env.valueFrom.fieldRef 可以注入的字段有:<ul>
<li>metadata.name: Pod 对象的名称</li>
<li>metadata.namespace: Pod 对象隶属于名称空间</li>
<li>metadata.uid: Pod 对象的 UID</li>
<li>metadata.labels “KEY”: 获取 Label 指定 KEY 对应的值</li>
<li>metadata.annotations “KEY”: 获取 Annotations 对应 KEY 的值</li>
<li>status.podIP: Pod 对象的IP地址</li>
<li>status.hostIP: 节点IP</li>
<li>status.nodeName: 节点名称</li>
<li>spec.serviceAcconuntName: Pod 对象使用的 ServiceAccount 资源名称</li>
</ul>
</li>
<li>使用 pod.spec.containers.env.valueFrom.resourceFieldRef 可以注入的字段有:<ul>
<li>requests.cpu</li>
<li>requests.memory</li>
<li>limits.cpu</li>
<li>limits.memory</li>
</ul>
</li>
</ul>
<h3 id="3-4-2-环境变量注入元数据"><a href="#3-4-2-环境变量注入元数据" class="headerlink" title="3.4.2 环境变量注入元数据"></a>3.4.2 <strong>环境变量注入元数据</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">// 创建Pod容器，将Pod相关环境变量注入到容器中，比如(pod名称、命名空间、标签、以及cpu、内存的请求和限制)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-downward.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: pod-app  </span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-downward</span><br><span class="line">    image: nginx</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">&quot;/bin/bash&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;env&quot;</span>]</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;200m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;32Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;200m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: K8S_POD_NAME</span><br><span class="line">      valueFrom:</span><br><span class="line">        fieldRef:</span><br><span class="line">          fieldPath: metadata.name</span><br><span class="line">    - name: K8S_POD_NAMESPACE</span><br><span class="line">      valueFrom:</span><br><span class="line">        fieldRef:</span><br><span class="line">          fieldPath: metadata.namespace</span><br><span class="line">    - name: K8S_POD_LABELS</span><br><span class="line">      valueFrom:</span><br><span class="line">        fieldRef:</span><br><span class="line">          fieldPath: metadata.labels[<span class="string">&#x27;app&#x27;</span>]</span><br><span class="line">    - name: K8S_POD_CPU_LIMITS</span><br><span class="line">      valueFrom:</span><br><span class="line">        resourceFieldRef:</span><br><span class="line">          resource: limits.cpu</span><br><span class="line">    - name: K8S_POD_MEMORY_REQUESTS</span><br><span class="line">      valueFrom:</span><br><span class="line">        resourceFieldRef:</span><br><span class="line">          resource: requests.memory</span><br><span class="line">          divisor: 1Mi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-downward.yaml</span></span><br><span class="line"><span class="comment"># kubectl logs -n inadm pod-downward | grep ^K8S</span></span><br><span class="line">K8S_POD_NAME=pod-downward</span><br><span class="line">K8S_POD_NAMESPACE=inadm</span><br><span class="line">K8S_POD_MEMORY_REQUESTS=32          <span class="comment"># 单位为兆，默认字节</span></span><br><span class="line">K8S_POD_CPU_LIMITS=1                <span class="comment"># 200毫核，不足1核，则进行取整</span></span><br><span class="line">K8S_POD_LABELS=pod-app</span><br></pre></td></tr></table></figure>

<h3 id="3-4-3-存储卷注入元数据"><a href="#3-4-3-存储卷注入元数据" class="headerlink" title="3.4.3 存储卷注入元数据"></a>3.4.3 <strong>存储卷注入元数据</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-downward-api.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward-volumes</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: pod-app</span><br><span class="line">    zone: shenzhen</span><br><span class="line">    role: backend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-downward-volumes</span><br><span class="line">    image: nginx</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;200m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;32Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;200m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">    volumeMounts:                <span class="comment"># 将环境变量挂载到 /etc/info目录中，注入一条元数据都会产生一个文件</span></span><br><span class="line">    - name: podinfo</span><br><span class="line">      mountPath: /etc/info</span><br><span class="line"></span><br><span class="line">  volumes:</span><br><span class="line">  - name: podinfo</span><br><span class="line">    downwardAPI:</span><br><span class="line">      items:</span><br><span class="line">      - path: pod_name</span><br><span class="line">        fieldRef:</span><br><span class="line">          fieldPath: metadata.name</span><br><span class="line">      - path: pod_labels</span><br><span class="line">        fieldRef:</span><br><span class="line">          fieldPath: metadata.labels</span><br><span class="line">      - path: pod_namespace</span><br><span class="line">        fieldRef:</span><br><span class="line">          fieldPath: metadata.namespace</span><br><span class="line">      - path: mem_limits</span><br><span class="line">        resourceFieldRef:</span><br><span class="line">          resource: limits.memory</span><br><span class="line">          containerName: pod-downward-volumes</span><br><span class="line">          divisor: 1Mi</span><br><span class="line">      - path: mem_requests</span><br><span class="line">        resourceFieldRef:</span><br><span class="line">          resource: requests.memory</span><br><span class="line">          containerName: pod-downward-volumes</span><br><span class="line">          divisor: 1Mi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-downward-api.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-downward-volumes -- /bin/bash</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-361.png" alt="upload successful"></p>
<h3 id="3-4-4-为注册服务注入pod名称"><a href="#3-4-4-为注册服务注入pod名称" class="headerlink" title="3.4.4 为注册服务注入pod名称"></a>3.4.4 <strong>为注册服务注入pod名称</strong></h3><ul>
<li>使用 DownwardAPI 实现注册与卸载</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-register.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward-register</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tools</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tools</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: register</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">        - <span class="string">&quot;-c&quot;</span></span><br><span class="line">        - |</span><br><span class="line">          mysql -h 10.16.41.152 -uroot -pink8s.com -e <span class="string">&quot;create database <span class="variable">$&#123;POD_NAME//-/_&#125;</span>&quot;</span></span><br><span class="line">          <span class="built_in">sleep</span> 999999</span><br><span class="line">        <span class="built_in">env</span>:</span><br><span class="line">        - name: POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">        lifecycle:</span><br><span class="line">          preStop:</span><br><span class="line">            <span class="built_in">exec</span>:</span><br><span class="line">              <span class="built_in">command</span>:</span><br><span class="line">              - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">              - <span class="string">&quot;-c&quot;</span></span><br><span class="line">              - mysql -h 10.16.41.152 -uroot -pink8s.com -e <span class="string">&quot;drop database <span class="variable">$&#123;POD_NAME//-/_&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"><span class="comment"># 引入外部 MySQL 需要用到自定义 endpoint</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-mysql-external</span><br><span class="line">  namespace: inadm</span><br><span class="line">subsets:</span><br><span class="line">  - addresses:</span><br><span class="line">    - ip: 10.16.41.152</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 3306</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-mysql-external</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: ClusterIP</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 3306</span><br><span class="line">    targetPort: 3306</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-register.yaml </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL 赋予相应权限</span></span><br><span class="line">mysql&gt; grant all privileges on *.* to <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;10.16.41.%&#x27;</span> identified by <span class="string">&#x27;ink8s.com&#x27;</span> with grant option;</span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line"></span><br><span class="line">// 应用前后，数据库的创建演示效果。仅仅演示效果，生产或测试不可操作</span><br><span class="line"><span class="comment"># kubectl delete -f pod-register.yaml 	# 登录MySQL后 apply 和 delete 之后前后对比</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-362.png" alt="upload successful"></p>
<h3 id="3-4-5-为Tomcat注入对内存限制"><a href="#3-4-5-为Tomcat注入对内存限制" class="headerlink" title="3.4.5 为Tomcat注入对内存限制"></a>3.4.5 <strong>为Tomcat注入对内存限制</strong></h3><ul>
<li><p>默认Tomcat应用会使用Pod所在的物理节点内存，初始堆内存为1&#x2F;64，最大堆内存为1&#x2F;4</p>
</li>
<li><p>1、运行一个默认的Tomcat，检查初始jvm堆内存大小</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-tomcat-1.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward-tomcat</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-downward-tomcat</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tomcat:9.0.63</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line"></span><br><span class="line">// 查看默认tomcat的内存堆大小</span><br><span class="line"><span class="comment"># kubectl apply -f pod-tomcat-1.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-downward-tomcat -- /bin/bash</span></span><br><span class="line">root@pod-downward-tomcat:/usr/local/tomcat<span class="comment"># jps                    # 查看进程 [进程为 1]</span></span><br><span class="line">root@pod-downward-tomcat:/usr/local/tomcat<span class="comment"># jinfo 1                # 查看进程为 1 的 jvm 堆内存大小</span></span><br><span class="line">    InitialHeapSize=262144000    初始内存: 250M</span><br><span class="line">    MaxHeapSize=4164943872       最大可用: 3972M</span><br><span class="line">// 上面 2 个值，是通过本机节点的 1/64 1/4 获得的</span><br><span class="line">root@pod-downward-tomcat:/usr/local/tomcat<span class="comment"># free -m</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-363.png" alt="upload successful"></p>
<ul>
<li>2.对Tomcat设定资源限制，看看这个资源限制对Tomcat分配内存有没有影响</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-tomcat-2.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward-tomcat-2</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-downward-tomcat-2</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tomcat:9.0.63</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 100Mi</span><br><span class="line">      limits:</span><br><span class="line">        memory: 200Mi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-tomcat-2.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-downward-tomcat-2 -- /bin/bash</span></span><br><span class="line">root@pod-downward-tomcat:/usr/local/tomcat<span class="comment"># jps</span></span><br><span class="line">root@pod-downward-tomcat:/usr/local/tomcat<span class="comment"># jinfo 1</span></span><br><span class="line">    InitialHeapSize=8388608 字节            8M</span><br><span class="line">    MaxHeapSize=104857600 字节              100M</span><br><span class="line">// 说明资源限制对堆内存jvm的分配是有一定影响，不是使用的物理节点进行的分配的了</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-364.png" alt="upload successful"></p>
<ul>
<li>3.手动为Tomcat指定堆内存，500M，对pod限制100M</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-tomcat-3.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward-tomcat-3</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-downward-tomcat-3</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tomcat:9.0.63</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: JAVA_OPTS</span><br><span class="line">      value: -server -Xms500m -Xmx500m -XX:+UseConcMarkSweepGC</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 100Mi</span><br><span class="line">      limits:</span><br><span class="line">        memory: 100Mi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-tomcat-3.yaml </span></span><br><span class="line">// 让jvm使用500Mi，但实际最大是能使用到100Mi。使用 ab 压力测试验证，ab 过程中,pod就会被频繁 <span class="built_in">kill</span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-downward-tomcat-3 -- /bin/bash</span></span><br><span class="line">root@pod-downward-tomcat:/usr/local/tomcat<span class="comment"># jinfo 1                    			# jvm 内存堆是设定的 500</span></span><br><span class="line"></span><br><span class="line">// 压测</span><br><span class="line">[root@szbwx-ops-sly-k8s-master01 ~]<span class="comment"># kubectl top -n inadm pod pod-downward-tomcat    # 已经使用 75Mi</span></span><br><span class="line">pod-downward-tomcat   2m           75Mi</span><br><span class="line"><span class="comment"># watch -n 1 kubectl top -n inadm pod pod-downward-tomcat-3                            # 只要内存上到100Mi，pod 就会被 kill</span></span><br><span class="line"><span class="comment"># ab -n 10000 -c 10 http://10.1.58.224:8080/                                     	# 将 pod 压挂了</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-365.png" alt="upload successful"></p>
<ul>
<li>4.将 request limits 值，传递给 jvm 内存设定</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-tomcat-4.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downward-tomcat-4</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-downward-tomcat-4</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tomcat:9.0.63</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: JAVA_OPTS</span><br><span class="line">      value: -server -Xms<span class="variable">$&#123;JVM_XMS&#125;</span> -Xmx<span class="variable">$&#123;JVM_XMX&#125;</span> -XX:+UseConcMarkSweepGC</span><br><span class="line">    - name: JVM_XMS</span><br><span class="line">      valueFrom:</span><br><span class="line">        resourceFieldRef:</span><br><span class="line">          resource: requests.memory</span><br><span class="line">    - name: JVM_XMX</span><br><span class="line">      valueFrom:</span><br><span class="line">        resourceFieldRef:</span><br><span class="line">          resource: limits.memory</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 200Mi</span><br><span class="line">      limits:</span><br><span class="line">        memory: 500Mi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-tomcat-4.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-downward-tomcat-4 -- /bin/bas</span></span><br><span class="line">root@pod-downward-tomcat-4:/usr/local/tomcat<span class="comment"># jinfo 1</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-366.png" alt="upload successful"></p>
<h1 id="4-0-deployment"><a href="#4-0-deployment" class="headerlink" title="4.0 deployment"></a>4.0 <strong>deployment</strong></h1><h2 id="4-1-replicaset"><a href="#4-1-replicaset" class="headerlink" title="4.1 replicaset"></a>4.1 <strong>replicaset</strong></h2><ul>
<li>ReplicaSet 控制器包含了3个基本组成部分<ul>
<li>selector 标签选择器: 匹配并关联 Pod 对象，并加入控制器的管理中</li>
<li>replicas 期望的副本数: 期望在集群中所运行的 Pod 对象数量</li>
<li>template Pod 模板: 定义 Pod 规范，相当于把一个 Pod 的描述以模板形式嵌入到了 ReplicaSet</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// RS 控制器更新缺点：更新时会统一 <span class="built_in">kill</span> 掉 Pod 然后统一拉起</span><br><span class="line"><span class="comment"># cat rs-deploy.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: ReplicaSet</span><br><span class="line">metadata:</span><br><span class="line">  name: rs-deploy               <span class="comment"># RS 控制器名称</span></span><br><span class="line">  namespace: inadm </span><br><span class="line">spec:                           <span class="comment"># 定义 RS 规范</span></span><br><span class="line">  replicas: 3                   <span class="comment"># 定义 Pod 副本数量，默认为 1 个</span></span><br><span class="line">  selector:                      <span class="comment"># RS 通过 Selector 选择一组 Pod，并进行管理</span></span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:                     <span class="comment"># 定义Pod模板，当RS需要创建Pod时就通过模板来创建</span></span><br><span class="line">    metadata:</span><br><span class="line">      labels:                   <span class="comment"># 所有通过该模板运行的Pod都有 app=nginx 这个标签</span></span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: rs-container</span><br><span class="line">        image: nginx:1.16</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f rs-deploy.yaml </span></span><br></pre></td></tr></table></figure>

<h2 id="4-2-deploy"><a href="#4-2-deploy" class="headerlink" title="4.2 deploy"></a>4.2 <strong>deploy</strong></h2><ul>
<li>Deployment 控制器包含 3 个基本组成部分<ul>
<li>selector 标签选择器: 匹配并关联 Pod 对象，并加入控制器的管理中</li>
<li>replicas 期望的副本数: 期望在集群中所运行的 Pod 对象数量</li>
<li>template Pod 模板: 定义 Pod 规范，相当于把一个 Pod 的描述以模板形式嵌入到了 ReplicaSet</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-demo.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: deploy-demo               <span class="comment"># deploy 控制器名称</span></span><br><span class="line">  namespace: inadm </span><br><span class="line">spec:</span><br><span class="line">  replicas: 3                     <span class="comment"># 定义 Deploy 控制 Pod 副本数量</span></span><br><span class="line">  minReadySeconds: 10             <span class="comment"># Pod 就绪后，多少秒内任一容器无崩溃视为 &quot;就绪&quot;，默认 0s</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  template:                       <span class="comment"># 定义Pod模板，申明 Pod 名称，使用惊喜那个、拥有哪些标签等</span></span><br><span class="line">    metadata:</span><br><span class="line">      labels:                     <span class="comment"># Pod标签</span></span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: deploy-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-demo.yaml </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-367.png" alt="upload successful"></p>
<ul>
<li>检查集群 Deployment<ul>
<li>NAME：列出了集群中的 Deployment 的名称</li>
<li>READY：显示应用程序的可用副本数。显示的模式是 “就绪个数”&#x2F;“期望个数”</li>
<li>UP-TO-DATE：显示为了达到期望状态已经更新的副本数</li>
<li>AVAILABLE：显示应用程序可供用户使用的副本数</li>
<li>AGE：显示应用程序运行的时间</li>
</ul>
</li>
</ul>
<h2 id="4-3-svc"><a href="#4-3-svc" class="headerlink" title="4.3 svc"></a>4.3 <strong>svc</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-svc.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: demo-svc</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-svc.yaml </span></span><br><span class="line"><span class="comment"># curl POD_IP/version</span></span><br><span class="line"><span class="comment"># curl SVC_IP/version            # 重复操作会轮询</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-368.png" alt="upload successful"></p>
<h2 id="4-4-hpa"><a href="#4-4-hpa" class="headerlink" title="4.4 hpa"></a>4.4 <strong>hpa</strong></h2><ul>
<li>什么是 HPA：k8s 实现 Pod 的扩容缩容需要通过手动来实现，但线上业务情况复杂，依赖于纯手动方式不太现实。所以希望系统能自动感知 Pod 的压力来完成扩缩容，比如: 当 Pod 的 CPU 达到 50% 则扩容，当 Pod 的 CPU 低于 50% 自动缩容。为此 k8s 提供了一个资源对象 HPA（horizontal-pod-autoscaler），专用来实现 Pod 的水平自动扩缩容。HPA 通过监控分析一些控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量</li>
<li>自动扩缩容算法(基于 metricserver):<ul>
<li>当前指标：当前 Pod 已经达到百分之多少的压力</li>
<li>期望指标：当前 Pod 达到期望的指标百分比时就要进行扩容</li>
<li>例如：当前副本数 1，当前指标值 200%，期望指标值 50%，则副本数为 1 * （200% &#x2F; 50%） &#x3D; 4</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat hpa-deploy.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: hpa-deploy</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: hpa-deploy</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: hpa-deploy</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: php-apache-hpa</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/hpa-example:latest</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 200m</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 500m</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: hpa-deploy</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    run: hpa-deploy</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"><span class="comment"># 创建 hpa，设定 cpu 超过 50%，则触发自动创建 Pod 副本；</span></span><br><span class="line"><span class="comment"># 可以采用 -o yaml 方式获取文本方式编辑</span></span><br><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: hpa-deploy</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: hpa-deploy</span><br><span class="line">  targetCPUUtilizationPercentage: 50</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f hpa-deploy.yaml </span></span><br><span class="line">// 命令创建扩缩容: kubectl autoscale deploy hpa-deploy --min=1 max=10 --cpu-percent=50</span><br><span class="line"><span class="comment"># kubectl get pod -l run=hpa-deploy -n inadm</span></span><br><span class="line"><span class="comment"># kubectl get hpa -n inadm</span></span><br><span class="line"><span class="comment"># while sleep 0.01; do curl http://SVC_IP; done                # 模拟增加负载模拟；如暂停压力测试之后，5分钟后才会恢复设定的初始 rs 值状态</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-369.png" alt="upload successful"></p>
<p><img src="/images/pasted-370.png" alt="upload successful"></p>
<h2 id="4-5-recreate-重建策略"><a href="#4-5-recreate-重建策略" class="headerlink" title="4.5 recreate 重建策略"></a>4.5 <strong>recreate 重建策略</strong></h2><ul>
<li>什么是 Recreate：当更新策略设定为 Recreate，在更新镜像时，它会先杀死正在运行的 Pod，等彻底杀死后，重新创建的 RS，然后启动对应的 Pod，在更新过程中，会造成服务一段时间停止提供服务<ul>
<li>1.同时杀死所有旧版 Pod，此时 Pod 无法正常对外提供服务</li>
<li>2.创建新的 RS，启动新的 Pod</li>
<li>3.等待 Pod就绪，对外提供服务</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-recreate.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: recreate-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: Recreate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"> apiVersion: v1</span><br><span class="line"> kind: Service</span><br><span class="line"> metadata:</span><br><span class="line">   name: svc-demo</span><br><span class="line">   namespace: inadm</span><br><span class="line"> spec:</span><br><span class="line">   selector:</span><br><span class="line">     app: demoapp</span><br><span class="line">   ports:</span><br><span class="line">   - name: http</span><br><span class="line">     port: 80</span><br><span class="line">     targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-recreate.yaml</span></span><br><span class="line"><span class="comment"># watch -n1 kubectl get pod -n inadm</span></span><br><span class="line"><span class="comment"># while sleep 1; do curl http://SVC_IP/version; done</span></span><br><span class="line">// 当将 image 的版本更新到 demoapp:v1.1 后观察如下图变化，会统一将 Pod Kill 掉，然后将统一拉起新的 Pod</span><br><span class="line">// 通常只有当应用的新旧版本不兼容(例如依赖的后端数据格式不同且无法兼容)时才会使用 Recreate 重建策略</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl edit deploy -n inadm recreate-demo</span></span><br><span class="line">...</span><br><span class="line">      - image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-371.png" alt="upload successful"></p>
<p><img src="/images/pasted-372.png" alt="upload successful"></p>
<p><img src="/images/pasted-373.png" alt="upload successful"></p>
<h2 id="4-6-rollingupdate"><a href="#4-6-rollingupdate" class="headerlink" title="4.6 rollingupdate"></a>4.6 <strong>rollingupdate</strong></h2><ul>
<li>什么是滚动更新：一次仅更新一批 Pod，当更新的 Pod 就绪后，在更新另一批，知道全部更新完成为止；该策略实现了不间断服务的目标，在更新过程中可能会出现不同的应用版本并存且同时提供服务的情况<ul>
<li>1.创建新的 ReplicaSet，然后根据新的镜像运行新的 Pod</li>
<li>2.删除旧 Pod，启动新 Pod，新 Pod 就绪后，继续删除旧 Pod，启动新 Pod</li>
<li>3.持续第二步过程，直到所有 Pod 都被更新成功</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-rollingupdate.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: recreate-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate               <span class="comment"># 滚动更新，默认策略</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-rollingupdate.yaml </span></span><br><span class="line"><span class="comment"># watch -n1 kubectl get pod -n inadm</span></span><br><span class="line"><span class="comment"># while sleep 0.5; do curl http://10.96.184.87/version; done	# 不会中断</span></span><br><span class="line">// 当将 image 的版本更新到 demoapp:v1.1 后观察如下图变化，会滚动将单个 Pod Kill 掉，再将单个 Pod 拉起，依此类推知道完全启动成功</span><br><span class="line"><span class="comment"># kubectl edit deploy -n inadm recreate-demo</span></span><br><span class="line">...</span><br><span class="line">      - image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.1</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-374.png" alt="upload successful"></p>
<h2 id="4-7-rollout"><a href="#4-7-rollout" class="headerlink" title="4.7 rollout"></a>4.7 <strong>rollout</strong></h2><ul>
<li>可以通过修改 revisionHistoryLimit 调整保留的数量，默认 10 条</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl rollout history deploy -n inadm recreate-demo                        # 查看历史版本</span></span><br><span class="line"><span class="comment"># kubectl rollout history deploy -n inadm recreate-demo --revision=3           # 查看对应 version 对应的具体信息</span></span><br><span class="line">// 通过命令回滚到特定版本 <span class="string">&quot;1.0&quot;</span> 版本</span><br><span class="line"><span class="comment"># kubectl rollout undo deploy -n inadm recreate-demo --to-revision=3</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-375.png" alt="upload successful"></p>
<p><img src="/images/pasted-377.png" alt="upload successful"></p>
<h2 id="4-8-deploy-更新策略"><a href="#4-8-deploy-更新策略" class="headerlink" title="4.8 deploy 更新策略"></a>4.8 <strong>deploy 更新策略</strong></h2><ul>
<li>Deplolyment 会在 .spec.strategy.type&#x3D;RollingUpdate 时，采取滚动更新方式更新 Pod。可以指定 maxUnavailable 和 maxSurge 来控制滚动更新过程。<ul>
<li>maxSurge 最大可用 Pod<ul>
<li>用来指定可以创建超出期望 Pod 个数的 Pod 数量。可以是数字，可以是百分比。此字段默认值 25%</li>
<li>例如：当此值为 20% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod 的总数不超过所需 Pod 总数的 120%。一旦旧 Pod 被杀死，新的 ReplicaSet 可以进一步扩容，同时确保更新期间任何时候运行的 Pod 总数最多为所需 Pod 总数的 120%。计算公式: 10+(10*20%)&#x3D;12</li>
</ul>
</li>
<li>maxUnavailable 最大可用 Pod<ul>
<li>用来指定更新过程中不可用的 Pod 的个数上线。</li>
<li>例如：当此值设置为 20% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的 70%。新 Pod 准备就绪后，继续缩容旧的 ReplicaSet，然后对新 ReplicaSet 扩容，确保更新期间可用的 Pod 总数任何时候都是所需的 Pod 个数的 70%。计算公式: 10-(10*20%)&#x3D;8</li>
</ul>
</li>
</ul>
</li>
<li>maxSurge 和 maxUnavailable 两个属性协同工作，可以组合定义出 3 种不同策略完成多批次应用更新<ul>
<li>先增新，后减旧：将 maxSurge 设置为 30%，将 maxUnavailable 的值设为 0</li>
<li>先减旧，后增新：将 maxUnavailable 设置为 30%，将 maxSurge 值设置为 0</li>
<li>同时增减，将 maxSurge 和 maxUnavailable 分别设置为 20%，期望是 12 Pod，至少就绪 8 个 Pod</li>
</ul>
</li>
</ul>
<h3 id="4-8-1-maxSurge"><a href="#4-8-1-maxSurge" class="headerlink" title="4.8.1 maxSurge"></a>4.8.1 <strong>maxSurge</strong></h3><ul>
<li>指定升级期间存在的总 Pod 对象数量最多可超出期望值的个数，可以是 0，也可以是整数，也可以是一个百分比</li>
<li>例如：副本数为 10，maxSurge 属性为 2，则表示 Pod 对象总数不能超过 12 个。计算公式: 10+(10*20%)&#x3D;12</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-maxsurge.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: maxsurge-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 10</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 20%</span><br><span class="line">      maxUnavailable: 0</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-maxsurge.yaml </span></span><br><span class="line"><span class="comment"># kubectl get rs -n inadm</span></span><br><span class="line">// 当将 image 的版本更新到 demoapp:v1.1 后观察如下图变化</span><br><span class="line">// DESIRED 为 12 个</span><br><span class="line"><span class="comment"># kubectl edit -n inadm deploy maxsurge-demo</span></span><br><span class="line">...</span><br><span class="line">      - image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.1</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-379.png" alt="upload successful"></p>
<h3 id="4-8-2-maxUnavailable"><a href="#4-8-2-maxUnavailable" class="headerlink" title="4.8.2 maxUnavailable"></a>4.8.2 <strong>maxUnavailable</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-maxUnavailable.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: maxunavailable-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 10</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 0</span><br><span class="line">      maxUnavailable: 20%</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-maxUnavailable.yaml </span></span><br><span class="line"></span><br><span class="line">// 当将 image 的版本更新到 demoapp:v1.1 后观察如下图变化</span><br><span class="line">// READY 为 8 个</span><br><span class="line"><span class="comment"># kubectl edit -n inadm deploy maxunavailable-demo</span></span><br><span class="line">...</span><br><span class="line">      - image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-380.png" alt="upload successful"></p>
<h3 id="4-8-3-surge-和-unavailable"><a href="#4-8-3-surge-和-unavailable" class="headerlink" title="4.8.3 surge 和 unavailable"></a>4.8.3 <strong>surge 和 unavailable</strong></h3><ul>
<li>同时设定 maxsurge 和 maxunavailable</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-max.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: max-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 10</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 20%</span><br><span class="line">      maxUnavailable: 20%</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-max.yaml </span></span><br><span class="line"></span><br><span class="line">// 当将 image 的版本更新到 demoapp:v1.1 后观察如下图变化</span><br><span class="line">DESIRED: 12 个</span><br><span class="line">READY: 8 个</span><br><span class="line"><span class="comment"># kubectl edit -n inadm deploy max-demo</span></span><br><span class="line">...</span><br><span class="line">      - image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.1</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-381.png" alt="upload successful"></p>
<h3 id="4-8-4-paused-暂停更新"><a href="#4-8-4-paused-暂停更新" class="headerlink" title="4.8.4 paused 暂停更新"></a>4.8.4 <strong>paused 暂停更新</strong></h3><ul>
<li>例如在滚动更新时，发现异常，可以进行更新暂停操作</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-paused.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: max-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  paused: <span class="literal">true</span>            <span class="comment"># 当更新后，发现异常。及时开启 puased 这个配置选项并 apply，则更新暂停</span></span><br><span class="line">  replicas: 10</span><br><span class="line">  ......</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="4-8-5-minreadyseconds"><a href="#4-8-5-minreadyseconds" class="headerlink" title="4.8.5 minreadyseconds"></a>4.8.5 <strong>minreadyseconds</strong></h3><ul>
<li>deploy 支持使用 spec.minReadySeconds 字段来控制滚动更新的速度，默认值为 0，表示新建的 Pod 对象一旦 “就绪” 将立即被视作可用，随后即可开始下一轮更新过程。如果设定了 spec.minReadySeconds: 5 及表示新建的 Pod 对象至少要成功运行多久才会被视作可用，及就绪之后还要等待指定的 5s 才能开始下一批次的更新。在一个批次内新建的所有 Pod 就绪后再转为可用状态前，更新操作会被阻塞，并且任何一个 Pod 就绪探测失败，都会导致滚动更新被终止。因此，为 minreadySeconds 设定一个合理的值，不仅能够减缓更新的速度，还能够让 deploy 提前发现一部分程序因为 bug 导致的升级故障。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-minready.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: max-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment"># paused: true</span></span><br><span class="line">  minReadySeconds: 5            <span class="comment"># 5s</span></span><br><span class="line">  replicas: 10</span><br><span class="line">  ......</span><br></pre></td></tr></table></figure>

<h3 id="4-8-6-revisionhistorylimit"><a href="#4-8-6-revisionhistorylimit" class="headerlink" title="4.8.6 revisionhistorylimit"></a>4.8.6 <strong>revisionhistorylimit</strong></h3><ul>
<li>deploy 保留一部分更新历史中旧版本的 ReplicaSet 对象，当我们执行回滚操作的时候，就直接使用旧版本的 ReplicaSet，在 deploy 资源保存历史版本数量有 spec.revisionHistoryLimit 属性进行定义。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-revi.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: max-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  minReadySeconds: 5            <span class="comment"># 5s</span></span><br><span class="line">  revisionHistoryLimit: 5       <span class="comment"># 保留历史版本 5 个。经测试重复的不算入历史记录里</span></span><br><span class="line">  replicas: 3</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl rollout history deploy max-demo -n inadm            # 查看当前指定空间的指定 deploy 保留的更新历史，有 2 次</span></span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br><span class="line">2         &lt;none</span><br></pre></td></tr></table></figure>

<h3 id="4-8-7-progressdeadlineSeconds"><a href="#4-8-7-progressdeadlineSeconds" class="headerlink" title="4.8.7 progressdeadlineSeconds"></a>4.8.7 <strong>progressdeadlineSeconds</strong></h3><ul>
<li>滚动更新故障超时时长，默认 600s，k8s 在升级过程中有可能由于各种原因升级卡主（这时还没明确的升级失败），比如在拉取被墙的镜像，权限不够等错误。如果配置 progressDeadlineSeconds，当达到时间还卡着，则会上报这个异常情况，这时 deploy 状态就被标记为 False，并且注明原因。但是它并不会阻止 deploy 继续进行卡住后面的升级操作</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-progress.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: max-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  minReadySeconds: 5              <span class="comment"># 5s</span></span><br><span class="line">  revisionHistoryLimit: 5</span><br><span class="line">  progressDeadlineSeconds: 600    <span class="comment"># 默认 600s</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="4-9-deploy-灰度发布"><a href="#4-9-deploy-灰度发布" class="headerlink" title="4.9 deploy 灰度发布"></a>4.9 <strong>deploy 灰度发布</strong></h2><ul>
<li>灰度发布（又名金丝雀发布）是指黑与白之间，能够平滑过渡的一种发布方式</li>
</ul>
<h3 id="4-9-1-deploy-v1-0"><a href="#4-9-1-deploy-v1-0" class="headerlink" title="4.9.1 deploy v1.0"></a>4.9.1 <strong>deploy v1.0</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-demo-a.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: demo-a</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">      version: v1.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">        version: v1.0</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-demo-a.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="4-9-2-deploy-v1-1"><a href="#4-9-2-deploy-v1-1" class="headerlink" title="4.9.2 deploy v1.1"></a>4.9.2 <strong>deploy v1.1</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-demo-b.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: demo-b</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demoapp</span><br><span class="line">      version: v1.1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demoapp</span><br><span class="line">        version: v1.1</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: recreate-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.1</span><br><span class="line">        ports: </span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-demo-b.yaml</span></span><br></pre></td></tr></table></figure>

<h3 id="4-9-3-实现方式"><a href="#4-9-3-实现方式" class="headerlink" title="4.9.3 实现方式"></a>4.9.3 <strong>实现方式</strong></h3><ul>
<li>先增加 v1.1 版本的 replicas 副本数量，然后减少 v1.0 版本 replicas 的副本数量，逐步进行切换</li>
<li>其中 deploy 中 selector 有2个标签，svc 中有一个标签</li>
<li>最后在清除下线的 deploy 时，注意不要删除 svc 服务</li>
</ul>
<h1 id="5-0-daemonset"><a href="#5-0-daemonset" class="headerlink" title="5.0 daemonset"></a>5.0 <strong>daemonset</strong></h1><ul>
<li>什么是 DaemonSet：DaemonSet 控制器是用来保证在所有节点上运行一个 Pod 的副本。当有节点加入集群时，也会为他们新增一个 Pod。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod</li>
<li>DaemonSet 典型用法:<ul>
<li>在每个节点上运行集群存储守护进程：gluster、ceph</li>
<li>在每个节点上运行日志收集守护进程：fluentd、filebeat、logstash</li>
<li>在每个节点上运行监控守护进程：prometheus、node_exporter</li>
<li>在每个节点上运行网络插件为 Pod 提供网络服务：flannel、calico</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: &lt;string&gt;</span><br><span class="line">  namespace: &lt;string&gt;</span><br><span class="line">spec:</span><br><span class="line">  minReadySeconds: &lt;<span class="built_in">integer</span>&gt;              <span class="comment"># Pod 就绪后多少秒内容器无崩溃视为&quot;就绪&quot;</span></span><br><span class="line">  revisionHistoryLimit: &lt;<span class="built_in">integer</span>&gt;         <span class="comment"># 滚动更新历史记录数量</span></span><br><span class="line">  selector: &lt;object&gt;                      <span class="comment"># 标签选择器</span></span><br><span class="line">  updateStrategy: &lt;object&gt;                <span class="comment"># 滚动更新策略</span></span><br><span class="line">    <span class="built_in">type</span>: &lt;string&gt;                        <span class="comment"># 滚动更新类型，可用值有 OnDelete 和 RollingUpdate</span></span><br><span class="line">    rollingUpdate: &lt;object&gt;               <span class="comment"># 滚动更新参数，专用于 RollingUpdate 类型</span></span><br><span class="line">      maxSurge: &lt;string&gt;                  <span class="comment"># 滚动更新参数，更新期间存在的总 Pod 对象数量最多可超出期望值的个数</span></span><br><span class="line">      maxUnavailable: &lt;string&gt;            <span class="comment"># 滚动更新参数，升级期间不可用的 Pod 副本数</span></span><br><span class="line">  template: &lt;object&gt;                      <span class="comment"># Pod 模板</span></span><br><span class="line">    metadata: &lt;object&gt;                    <span class="comment"># Pod 名称</span></span><br><span class="line">    spec: &lt;object&gt;                        <span class="comment"># Pod 详情</span></span><br></pre></td></tr></table></figure>

<h2 id="5-1-示例"><a href="#5-1-示例" class="headerlink" title="5.1 示例"></a>5.1 <strong>示例</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ds-demo.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: ds-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector: </span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template: </span><br><span class="line">    metadata: </span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec: </span><br><span class="line">      nodeSelector:                        <span class="comment"># 节点选择器，通过标签来选择</span></span><br><span class="line">        <span class="built_in">type</span>: ssd-nginx                    <span class="comment"># 节点必须存在的标签： key=type、value=ssd-nginx</span></span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx-container</span><br><span class="line">        image: nginx:1.16</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">        livenessProbe:                     <span class="comment"># 存活探针，监听 80端口，如果 80 端口不存在则重启</span></span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 80</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">        readinessProbe:                    <span class="comment"># 就绪探针，监听 80 端口，如果 80 不存在则重启</span></span><br><span class="line">          httpGet:</span><br><span class="line">            path: <span class="string">&quot;/&quot;</span></span><br><span class="line">            port: 80</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line"></span><br><span class="line">// 添加节点的标签</span><br><span class="line"><span class="comment"># kubectl get nodes --show-labels                            # 查看节点标签</span></span><br><span class="line"><span class="comment"># kubectl label nodes k8s-node01 type=ssd-nginx              # 给节点 node01 打标签</span></span><br><span class="line"><span class="comment"># kubectl get nodes --show-labels k8s-node01                 # 查看 node01 节点的标签信息</span></span><br><span class="line">// 去掉节点上的标签命令</span><br><span class="line"><span class="comment"># kubectl label nodes k8s-node01 type-                       # key=type，将 type 减去</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-383.png" alt="upload successful"></p>
<h2 id="5-2-部署-node-exporter"><a href="#5-2-部署-node-exporter" class="headerlink" title="5.2 部署 node_exporter"></a>5.2 <strong>部署 node_exporter</strong></h2><ul>
<li>为每个节点运行一份 node_exporter，采集当前节点信息</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ds-nodexport.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: node-export</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: node-export</span><br><span class="line">spec:</span><br><span class="line">  selector: </span><br><span class="line">    matchLabels:</span><br><span class="line">      app: node-export</span><br><span class="line">  template: </span><br><span class="line">    metadata: </span><br><span class="line">      labels:</span><br><span class="line">        app: node-export</span><br><span class="line">    spec: </span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      hostNetwork: <span class="literal">true</span>                   <span class="comment"># 共享主机网络</span></span><br><span class="line">      hostPID: <span class="literal">true</span>                       <span class="comment"># 获取主机 PID</span></span><br><span class="line">      hostIPC: <span class="literal">true</span></span><br><span class="line">      nodeSelector:</span><br><span class="line">        kubernetes.io/os: linux</span><br><span class="line">      containers:</span><br><span class="line">      - name: monitor-node-export</span><br><span class="line">        image: harbor.inadm.com/ops_monitor/node-exporter:v1.3.1</span><br><span class="line">        ports:</span><br><span class="line">        - name: node-ex-port</span><br><span class="line">          containerPort: 9100                  <span class="comment"># 监听在节点的 9100 端口上</span></span><br><span class="line">          hostPort: 9100</span><br><span class="line"></span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: node-ex-port</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: <span class="string">&quot;/metrics&quot;</span></span><br><span class="line">            port: node-ex-port</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line"></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 150m</span><br><span class="line">            memory: 180Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 150m</span><br><span class="line">            memory: 180Mi</span><br><span class="line">        securityContext:</span><br><span class="line">          runAsNonRoot: <span class="literal">true</span></span><br><span class="line">          runAsUser: 65534</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ds-nodexport.yaml </span></span><br><span class="line">// 浏览器访问: http://NODE_IP:9100/metrics</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-384.png" alt="upload successful"></p>
<p><img src="/images/pasted-386.png" alt="upload successful"></p>
<h2 id="5-3-ds-更新策略"><a href="#5-3-ds-更新策略" class="headerlink" title="5.3 ds 更新策略"></a>5.3 <strong>ds 更新策略</strong></h2><ul>
<li>ds 也支持更新策略，它支持 OnDelete 和 RollingUpdate：<ul>
<li>OnDelete：是在相应节点的 Pod 资源被删除后重建新版本，从而运行用户手动编排更新过程</li>
<li>RollingUpdate：滚动更新，工作逻辑和 deploy 滚动更新类似</li>
</ul>
</li>
</ul>
<h3 id="5-3-1-RollingUpdate"><a href="#5-3-1-RollingUpdate" class="headerlink" title="5.3.1 RollingUpdate"></a>5.3.1 <strong>RollingUpdate</strong></h3><ul>
<li>将此前创建的 node-expoter 中的 Pod 模板镜像修改为 v1.4.0，测试其更新过程</li>
<li>安装默认的 RollingUpdate 策略，node-exports-ds 资源将采用一次更新一个 Pod 对象，待新建 Pod 的对象就绪后，在更新下一个 Pod 对象，直到全部完成</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ds-node-expoter.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: node-export</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: node-export</span><br><span class="line">spec:</span><br><span class="line">  minReadySeconds: 3</span><br><span class="line">  revisionHistoryLimit: 5</span><br><span class="line">  updateStrategy:</span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate                   <span class="comment"># 滚动更新(默认更新策略)</span></span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  selector: </span><br><span class="line">    matchLabels:</span><br><span class="line">      app: node-export</span><br><span class="line">  template: </span><br><span class="line">    metadata: </span><br><span class="line">      labels:</span><br><span class="line">        app: node-export</span><br><span class="line">    spec: </span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      hostNetwork: <span class="literal">true</span>                  <span class="comment"># 共享主机网络</span></span><br><span class="line">      hostPID: <span class="literal">true</span>                       <span class="comment"># 获取主机 PID</span></span><br><span class="line">      hostIPC: <span class="literal">true</span></span><br><span class="line">      nodeSelector:</span><br><span class="line">        kubernetes.io/os: linux</span><br><span class="line">      containers:</span><br><span class="line">      - name: monitor-node-export</span><br><span class="line">        image: harbor.inadm.com/ops_monitor/node-exporter:v1.4.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: node-ex-port</span><br><span class="line">          containerPort: 9100</span><br><span class="line">          hostPort: 9100                  <span class="comment"># 监听在节点的 9100 端口上</span></span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: node-ex-port</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: <span class="string">&quot;/metrics&quot;</span></span><br><span class="line">            port: node-ex-port</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 150m</span><br><span class="line">            memory: 180Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 150m</span><br><span class="line">            memory: 180Mi</span><br><span class="line">        securityContext:</span><br><span class="line">          runAsNonRoot: <span class="literal">true</span></span><br><span class="line">          runAsUser: 65534</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ds-node-expoter.yaml </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-387.png" alt="upload successful"></p>
<h3 id="5-3-2-OnDelete"><a href="#5-3-2-OnDelete" class="headerlink" title="5.3.2 OnDelete"></a>5.3.2 <strong>OnDelete</strong></h3><ul>
<li>将此前创建的 node-expoter 中的 Pod 模板镜像更新为 v1.5.0，由于升级版本跨度过大，无法确保升级过程中稳定性，我们就不得不适用 OnDelete 策略来替换默认的 RollingUpdate 策略</li>
<li>由于 OnDelete 并非自动完成升级，它需要管理员手动删除 Pod，然后重新拉起新的 Pod，才能完成更新。（对于升级有着先后顺序的软件这种方法就非常有用）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ds-node-expoter.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: node-export</span><br><span class="line">  namespace: inadm</span><br><span class="line">  labels:</span><br><span class="line">    app: node-export</span><br><span class="line">spec:</span><br><span class="line">  minReadySeconds: 3</span><br><span class="line">  revisionHistoryLimit: 5</span><br><span class="line">  updateStrategy:</span><br><span class="line">    <span class="built_in">type</span>: OnDelete                   <span class="comment"># 使用 OnDelete 更新策略</span></span><br><span class="line">  selector: </span><br><span class="line">    matchLabels:</span><br><span class="line">      app: node-export</span><br><span class="line">  template: </span><br><span class="line">    metadata: </span><br><span class="line">      labels:</span><br><span class="line">        app: node-export</span><br><span class="line">    spec: </span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      hostNetwork: <span class="literal">true</span>                   <span class="comment"># 共享主机网络</span></span><br><span class="line">      hostPID: <span class="literal">true</span>                       <span class="comment"># 获取主机 PID</span></span><br><span class="line">      hostIPC: <span class="literal">true</span></span><br><span class="line">      nodeSelector:</span><br><span class="line">        kubernetes.io/os: linux</span><br><span class="line">      containers:</span><br><span class="line">      - name: monitor-node-export</span><br><span class="line">        image: harbor.inadm.com/ops_monitor/node-exporter:v1.5.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: node-ex-port</span><br><span class="line">          containerPort: 9100</span><br><span class="line">          hostPort: 9100                  <span class="comment"># 监听在节点的 9100 端口上</span></span><br><span class="line"></span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: node-ex-port</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: <span class="string">&quot;/metrics&quot;</span></span><br><span class="line">            port: node-ex-port</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line"></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 150m</span><br><span class="line">            memory: 180Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 150m</span><br><span class="line">            memory: 180Mi</span><br><span class="line">        securityContext:</span><br><span class="line">          runAsNonRoot: <span class="literal">true</span></span><br><span class="line">          runAsUser: 65534</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ds-node-expoter.yaml </span></span><br><span class="line">// 手动删除老的 Pod</span><br><span class="line"><span class="comment"># kubectl get pod -l app=node-export -n inadm</span></span><br><span class="line"><span class="comment"># kubectl delete pod -l app=node-export -n inadm</span></span><br><span class="line">// 浏览器访问验证: http://NODE_IP:9100/metrics</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-389.png" alt="upload successful"></p>
<h1 id="6-0-job、cronjob"><a href="#6-0-job、cronjob" class="headerlink" title="6.0 job、cronjob"></a>6.0 <strong>job、cronjob</strong></h1><ul>
<li>什么是 Job：Job 控制器常用于管理那些运行一段时间就能够 “完成” 的任务，例如离线数据分析，数据备份等，当任务完成后，由 Job 控制器将该 Pod 对象至于 Complete 完成状态，在完成一定时间后，当达到用户指定的生存周期，由系统自动删除任务。如果容器中的进程因 “错误” 而终止，则需要依赖 RestartPolicy 配置来确定是否重启，如果是因为节点故障造成 Pod 意外终止的话，会被重新创建起来继续运行。<ul>
<li>Pod 执行，退出状态为 0，则表示执行成功，而后将该 Pod 状态置于 Complete</li>
<li>Pod 执行，退出状态码为非 0，检查 restartpolicy 为 Never，表示永不重启，而后该 Pod 状态置于 Failure</li>
<li>Pod 执行，退出状态码为非0，检查 restartpolicy 为 OnFailure，表示退出状态码如果不为 0 时重启该 Pod，所以会尝试重新拉取 Pod，直到执行成功为止</li>
</ul>
</li>
<li>Job 工作方式：<ul>
<li>实际生产环境中，有些任务可能需要运行不止一次，用户可以配置他们以串行或并行方式运行起来。<ul>
<li>串行 Job：将一个作业串行执行多次知道满足期望的次数</li>
<li>并行 Job：设定工作队列数，同时运行，而每个队列仅运行一个作业</li>
</ul>
</li>
<li>注意：对于有严格次序要求的作业，只能选择串行执行，而没有严格次序要求的可以选择并行来运行的效率和速度</li>
</ul>
</li>
<li>job: 运行完便结束：消费者–&gt; 程序 –&gt; Dockerfile –&gt; job –&gt; pod</li>
<li>cronjob: 定时执行：消费者–&gt; 程序 –&gt; Dockerfile –&gt; cronjob –&gt; job –&gt; pod</li>
</ul>
<h2 id="6-1-job-基础资源"><a href="#6-1-job-基础资源" class="headerlink" title="6.1 job 基础资源"></a>6.1 <strong>job 基础资源</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat job-demo.yaml</span></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: &lt;string&gt;</span><br><span class="line">  namespace: &lt;string&gt;</span><br><span class="line">spec:</span><br><span class="line">  selector: &lt;Object&gt;</span><br><span class="line">  completions: &lt;<span class="built_in">integer</span>&gt;                    <span class="comment"># 期望成功完成作业的次数</span></span><br><span class="line">  parallelism: &lt;<span class="built_in">integer</span>&gt;                    <span class="comment"># 作业的最大并行度，默认为 1</span></span><br><span class="line">  backoffiLimit: &lt;<span class="built_in">integer</span>&gt;                  <span class="comment"># 将作业标记为 Failed 之前的重试次数，默认为 6</span></span><br><span class="line">  activeDeadlineSeconds: &lt;<span class="built_in">integer</span>&gt;          <span class="comment"># 作业启动后可处于活动状态的时长，超出则会被标记为 Failed</span></span><br><span class="line">  ttlSecondsAfterFinished: &lt;<span class="built_in">integer</span>&gt;        <span class="comment"># 作业的最大生存时长，超期将被删除，0 表示立即删除</span></span><br><span class="line">  template: &lt;Object&gt;</span><br><span class="line">  spec: &lt;object&gt;</span><br><span class="line">    containers: &lt;Object&gt;</span><br><span class="line">    restartpolicy: &lt;string&gt;</span><br></pre></td></tr></table></figure>

<h2 id="6-2-job-示例代码"><a href="#6-2-job-示例代码" class="headerlink" title="6.2 job 示例代码"></a>6.2 <strong>job 示例代码</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat job-demo.yaml </span></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: job-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  completions: 5                            <span class="comment"># 需要成功运行 5 次</span></span><br><span class="line">  parallelism: 2                            <span class="comment"># 并行执行为 2</span></span><br><span class="line">  backoffLimit: 2                           <span class="comment"># 失败后，允许重试 2 次</span></span><br><span class="line">  activeDeadlineSeconds: 120                <span class="comment"># 总活跃时间为 120s，包含运行 Pod 时间+异常重试次数时间</span></span><br><span class="line">  ttlSecondsAfterFinished: 100              <span class="comment"># Job-demo 在结束 100s 之后，会被系统自动删除 (无论执行成功或失败)</span></span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: job-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">        <span class="built_in">command</span>: [<span class="string">&quot;/bin/bash&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 3&quot;</span>]</span><br><span class="line">      restartPolicy: OnFailure</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f job-demo.yaml </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-390.png" alt="upload successful"></p>
<h2 id="6-3-并行读取RabbitMQ数据演示"><a href="#6-3-并行读取RabbitMQ数据演示" class="headerlink" title="6.3 并行读取RabbitMQ数据演示"></a>6.3 <strong>并行读取RabbitMQ数据演示</strong></h2><ul>
<li>本例中，运行包含多个并行工作进程的 k8s job。文档中，每个 Pod 一旦被创建，会立即从任务重取走一个消息，然后将消息从队列中删除并退出本次任务</li>
<li>示例主要步骤：<ul>
<li>启动一个消息队列服务：使用 RabbitMQ</li>
<li>创建一个队列，放上消息数据：每个消息表示一个要执行的任务</li>
<li>启动一个 Job，该 Job 启动多个 Pod：每个 Pod 从消息队列中读走一个任务，处理它，然后重复执行，知道队列的队尾</li>
</ul>
</li>
</ul>
<p><img src="/images/pasted-391.png" alt="upload successful"></p>
<h3 id="6-3-1-创建-RabbitMQ-服务"><a href="#6-3-1-创建-RabbitMQ-服务" class="headerlink" title="6.3.1 创建 RabbitMQ 服务"></a>6.3.1 <strong>创建 RabbitMQ 服务</strong></h3><ul>
<li>RabbitMQ 消息队列服务</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat job-rabbitmq-server.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: job-rabbitmq</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: rbtmq</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: rbtmq</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: mq-container</span><br><span class="line">        image: rabbitmq</span><br><span class="line">        ports:</span><br><span class="line">        - name: rbt</span><br><span class="line">          containerPort: 5672</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 300m</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: job-rabbitmq</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: rbtmq</span><br><span class="line">  ports:</span><br><span class="line">  - name: rbt</span><br><span class="line">    port: 5672</span><br><span class="line">    targetPort: 5672</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f job-rabbitmq-server.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="6-3-2-消息发布者"><a href="#6-3-2-消息发布者" class="headerlink" title="6.3.2 消息发布者"></a>6.3.2 <strong>消息发布者</strong></h3><ul>
<li>启动临时容器测试</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 启动 ubuntu18.04 镜像，然后安装一些工具</span><br><span class="line"><span class="comment"># kubectl run -it amqp-consume --image ubuntu:18.04</span></span><br><span class="line">root@temp:/<span class="comment"># apt-get update</span></span><br><span class="line">root@temp:/<span class="comment"># apt-get -y install curl ca-certificates amqp-tools python dnsutils</span></span><br><span class="line"></span><br><span class="line">// 验证 RabbitMQ 服务 [手动方式]</span><br><span class="line">root@temp:/<span class="comment"># nslookup job-rabbitmq.inadm.svc.cluster.local</span></span><br><span class="line">root@temp:/<span class="comment"># export BROKER_URL=amqp://guest:guest@job-rabbitmq.inadm:5672</span></span><br><span class="line">root@temp:/<span class="comment"># /usr/bin/amqp-declare-queue --url=$BROKER_URL -q foo                            // 创建队列</span></span><br><span class="line">root@temp:/<span class="comment"># /usr/bin/amqp-publish --url=$BROKER_URL -r foo -p -b inadm                      // 向它推送一条消息</span></span><br><span class="line">root@temp:/<span class="comment"># /usr/bin/amqp-consume --url=$BROKER_URL -q foo -c 1 cat &amp;&amp; echo                 // 取消息</span></span><br><span class="line">    inadm</span><br><span class="line"></span><br><span class="line">// 为队列增加任务，创建一个 job1 队列，然后给队列中填充 8 个消息 [脚本方式]</span><br><span class="line"><span class="built_in">export</span> BROKER_URL=amqp://guest:guest@job-rabbitmq.inadm:5672</span><br><span class="line">/usr/bin/amqp-declare-queue --url=<span class="variable">$BROKER_URL</span> -q job1</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> aa bb cc <span class="built_in">dd</span> ee ff gg hh; <span class="keyword">do</span> /usr/bin/amqp-publish --url=<span class="variable">$BROKER_URL</span> -r job1 -p -b <span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="6-3-3-消息订阅"><a href="#6-3-3-消息订阅" class="headerlink" title="6.3.3 消息订阅"></a>6.3.3 <strong>消息订阅</strong></h3><ul>
<li>创建镜像，获取数据，然后以 Job 方式运行起来</li>
<li>编写获取队列程序</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 获取队列数据，然后等待 10s，结束</span><br><span class="line"><span class="comment"># cat worker.py</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line">import sys</span><br><span class="line">import time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Processing &quot;</span> + sys.stdin.readlines()[0])</span><br><span class="line">time.sleep(10)</span><br></pre></td></tr></table></figure>

<ul>
<li>编写 Dockerfile</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 编写 Dockerfile 文件，制作为镜像，然后推送到自己的仓库；(注意镜像中需要传递的变量)</span><br><span class="line"><span class="comment"># cat Dockerfile </span></span><br><span class="line">FROM ubuntu:18.04</span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">  apt-get -y install curl ca-certificates amqp-tools python dnsutils --no-install-recommends \</span><br><span class="line">  &amp;&amp; <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span><br><span class="line">COPY ./worker.py /worker.py</span><br><span class="line">RUN <span class="built_in">chmod</span> +x /worker.py</span><br><span class="line">CMD /usr/bin/amqp-consume --url=<span class="variable">$BROKER_URL</span> -q <span class="variable">$QUEUE</span> -c 1 /worker.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker build -t rabbit-mq-consumer-job .</span></span><br><span class="line"><span class="comment"># docker tag rabbit-mq-consumer-job:latest harbor.inadm.com/inadm_kubernetes/rabbit-mq-consumer-job:latest</span></span><br><span class="line"><span class="comment"># docker push harbor.inadm.com/inadm_kubernetes/rabbit-mq-consumer-job:latest</span></span><br></pre></td></tr></table></figure>

<ul>
<li>编写 Job 任务：每个 Pod 使用队列中的一个消息然后退出。这样， Job 的完成计数就代表了完成的工作项的数量</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat job-rabbitmq-consumer.yaml</span></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: rabbitmq-consumer</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  completions: 8                    <span class="comment"># 总共运行 8 次，因为队列中有 8 条消息</span></span><br><span class="line">  parallelism: 2                    <span class="comment"># 并行执行 2 个任务</span></span><br><span class="line">  ttlSecondsAfterFinished: 1000     <span class="comment"># 结束后 1000s 删除</span></span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: mq-consumer-work</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/rabbit-mq-consumer-job:latest</span><br><span class="line">        <span class="built_in">env</span>:</span><br><span class="line">        - name: BROKER_URL</span><br><span class="line">          value: amqp://guest:guest@job-rabbitmq.inadm:5672</span><br><span class="line">        - name: QUEUE</span><br><span class="line">          value: job1</span><br><span class="line">      restartPolicy: OnFailure</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f job-rabbitmq-consumer.yaml</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe jobs rabbitmq-consumer -n inadm</span></span><br><span class="line"><span class="comment"># kubectl logs reabbitmq-consumer-xxxx -n inadm            # 通过检查 Pod 的 logs，可以看到消息被取走了</span></span><br><span class="line"><span class="comment"># kubectl describe jobs -n inadm rabbitmq-consumer         # 检查 Job</span></span><br><span class="line">// 如果设置的完成数量小于队列中的消息数量，会导致一部分消息项不会被执行</span><br><span class="line">// 如果设置的完成数量大于队列中的消息数量，当队列中的所有的消息都处理完成后，Job 也会显示为未完成。Job 将创建 Pod 并阻塞等待消息输入。</span><br><span class="line">// 当发生下面两种情况时，即使队列中所有消息都处理完成了，Job 也不会显示为完成状态</span><br><span class="line">    1. 在 amqp-consume 命令拿到消息和容器成功退出之间的时间段内，执行杀死容器操作</span><br><span class="line">    2. 在 kubectl 向 api-server 传回 Pod 成功运行之前，发生节点崩</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-392.png" alt="upload successful"></p>
<p><img src="/images/pasted-393.png" alt="upload successful"></p>
<h2 id="6-4-并行读取redis数据"><a href="#6-4-并行读取redis数据" class="headerlink" title="6.4 并行读取redis数据"></a>6.4 <strong>并行读取redis数据</strong></h2><ul>
<li>运行一个 K8s Job，其中 Pod 会运行多个并行工作进程</li>
<li>在例子中，当每个 Pod 被创建时，它会从一个任务队列中获取一个工作单元，处理它，然后重复，直到达到队列尾部<ul>
<li>启动 Redis 存储服务用于保存工作队列：在上一个例子中，使用了 RabbitMQ，但无法提供一个良好的方式来检测一个有限长度的工作队列是否为空，所以本次使用 Redis，和一个自定义的工作队列客户端。</li>
<li>创建一个队列，然后向其中填充消息：每个消息表示一个将要被处理的工作任务</li>
<li>启动一个 Job 队列中的人物进行处理：这个 Job 启动了若干个 Pod。每个 Pod 从消息队列中取出一个工作任务，处理它，然后重复，直到到达队列的尾部</li>
</ul>
</li>
</ul>
<p><img src="/images/pasted-394.png" alt="upload successful"></p>
<h3 id="6-4-1-创建-Redis-服务"><a href="#6-4-1-创建-Redis-服务" class="headerlink" title="6.4.1 创建 Redis 服务"></a>6.4.1 <strong>创建 Redis 服务</strong></h3><ul>
<li>部署 redis 消息队列服务</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat job-redis-server.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-deploy</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: redis</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: redis</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: redis-server</span><br><span class="line">        image: redis</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 6379</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: redis</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: redis</span><br><span class="line">  ports:</span><br><span class="line">  - port: 6379</span><br><span class="line">    targetPort: 6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f job-redis-server.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="6-4-2-消息发布者"><a href="#6-4-2-消息发布者" class="headerlink" title="6.4.2 消息发布者"></a>6.4.2 <strong>消息发布者</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 启动临时可交互的 Pod 用于运行 Redis 命令行</span><br><span class="line"><span class="comment"># kubectl run -it tmp-redis -n inadm --image redis --command &quot;/bin/sh&quot;</span></span><br><span class="line"></span><br><span class="line">// 连接 Redis 服务，人啊后王队列中添加任务，job2 列表，就是工作队列</span><br><span class="line">redis-cli -h redis.inadm</span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm01&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm02&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm03&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm04&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm05&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm06&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm07&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm08&quot;</span></span><br><span class="line"></span><br><span class="line">redis.inadm:6379&gt; LRANGE job2 0 -1        <span class="comment"># 查看输入的值</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-395.png" alt="upload successful"></p>
<h3 id="6-4-3-消息订阅"><a href="#6-4-3-消息订阅" class="headerlink" title="6.4.3 消息订阅"></a>6.4.3 <strong>消息订阅</strong></h3><ul>
<li>创建镜像，获取数据，然后以 Job 方式运行起来</li>
<li>编写获取队列程序：使用一个带有 Redis 客户端的 python 工作程序从消息队列中读出消息</li>
<li><a target="_blank" rel="noopener" href="https://kubernetes.io/examples/application/job/redis/rediswq.py">rediswq.py</a> 制作镜像中的一个文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// 这里提供了一个简单的 Redis 工作队列客户端库，叫 rediswq.py。然后 Job 中每个 Pod 内的 <span class="string">&quot;工作程序&quot;</span> 使用工作队列客户端库获取数据</span><br><span class="line"><span class="comment"># cat worker.py</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line">import time</span><br><span class="line">import rediswq</span><br><span class="line">host=<span class="string">&quot;redis.inadm&quot;</span>      <span class="comment"># 连接 redis 的 svc 名称</span></span><br><span class="line">q = rediswq.RedisWQ(name=<span class="string">&quot;job2&quot;</span>, host=host)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Worker with sessionID: &quot;</span> + q.sessionID())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Initial queue state: empty=&quot;</span> + str(q.empty()))</span><br><span class="line"><span class="keyword">while</span> not q.empty():</span><br><span class="line">    item = q.lease(lease_secs=10, block=True, <span class="built_in">timeout</span>=2)</span><br><span class="line">    <span class="keyword">if</span> item is not None:</span><br><span class="line">        itemstr = item.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Working on &quot;</span> + itemstr)</span><br><span class="line">        time.sleep(10)</span><br><span class="line">        q.complete(item)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Waiting for work&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Queue empty, exiting&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>编写 Dockerfile</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dockerfile</span></span><br><span class="line">FROM harbor.inadm.com/inadm_kubernetes/python:alpine3.15</span><br><span class="line">RUN pip install redis</span><br><span class="line">COPY ./worker.py /worker.py</span><br><span class="line">COPY ./rediswq.py /rediswq.py</span><br><span class="line">RUN <span class="built_in">chmod</span> +x /worker.py /rediswq.py</span><br><span class="line">CMD python worker.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker build -t harbor.inadm.com/inadm_kubernetes/redis-mq-consumer-job:v1.0 .</span></span><br><span class="line"><span class="comment"># docker push harbor.inadm.com/inadm_kubernetes/redis-mq-consumer-job:v1.0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>编写 Job 任务</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat redis-consumer.yaml</span></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-consumber</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment"># completions: 1          # 默认 1</span></span><br><span class="line">  parallelism: 2</span><br><span class="line">  ttlSecondsAfterFinished: 1000</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: redis-consumber</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/redis-mq-consumer-job:v1.0</span><br><span class="line">      restartPolicy: OnFailure</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f redis-consumer.yaml</span></span><br></pre></td></tr></table></figure>

<ul>
<li>检查</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe jobs redis-consumber -n inadm</span></span><br><span class="line"><span class="comment"># kubectl logs -n inadm redis-consumber-jwstt</span></span><br><span class="line">// 在这个例子中，每个 Pod 处理了队列中的多个项目，直到队列中没有项目时便退出。因为由工作程序自行检测工作队列是否为空，并且 Job 控制器不知道工作队列的存在，这依赖于工作程序在完成时发出信号。</span><br><span class="line">// 工作程序以成功退出的形式发出信号表示工作队列已经为空。所以，只要有任意一个工作程序成功退出，控制器就知道工作已经完成了，所有的 Pod 将很快会退出。</span><br><span class="line">// 因此，我们将 Job 的完成技术（Complateion count）设置为 1</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-396.png" alt="upload successful"></p>
<h2 id="6-5-contjob"><a href="#6-5-contjob" class="headerlink" title="6.5 contjob"></a>6.5 <strong>contjob</strong></h2><ul>
<li>什么是 CronJob：CronJob 资源用于管理 Job 资源的运行时间，它允许用户在特定时间或指定时间运行 Job，它适合自动执行特定的任务，例如 备份、报告、发送邮件、垃圾清理。而一个 CronJob 对象就像 crontab 文件中的一行。它用cron 格式进行编写 分 时 日 月 周</li>
</ul>
<p><img src="/images/pasted-397.png" alt="upload successful"></p>
<ul>
<li>CronJob 并发执行：CronJob 资源的 对象可能不支持同时运行多个实例，用户可基于 spec.concurrencyPolicy 属性来控制多个 CronJob 并存的机制<ul>
<li>Allow：运行不同时间点的多个 CronJob 实例同时运行（默认）</li>
<li>Forbid：CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行</li>
<li>Replace：用于让后一个 CronJob 取代前一个，即终止一个并启动后一个</li>
</ul>
</li>
<li>注意：并发性规则仅适用于相同 CronJob 创建的任务。如果有多个不同的 CronJob，它们相应的任务总是允许并发执行的。</li>
</ul>
<h3 id="6-5-1-CrobJob-基础资源"><a href="#6-5-1-CrobJob-基础资源" class="headerlink" title="6.5.1 CrobJob 基础资源"></a>6.5.1 <strong>CrobJob 基础资源</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: betach/v1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: &lt;string&gt;</span><br><span class="line">  namespace: &lt;string&gt;</span><br><span class="line">spec:</span><br><span class="line">  concurrencyPolicy: &lt;string&gt;                 <span class="comment"># 并发策略，可用值有 Allow、Forbid、Replace</span></span><br><span class="line">  failedJobdsHistoryLimit: &lt;<span class="built_in">integer</span>&gt;          <span class="comment"># 失败作业历史记录数，默认为 1</span></span><br><span class="line">  successfulJobsHistoryLimit: &lt;<span class="built_in">integer</span>&gt;       <span class="comment"># 成功作业历史记录数，默认为 1</span></span><br><span class="line">  startingDeadlineSeconds: &lt;<span class="built_in">integer</span>&gt;          <span class="comment"># 因错误时间点而未执行的作业可超期时长</span></span><br><span class="line">  schedule: &lt;string&gt; -required-               <span class="comment"># 调度时间设定，必选字段</span></span><br><span class="line">  jobTemplate: &lt;Object&gt; -required-            <span class="comment"># Job 作业模板，必选字段</span></span><br><span class="line">    metadate: &lt;Object&gt;</span><br><span class="line">    spec: &lt;Object&gt;</span><br><span class="line">      completions: &lt;<span class="built_in">integer</span>&gt;                  <span class="comment"># 期望成功完成作业的次数</span></span><br><span class="line">      parallelism: &lt;<span class="built_in">integer</span>&gt;                  <span class="comment"># 作业的最大并行度，默认为 1</span></span><br><span class="line">      backoffLimit: &lt;<span class="built_in">integer</span>&gt;                 <span class="comment"># 将作业标记为 Failed 之前的重试次数，默认为 6</span></span><br><span class="line">      activeDeadlineSeconds: &lt;<span class="built_in">integer</span>&gt;        <span class="comment"># 作业启动后可处于活动状态的时长，超出则会被标记为 Failed</span></span><br><span class="line">      ttlSecondsAfterFinished: &lt;<span class="built_in">integer</span>&gt;      <span class="comment"># 作业的最大生存时长，超期将被删除， 0 表示立即删除</span></span><br><span class="line">      template: &lt;Object&gt;                      <span class="comment"># Pod 模板</span></span><br><span class="line">        spec: &lt;Object&gt;</span><br><span class="line">          containers: &lt;Object&gt;</span><br><span class="line">          restartPolicy: &lt;string&gt;             <span class="comment"># Pod 重启策略，默认的 Always 并不适应，因此必须单独指定</span></span><br></pre></td></tr></table></figure>

<ul>
<li>CronJob 示例</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cronjob-demo.yaml</span></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: cronjob-demo</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  failedJobsHistoryLimit: 5           <span class="comment"># 保留运行失败的 Job，5条</span></span><br><span class="line">  successfulJobsHistoryLimit: 5       <span class="comment"># 保留运行成功的 Job，5条</span></span><br><span class="line">  startingDeadlineSeconds: 300        <span class="comment"># 错误计划执行时间，而允许延迟启动的最长时间</span></span><br><span class="line">  schedule: <span class="string">&quot;* * * * *&quot;</span>               <span class="comment"># 每分钟执行一次</span></span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      parallelism: 1                  <span class="comment"># 并行执行为 1</span></span><br><span class="line">      completions: 1                  <span class="comment"># 需要成功运行 1 次</span></span><br><span class="line">      ttlSecondsAfterFinished: 3600   <span class="comment"># Job 在结束 3600s 之后，会被系统自动删除(无论执行成功还是失败)</span></span><br><span class="line">      <span class="comment">#activeDeadlineSeconds: 120      # 总活跃时间为 120s，包含运行 Pod时间+异常时间重试次数时间</span></span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          imagePullSecrets:</span><br><span class="line">          - name: harbor-auth</span><br><span class="line">          restartPolicy: OnFailure</span><br><span class="line">          containers:</span><br><span class="line">          - name: cronjob-container</span><br><span class="line">            image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">            <span class="built_in">command</span>:</span><br><span class="line">            - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">            - <span class="string">&quot;-c&quot;</span></span><br><span class="line">            - <span class="string">&quot;echo Hell From CronJob; sleep 5&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f cronjob-demo.yaml</span></span><br><span class="line"><span class="comment"># kubectl logs -n inadm cronjob-demo-29219016-p5dtn</span></span><br><span class="line">Hell From CronJob</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-398.png" alt="upload successful"></p>
<h2 id="6-6-每分钟从redis队列获取数据"><a href="#6-6-每分钟从redis队列获取数据" class="headerlink" title="6.6 每分钟从redis队列获取数据"></a>6.6 <strong>每分钟从redis队列获取数据</strong></h2><ul>
<li>消费者–&gt; 程序 –&gt; Dockerfile –&gt; cronjob –&gt; job –&gt; pod</li>
</ul>
<h3 id="6-6-1-消息发布者"><a href="#6-6-1-消息发布者" class="headerlink" title="6.6.1 消息发布者"></a>6.6.1 <strong>消息发布者</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. 创建 redis 服务：参考 6.4.1 </span><br><span class="line">2. 消息发布者</span><br><span class="line"><span class="comment"># kubectl exec -it tmp-redis -n inadm -- &quot;/bin/sh&quot;                    # 参考：6.4.2</span></span><br><span class="line">3. 连接 redis 服务，然后往队列中添加一些任务</span><br><span class="line"><span class="comment"># redis-cli -h redis.inadm</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob01&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob02&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob03&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob04&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="6-6-2-消息发布者"><a href="#6-6-2-消息发布者" class="headerlink" title="6.6.2 消息发布者"></a>6.6.2 <strong>消息发布者</strong></h3><ul>
<li>定期获取 Redis 中数据</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat redis-cronjob-consumber.yaml</span></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-consumer-cronjob</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  failedJobsHistoryLimit: 3          <span class="comment"># 保留运行失败的 Job，5条</span></span><br><span class="line">  successfulJobsHistoryLimit: 3       <span class="comment"># 保留运行成功的 Job，5条</span></span><br><span class="line">  startingDeadlineSeconds: 300        <span class="comment"># 错误计划执行时间，而允许延迟启动的最长时间</span></span><br><span class="line">  schedule: <span class="string">&quot;* * * * *&quot;</span>               <span class="comment"># 每分钟执行一次</span></span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      parallelism: 2                  <span class="comment"># 并行执行为 1</span></span><br><span class="line">      completions: 3                  <span class="comment"># 需要成功运行 1 次</span></span><br><span class="line">      ttlSecondsAfterFinished: 300   <span class="comment"># Job 在结束 3600s 之后，会被系统自动删除(无论执行成功还是失败)</span></span><br><span class="line">      <span class="comment">#activeDeadlineSeconds: 120      # 总活跃时间为 120s，包含运行 Pod时间+异常时间重试次数时间</span></span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          imagePullSecrets:</span><br><span class="line">          - name: harbor-auth</span><br><span class="line">          restartPolicy: OnFailure</span><br><span class="line">          containers:</span><br><span class="line">          - name: cronjob-container</span><br><span class="line">            image: harbor.inadm.com/inadm_kubernetes/redis-mq-consumer-job:v1.0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 检查</span><br><span class="line"><span class="comment"># kubectl describe -n inadm cronjobs.batch redis-consumer-cronjob</span></span><br><span class="line"><span class="comment"># kubectl logs -n inadm redis-consumer-cronjob-29219026-jt4fj </span></span><br><span class="line">// 在此发布消息</span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob05&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob06&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob07&quot;</span></span><br><span class="line">redis.inadm:6379&gt; rpush job2 <span class="string">&quot;inadm,cronjob08&quot;</span></span><br><span class="line"><span class="comment"># kubectl logs -n inadm redis-consumer-cronjob-29219026-hrjh</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-399.png" alt="upload successful"></p>
<h1 id="7-0-service"><a href="#7-0-service" class="headerlink" title="7.0 service"></a>7.0 <strong>service</strong></h1><ul>
<li><p>srevice 的作用：</p>
<ul>
<li>暴露流量：让用户可以通过 ServiceIP + ServicePort 访问对应后端的 Pod 应用</li>
<li>负载均衡：提供基于 4 层的 TCP&#x2F;IP 负载均衡，并不提供 HTTP&#x2F;HTTPS 等负载均衡</li>
<li>服务发现：当发现新增 Pod 则自动加入至 Service 的后端，如发现 Pod 异常则自动剔除 Service 后端</li>
</ul>
</li>
<li><p>service 工作逻辑：</p>
<ul>
<li><p>Service 持续监视 API-Server，监视 Service 标签选择器所匹配的后端 Pod，并实时跟踪这些 Pod 对象的变动情况，例如 IP 地址发生变化、或 Pod 对象新增与减少</p>
</li>
<li><p>不过 Service 并不直接与 Pod 简历关联关系，它们之间还有一个中间层 Endpoints，Endpoints 对象是由一个 IP 地址和端口组成的列表，这些 IP 地址和端口则来自于 Service 标签选择器所匹配到的 Pod，默认情况下，创建 Service 资源时，其关联的 Endpoints 对象会被自动创建</p>
</li>
<li><p>Service：用户通过 kbuectl 命令向 apiServer 发送创建 Service 请求，APIServer 收到后存入 etcd</p>
</li>
<li><p>ndpoints：获取 Service 所匹配的 Pod 地址，而后将信息写入与 Service 同名的 endpoints 资源中</p>
</li>
<li><p>kube-proxy：获取 Service 和 Endpoints 资源的变动，而后生存 iptables 、IPVS 规则，在本级执行</p>
</li>
<li><p>ptables：当用户请求 service_ip 时，使用 iptables 的 DNAT 技术将 ServiceIP 的请求调度至 endpoint 保存 ip 列表</p>
</li>
</ul>
</li>
<li><p>service 具体实现：</p>
<ul>
<li>在 k8s 中，Service 只是抽象的一个概念，真正起作用实现负载均衡规则的其实是 kube-proxy 这个进程。它在每个节点上都需要运行一个 kube-proxy，来完成负载均衡规则的创建<ul>
<li>创建 Service 资源后，会分配一个随机的 ServiceIP，返回给用户，然后写入 etcd</li>
<li>endpoints controller 负责生成和维护所有 endpoints，它会监听 Service 和 Pod 的状态，当 Pod 处于 running 且准备就绪时，endpoints controller 会将 Pod IP 更新到对应 Service 的 endpoints 对象中，然后写入 etcd</li>
<li>kube-proxy 通过 API-Server、Endpoints 的资源变动，一旦 Service 或 EndPoints 资源发生变化，kube-proxy 会将最新的信息转换为对应的 iptables、IPVS 访问规则，而后在本地主机执行</li>
<li>当客户端想要访问 Service 的时候，其实访问的就是本地节点上的 iptables、IPVS 规则，由他们路由到对应节点</li>
</ul>
</li>
</ul>
</li>
<li><p>service 资源类型：</p>
<ul>
<li>ClusterIP：通过集群的内部 IP 暴露服务，选择 ServiceIP 只能够在集群内部访问。这也是默认的 ServiceType</li>
<li>NodePort：NodePort 类型是对 ClusterIP 类型 Service 资源的扩展。它通过每个节点上的 IP 和端口接入集群外部流量，并分发给后端的 Pod 处理和响应。因此通过 &lt;节点IP&gt;:&lt;节点端口&gt;，可以从几圈外访问服务</li>
<li>oadBalance：这类 Service 依赖云厂商，需要通过云厂商调用 API 接口创建软件负载均衡将服务暴露到集群外部。当创建 LoadBalance 类型的 Service 对象时，它会在集群上自动创建一个 NodePort 类型的 Service。集群外部的请求流量会先路由该负载均衡，并由该负载均衡调度至各个节点的 NodePort</li>
<li>ExternalName：此类型不是用来定义如何访问集群内服务的，而是把集群外部的某些服务以 DNS、CNAME 方式映射到集群内，从而让集群内的 Pod 资源能够访问外部服务的一种实现方式</li>
</ul>
</li>
</ul>
<h2 id="7-1-配置示例"><a href="#7-1-配置示例" class="headerlink" title="7.1 配置示例"></a>7.1 <strong>配置示例</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: &lt;string&gt;</span><br><span class="line">  namespace: &lt;string&gt;</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: &lt;string&gt;              <span class="comment"># svc 类型，默认为 ClusterIP</span></span><br><span class="line">  selector:                   <span class="comment"># 标签选择器，用于匹配对应的 Pod</span></span><br><span class="line">  ports:                      <span class="comment"># svc 端口列表</span></span><br><span class="line">  - name: &lt;string&gt;            <span class="comment"># 端口名称</span></span><br><span class="line">    protocol: &lt;string&gt;        <span class="comment"># 协议，目前支持 TCP、UDP、SCTP、默认 TCP</span></span><br><span class="line">    port: &lt;<span class="built_in">integer</span>&gt;           <span class="comment"># svc 的端口号</span></span><br><span class="line">    targetPort: &lt;string&gt;      <span class="comment"># 后端目标进程的端口号或名称</span></span><br><span class="line">    nodePort: &lt;<span class="built_in">integer</span>&gt;       <span class="comment"># 节点端口号，仅适用于 NodePort 和 Loadbalancer 类型</span></span><br><span class="line">    externalTrafficPolicy:    <span class="comment"># 外部流量路由策略， local 表示当前节点处理，cluster 表示向集群范围内调度</span></span><br><span class="line">    internalTrafficPolicy:    <span class="comment"># 内部流量路由策略</span></span><br></pre></td></tr></table></figure>

<h3 id="7-1-1-ClusterIP"><a href="#7-1-1-ClusterIP" class="headerlink" title="7.1.1 ClusterIP"></a>7.1.1 <strong>ClusterIP</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-clusterip</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    role: web</span><br><span class="line">  <span class="comment">#clusterIP: 10.96.x.x         # 自定指定IP，建议还是由 Service 自动分配</span></span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8888</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f svc-clusterip.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="7-1-2-NodePort"><a href="#7-1-2-NodePort" class="headerlink" title="7.1.2 NodePort"></a>7.1.2 <strong>NodePort</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat svc-nodeport.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-nodeport</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    role: npt</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8888</span><br><span class="line">    targetPort: 80</span><br><span class="line">    nodePort: 32000</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f svc-nodeport.yaml</span></span><br></pre></td></tr></table></figure>


<p><img src="/images/pasted-400.png" alt="upload successful"></p>
<h3 id="7-1-3-ExternalName"><a href="#7-1-3-ExternalName" class="headerlink" title="7.1.3 ExternalName"></a>7.1.3 <strong>ExternalName</strong></h3><ul>
<li>当查询主机 SERVICE.NAMESPACE.svc.cluster.local 时，集群的 DNS 服务将返回一个值为 <a href="http://www.ink8s.com/">www.ink8s.com</a> 的 CNAME 记录访问这个服务的工作方式与其它的相同，唯一不同的是重定向发生在 DNS 层面</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat pod-test.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-test</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-test</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-externalname</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: ExternalName</span><br><span class="line">  externalName: blog.ink8s.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-test.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it pod-test -n inadm -- &quot;/bin/sh&quot;</span></span><br><span class="line">[root@pod-test /]<span class="variable">$dig</span> svc-externalname.inadm.svc.cluster.local @10.96.0.10 +short blog.ink8s.com</span><br></pre></td></tr></table></figure>

<h2 id="7-2-自定义endpoint"><a href="#7-2-自定义endpoint" class="headerlink" title="7.2 自定义endpoint"></a>7.2 <strong>自定义endpoint</strong></h2><ul>
<li>service 通过 selector 和 Pod 建立关联，k8s 会根据  关联到的 PodIP 信息组合成一个 endpoint。若 service 定义中没有 selector 字段，service 被创建时，endpoint controller 不会自动创建 endpoint</li>
<li>我们可以通过配置清单创建 service，而无需使用标签选择器，而后自行创建一个同名的 endpoint 对象，指定对应的 IP。这种一般用于将外部 MySQL、Redis 等应用引入 k8s 集群内部，让内部通过 service 的方式访问外部资源</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">// 1. 准备外部 MySQL 服务，并且允许远程用户访问权限</span><br><span class="line">    10.16.41.152:3306</span><br><span class="line"></span><br><span class="line">// 2. 创建 endpoints 资源清单、创建与 endpoint 同名的 service 资源清单</span><br><span class="line"><span class="comment"># cat svc-mysql-external-endpoint.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-test</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-test</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-mysql-external</span><br><span class="line">  namespace: inadm</span><br><span class="line">subsets:</span><br><span class="line">  - addresses:</span><br><span class="line">    - ip: 10.16.41.152      <span class="comment"># 外部 MySQL IP 地址</span></span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 3306            <span class="comment"># 外部 MySQL 运行的端口</span></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-mysql-external</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: ClusterIP</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 3306            <span class="comment"># 访问 service 的端口</span></span><br><span class="line">    targetPort: 3306      <span class="comment"># 后端应用的端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f svc-mysql-external-endpoint.yaml </span></span><br><span class="line"><span class="comment"># kubectl get ep -n inadm svc-mysql-external </span></span><br><span class="line"><span class="comment"># kubectl describe svc -n inadm svc-mysql-external </span></span><br><span class="line"></span><br><span class="line">// 使用 pod 访问 service，验证能否正常访问 MySQL 服务</span><br><span class="line"><span class="comment"># kubectl exec -it pod-tools -n inadm -- /bin/bash</span></span><br><span class="line">// 测试在 Pod 中可以使用2种方式连接外部MySQL服务</span><br><span class="line">[root@pod-test /]<span class="comment"># mysql -h 10.16.41.152 -P3306 -uroot -pink8s.com     		   # service_ip 地址方式</span></span><br><span class="line">[root@pod-test /]<span class="comment"># mysql -h svc-mysql-external.inadm -P3306 -uroot -pink8s.com     # fqdn 方式</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-401.png" alt="upload successful"></p>
<p><img src="/images/pasted-402.png" alt="upload successful"></p>
<h2 id="7-3-service相关字段"><a href="#7-3-service相关字段" class="headerlink" title="7.3 service相关字段"></a>7.3 <strong>service相关字段</strong></h2><h3 id="7-3-1-sessionAffinity"><a href="#7-3-1-sessionAffinity" class="headerlink" title="7.3.1 sessionAffinity"></a>7.3.1 <strong>sessionAffinity</strong></h3><ul>
<li>如果要将来自于特定客户端的连接调度至同一 Pod，可以使用 sessionAffinity 基于客户端的 IP 地址进行会话保持</li>
<li>还可以通过 sessionAffinityConfig.clientIP.timeoutSeconds 来设置最大会话停留时间。（默认 10800s，即 3H）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-demoapp.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: demoapp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: demoapp-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-session</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">  sessionAffinity: ClientIP         <span class="comment"># 配置 sessionAffinity 策略</span></span><br><span class="line">  sessionAffinityConfig:</span><br><span class="line">    clientIP:</span><br><span class="line">      timeoutSeconds: 60            <span class="comment"># 最大会话停留时间 60s</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f deploy-demoapp.yaml </span></span><br><span class="line"><span class="comment"># curl svc_ip/version		    # 60s 后，请求保持才会更新</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-403.png" alt="upload successful"></p>
<h3 id="7-3-2-externalTrafficPolicy"><a href="#7-3-2-externalTrafficPolicy" class="headerlink" title="7.3.2 externalTrafficPolicy"></a>7.3.2 <strong>externalTrafficPolicy</strong></h3><ul>
<li>外部流量策略：当外部用户通过 NodePort 请求 Service，是将外部流量路由到本地节点上的 Pod，或是路由到集群范围的 Pod<ul>
<li>Cluster(默认)：将用户请求路由到集群范围的所有 Pod 节点，具有良好的整体负载均衡</li>
<li>Local：仅会将流量调度至请求的目标节点本地运行的 Pod 对象之上，以减少网络跳跃，降低网络延迟，但当请求指向的节点本地不存在目标 Service 相关的 Pod 对象时直接丢弃该报文</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat svc-externaltraffice.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: demoapp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: demoapp-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-external-traffic</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  externalTrafficPolicy: Local            <span class="comment"># 默认 Cluster 节点</span></span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    nodePort: 32002</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f svc-externaltraffice.yaml </span></span><br><span class="line"></span><br><span class="line">// 外部请求，仅只能使用指定节点IP+PORT才能请求到指定节点的Pod上资源</span><br><span class="line">// 如果 10.10.21.160 node 节点没有 Pod，则无法访问这个节点</span><br><span class="line"><span class="comment"># curl http://10.16.41.125:32002/</span></span><br><span class="line"><span class="comment"># curl http://10.16.41.66:32002/</span></span><br><span class="line"><span class="comment"># curl http://10.16.41.196:32002/</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-406.png" alt="upload successful"></p>
<p><img src="/images/pasted-404.png" alt="upload successful"></p>
<h3 id="7-3-3-internalTrafficPolicy"><a href="#7-3-3-internalTrafficPolicy" class="headerlink" title="7.3.3 internalTrafficPolicy"></a>7.3.3 <strong>internalTrafficPolicy</strong></h3><ul>
<li>本地流量策略：当本地 Pod 对 Service 发起访问时，是将流量路由到本地节点上的 Pod，还是路由到集群范围的 Pod<ul>
<li>luster (默认)：将 Pod 的请求路由到集群范围的所有 Pod 节点，具有良好的整体负载均衡</li>
<li>Local：将请求路由到与发起方处于相同节点的端点，这种机制有助于节省开销，提升效率。但当请求指向的节点本地不存在的目标 Service 相关的 Pod 对象时直接丢弃该报文</li>
</ul>
</li>
<li>注意：在一个 service 上，当 <strong>externalTrafficePolicy</strong> 已设置为 Local 时，<strong>internalTrafficPolicy</strong> 则无法使用。换句话说，在一个集群的不同 service 上可以同时使用这2个特性，但在一个 service 上不行</li>
</ul>
<p><img src="/images/pasted-405.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat svc-internal.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: demoapp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: demoapp-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-internal-traffic</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  internalTrafficPolicy: Local      <span class="comment"># 默认 (Cluster)</span></span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f svc-internal.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-test -- /bin/bash</span></span><br><span class="line">[root@pod-test /]<span class="comment"># curl svc_ip/version</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-407.png" alt="upload successful"></p>
<h3 id="7-3-4-publishNotReadyAddresses"><a href="#7-3-4-publishNotReadyAddresses" class="headerlink" title="7.3.4 publishNotReadyAddresses"></a>7.3.4 <strong>publishNotReadyAddresses</strong></h3><ul>
<li>publishNotReadyAddresses: 表示 Pod 就绪探针探测失败，也不会将失败的 PodIP 加入 NotReadyAddresses 列表中</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat publishNRA.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-internal-traffic</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  publishNotReadyAddresses: <span class="literal">true</span></span><br><span class="line">  internalTrafficPolicy: Local      <span class="comment"># 默认 (Cluster)</span></span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f publishNRA.yaml </span></span><br><span class="line">// 当某个 Pod 就绪探针失败后，Pod 也不会被加入到 NotReadyAddresses 列表中，一般不太建议这样加</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-409.png" alt="upload successful"></p>
<h2 id="7-4-coredns-策略"><a href="#7-4-coredns-策略" class="headerlink" title="7.4 coredns 策略"></a>7.4 <strong>coredns 策略</strong></h2><ul>
<li>DNS 策略可以单独对 Pod 进行设定，在创建 Pod 时可以为其指定 DNS 的策略，最终配置会落在 Pod 的 &#x2F;etc&#x2F;resolv.conf 文件中，可以通过 pod.spec.dnsPlicy 字段设置 DNS 的策略</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">// 下面演示环境需要的配置</span><br><span class="line"><span class="comment"># cat coredns_plicy.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: deploy-demoapp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: demoapp-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-demoapp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f coredns_plicy.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="7-4-1-ClusterFirst"><a href="#7-4-1-ClusterFirst" class="headerlink" title="7.4.1 ClusterFirst"></a>7.4.1 <strong>ClusterFirst</strong></h3><ul>
<li>ClusterFirst (默认DNS策略)：表示 Pod 内的 DNS 使用集群中配置的 DNS 服务，简单来说就是使用 k8s 中的 coredns 服务进行域名解析。如果解析不成功，会使用当前 Pod 所在的宿主机 DNS 进行解析</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dns-clusterfirst.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-demoapp</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  dnsPolicy: ClusterFirst</span><br><span class="line">  containers:</span><br><span class="line">  - name: tools</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8000</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f dns-clusterfirst.yaml </span></span><br><span class="line"></span><br><span class="line">// 下条命令，只是为了确认容器中的 resolv.conf 是否为 k8s 集群中的 nameserver</span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-demoapp -- cat /etc/resolv.conf            # 容器中是默认的svc dns 解析，如果请求的是非容器中的解析，则会使用节点的 resolv.conf 的解析</span></span><br><span class="line">search inadm.svc.cluster.local svc.cluster.local cluster.local </span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm deploy-demoapp-c748d6844-8bjvq -- /bin/sh</span></span><br><span class="line"><span class="variable">$nslookup</span> svc-demoapp.inadm</span><br><span class="line"><span class="variable">$nslookup</span> www.ink8s.com</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-410.png" alt="upload successful"></p>
<h3 id="7-4-2-ClusterFirstWithHostNet"><a href="#7-4-2-ClusterFirstWithHostNet" class="headerlink" title="7.4.2 ClusterFirstWithHostNet"></a>7.4.2 <strong>ClusterFirstWithHostNet</strong></h3><ul>
<li>某些场景下，我们的 Pod 是用 HostNetwork 模式启动的，一旦使用 Hostnetwork 模式，那该 Pod 则会使用当前宿主机的 resolv.conf 来进行 DNS 查询，但如果任然继续使用 k8s 的 DNS 服务，那就将 dnsPolicy 设置为 ClusterFirestWithHostNet</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dns-withhostnet.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dns-withhostnet</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  hostNetwork: <span class="literal">true</span>                          <span class="comment"># 与当前 Pod 所在 NODE 节点共享 resolv.conf 配置</span></span><br><span class="line">  <span class="comment">#dnsPolicy: ClusterFirstWithHostNet        # 如果配置了这条，其实就等同于 ClusterFirst 默认策略了，相当于 hostNetwork 不需要配置。如果内网有 dns，待验证是否可以使用内网 dns</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: dns-withhostnet</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8000</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f dns-withhostnet.yaml</span></span><br><span class="line"></span><br><span class="line">// 此 Pod 采用 <span class="string">&quot;hostNetwork: true&quot;</span> 方式则仅调用了 宿主机的 resolv.conf 配置</span><br><span class="line"><span class="comment"># kubectl exec -it dns-withhostnet -n inadm -- cat /etc/resolv.conf</span></span><br><span class="line">nameserver 8.8.4.4</span><br><span class="line">nameserver 8.8.8.8</span><br><span class="line"></span><br><span class="line">// 如果启用了 <span class="string">&quot;dnsPolicy: ClusterFirstWithHostNet&quot;</span> 配置，则等同于 <span class="string">&quot;ClusterFirst&quot;</span> 配置(如果内网有 dns，待验证是否可以使用内网 dns)</span><br><span class="line"><span class="comment"># kubectl exec -it dns-withhostnet -n inadm -- cat /etc/resolv.conf</span></span><br><span class="line">search inadm.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<h3 id="7-4-3-Default"><a href="#7-4-3-Default" class="headerlink" title="7.4.3 Default"></a>7.4.3 <strong>Default</strong></h3><ul>
<li>默认使用宿主机的 resolv.conf 但可以使用 kubelet 的 –resolv-conf&#x3D;&#x2F;etc&#x2F;resolv.conf 来指定 DNS 解析文件地址</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dns-default.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dns-default</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  dnsPolicy: Default</span><br><span class="line">  containers:</span><br><span class="line">  - name: dns-default</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8000</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f dns-default.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -in inadm dns-default -- cat /etc/resolv.conf</span></span><br><span class="line">nameserver 8.8.4.4</span><br><span class="line">nameserver 8.8.8.8</span><br></pre></td></tr></table></figure>

<h3 id="7-4-4-None"><a href="#7-4-4-None" class="headerlink" title="7.4.4 None"></a>7.4.4 <strong>None</strong></h3><ul>
<li>空的 DNS 设置，这种方式一般用于自定义 DNS 配置的场景，往往需要和 dnsConfig 一起使用才可以达到自定义的 DNS 的目的</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// 如果生产环境不便于 apply 应用文件，则可以采用 edit 修改 svc 然后将 svc 文件调整好待下次更新时调整及下次 apply 永久生效</span><br><span class="line"><span class="comment"># cat dns-none.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dns-none</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  dnsPolicy: <span class="string">&quot;None&quot;</span></span><br><span class="line">  dnsConfig:</span><br><span class="line">    nameservers:</span><br><span class="line">      - 10.96.0.10</span><br><span class="line">      - 223.5.5.5</span><br><span class="line">    searches:</span><br><span class="line">      - cluster.local</span><br><span class="line">      - svc.luster.local</span><br><span class="line">      - default.svc.cluster.local</span><br><span class="line">      - inadm.svc.cluster.local</span><br><span class="line">      - ink8s.com</span><br><span class="line">    options:</span><br><span class="line">      - name: ndots</span><br><span class="line">        value: <span class="string">&quot;5&quot;</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: dns-none</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/tools:latest</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8000</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f dns-none.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm dns-none -- cat /etc/resolv.conf</span></span><br><span class="line">search cluster.local svc.luster.local default.svc.cluster.local inadm.svc.cluster.local ink8s.com</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">nameserver 223.5.5.5</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<h2 id="7-5-headless"><a href="#7-5-headless" class="headerlink" title="7.5 headless"></a>7.5 <strong>headless</strong></h2><ul>
<li>什么是 HeadLess：HeadlessService 也叫无头服务，就是创建的 Service 没有 ClusterIP，而是为 Service 所匹配的每个 Pod 都创建一条 DNS 的解析记录，这样每个 Pod 都有一个唯一的 DNS 名称标识身份，访问的格式如 [SERVICE_NAME.NAMESPACE.svc.cluster.local]</li>
<li>HeadLess 作用：像 elasticsearch、mongodb、kafka 等分布式服务，在做集群初始化时，配置文件中要写上集群中所有节点的 IP(或是域名)但 Pod 是没有固定 IP 的，所以配置文件里写 DNS 名称是最合适的。<ul>
<li>那为什么不用 service，因为 service 作为 Pod 前置负载均衡，一般是为一组相同的后端 Pod 提供访问入口，而且 service 的 selector 也没有办法区分同一组 Pod 的 不同身份</li>
<li>但是我们可以使用 statefulset 控制器，它在创建每个 Pod 的时候，能为每个 Pod 做一个编号，就是为了能区分这一组 Pod 的不同角色，各个节点的角色不会变的混乱，然后再创建 headless service 资源，集群内的节点通过 Pod名称+序号.service名称，来进行彼此间通信的，只要序号不变，访问就不会出错</li>
<li>{statefulSet name}-{编号}.{headless service}.{namespace}.svc.cluster.local</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat headless.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-headless</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: <span class="string">&quot;None&quot;</span>                   <span class="comment"># 设置为 None，表示无头服务</span></span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  serviceName: <span class="string">&quot;svc-headless&quot;</span>        <span class="comment"># 要与 Headless 名称保持一致</span></span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.16</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">// 进入测试 Pod 验证是轮询的，能获得 web-0 和 web-1 的 IP</span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-tools -- /bin/bash</span></span><br><span class="line">ping svc-headless.inadm.svc.cluster.local</span><br><span class="line"></span><br><span class="line">// 单独解析 Pod 的 DNS</span><br><span class="line"><span class="comment"># ping web-0.svc-headless.inadm.svc.cluster.local</span></span><br><span class="line"><span class="comment"># ping web-1.svc-headless.inadm.svc.cluster.local</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-411.png" alt="upload successful"></p>
<p><img src="/images/pasted-412.png" alt="upload successful"></p>
<h1 id="8-0-ingress"><a href="#8-0-ingress" class="headerlink" title="8.0 ingress"></a>8.0 <strong>ingress</strong></h1><h2 id="8-1-ingress-资源清单"><a href="#8-1-ingress-资源清单" class="headerlink" title="8.1 ingress 资源清单"></a>8.1 <strong>ingress 资源清单</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1        <span class="comment"># 资源所属的 API 群组和版本</span></span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name:  &lt;string&gt;</span><br><span class="line">  namespace: &lt;string&gt;</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span>             <span class="comment"># 适配 Ingress 控制器类别，必须明确指定</span></span><br><span class="line">  rules: &lt;[]Object&gt;                     <span class="comment"># Ingress 规则列表</span></span><br><span class="line">  - host: &lt;string&gt;                      <span class="comment"># 虚拟主机的 FQDN，俗称域名</span></span><br><span class="line">    http: &lt;Object&gt;</span><br><span class="line">      paths: &lt;[]Object&gt;                 <span class="comment"># 虚拟主机 PATH 定义的列表，有 path 和 backend 组成</span></span><br><span class="line">      - path: &lt;string&gt;                  <span class="comment"># 匹配以什么开头，类似 nginx 中 location 的作用</span></span><br><span class="line">        pathType: &lt;string&gt;              <span class="comment"># Prefix 前缀匹配，不区分大小写 Exact 精确匹配 URL，区分大小写</span></span><br><span class="line">        backend: &lt;Object&gt;</span><br><span class="line">          service: &lt;Object&gt;             <span class="comment"># 关联后端 svc</span></span><br><span class="line">            name: &lt;string&gt;              <span class="comment"># 后端 svc 名称</span></span><br><span class="line">            port: &lt;Object&gt;              <span class="comment"># 后端 svc 端口</span></span><br><span class="line">              name: &lt;string&gt;            <span class="comment"># 端口名称</span></span><br><span class="line">              number: &lt;integr&gt;          <span class="comment"># 端口号</span></span><br></pre></td></tr></table></figure>

<h2 id="8-2-ingress-基于-url-实现路由"><a href="#8-2-ingress-基于-url-实现路由" class="headerlink" title="8.2 ingress 基于 url 实现路由"></a>8.2 <strong>ingress 基于 url 实现路由</strong></h2><ul>
<li>将来自同一域名，不同 URL 请求调度到不同 svc</li>
</ul>
<p><img src="/images/pasted-421.png" alt="upload successful"></p>
<ul>
<li>因后端 Pod 无法处理 &#x2F;app 这样的接口，所以需要调整代理到后端的路径<ul>
<li>默认 URL：用户请求 url.inadm.com&#x2F;app，代理到后端请求也会带上 &#x2F;app，后端无法处理该 url，就会报错</li>
<li>修改 URL：用户请求 url.inad.com&#x2F;app，代理到后端后，将请求的 &#x2F;app 删除，url 为 url.inadm.com&#x2F;</li>
</ul>
</li>
</ul>
<h3 id="8-2-1-demoapp-应用"><a href="#8-2-1-demoapp-应用" class="headerlink" title="8.2.1 demoapp 应用"></a>8.2.1 <strong>demoapp 应用</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ing-deploy-web.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-web-url</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: url-web</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: url-web</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: url-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-web-url</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: url-web</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-deploy-web.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="8-2-2-tomcat-应用"><a href="#8-2-2-tomcat-应用" class="headerlink" title="8.2.2 tomcat 应用"></a>8.2.2 <strong>tomcat 应用</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ing-deploy-tomcat.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-tomcat-url</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: url-tomcat</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: url-tomcat</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: url-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/tomcat:9.0.63</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        lifecycle:</span><br><span class="line">          postStart:</span><br><span class="line">            <span class="built_in">exec</span>:</span><br><span class="line">              <span class="built_in">command</span>:</span><br><span class="line">              - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">              - <span class="string">&quot;-c&quot;</span></span><br><span class="line">              - <span class="string">&quot;cp -rf /usr/local/tomcat/webapps.dist/* /usr/local/tomcat/webapps&quot;</span></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-tomcat-url</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: url-tomcat</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-deploy-tomcat.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="8-2-3-ing-配置"><a href="#8-2-3-ing-配置" class="headerlink" title="8.2.3 ing 配置"></a>8.2.3 <strong>ing 配置</strong></h3><ul>
<li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ImplementationSpecific 说明</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ing-web-tomcat.yaml </span></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: url-ing</span><br><span class="line">  namespace: inadm</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io/use-regex: <span class="string">&quot;true&quot;</span>   <span class="comment"># 关键：启用正则</span></span><br><span class="line">    nginx.ingress.kubernetes.io/rewrite-target: /<span class="variable">$2</span>   <span class="comment"># 配置 Rewrite 规则;捕获组$2对应(.*)</span></span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: url.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /app(/|$)(.*)</span><br><span class="line">        <span class="comment">#pathType: Prefix</span></span><br><span class="line">        pathType: ImplementationSpecific  <span class="comment"># 使用正则时必须用此类型</span></span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: svc-web-url</span><br><span class="line">            port:</span><br><span class="line">              number: 80</span><br><span class="line">      - path: /java(/|$)(.*)</span><br><span class="line">        <span class="comment">#pathType: Prefix</span></span><br><span class="line">        pathType: ImplementationSpecific  <span class="comment"># 使用正则时必须用此类型</span></span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: svc-tomcat-url</span><br><span class="line">            port:</span><br><span class="line">              number: 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-web-tomcat.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="8-2-4-验证"><a href="#8-2-4-验证" class="headerlink" title="8.2.4 验证"></a>8.2.4 <strong>验证</strong></h3><ul>
<li>浏览器访问：<ul>
<li><a target="_blank" rel="noopener" href="http://url.inadm.com/app">http://url.inadm.com/app</a></li>
<li><a target="_blank" rel="noopener" href="http://url.inadm.com/java/index.jsp">http://url.inadm.com/java/index.jsp</a></li>
</ul>
</li>
</ul>
<p><img src="/images/pasted-422.png" alt="upload successful"></p>
<p><img src="/images/pasted-423.png" alt="upload successful"></p>
<h2 id="8-3-多域名"><a href="#8-3-多域名" class="headerlink" title="8.3 多域名"></a>8.3 <strong>多域名</strong></h2><ul>
<li>将来自不同域名的请求调度到不同 svc</li>
</ul>
<p><img src="/images/pasted-424.png" alt="upload successful"></p>
<h3 id="8-3-1-app-应用"><a href="#8-3-1-app-应用" class="headerlink" title="8.3.1 app 应用"></a>8.3.1 <strong>app 应用</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ing-app-domian.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-app-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: app-domain</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: app-domain</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: app-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-app-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: app-domain</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-app-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: app.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: <span class="string">&quot;/&quot;</span></span><br><span class="line">        pathType: Prefix</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: svc-app-domain</span><br><span class="line">            port:</span><br><span class="line">              number: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-app-domian.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="8-3-2-tomcat-应用"><a href="#8-3-2-tomcat-应用" class="headerlink" title="8.3.2 tomcat 应用"></a>8.3.2 <strong>tomcat 应用</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ing-tomcat-domian.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-tomcat-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat-domain</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat-domain</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: domain-container</span><br><span class="line">        image: harbor.inadm.com/inadm_kubernetes/tomcat:9.0.63</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        lifecycle:</span><br><span class="line">          postStart:</span><br><span class="line">            <span class="built_in">exec</span>:</span><br><span class="line">              <span class="built_in">command</span>:</span><br><span class="line">              - <span class="string">&quot;/bin/bash&quot;</span></span><br><span class="line">              - <span class="string">&quot;-c&quot;</span></span><br><span class="line">              - <span class="string">&quot;cp -rf /usr/local/tomcat/webapps.dist/* /usr/local/tomcat/webapps&quot;</span></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-tomcat-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-domain</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-tomcat-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: <span class="string">&quot;/&quot;</span></span><br><span class="line">        pathType: Prefix</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: svc-tomcat-domain</span><br><span class="line">            port:</span><br><span class="line">              number: 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-tomcat-domian.yaml </span></span><br></pre></td></tr></table></figure>

<h3 id="8-3-2-验证"><a href="#8-3-2-验证" class="headerlink" title="8.3.2 验证"></a>8.3.2 <strong>验证</strong></h3><ul>
<li>浏览器访问:<ul>
<li><a target="_blank" rel="noopener" href="http://app.inadm.com/">http://app.inadm.com</a></li>
<li><a target="_blank" rel="noopener" href="http://tomcat.inadm.com/">http://tomcat.inadm.com</a></li>
</ul>
</li>
</ul>
<p><img src="/images/pasted-425.png" alt="upload successful"></p>
<p><img src="/images/pasted-426.png" alt="upload successful"></p>
<p><img src="/images/pasted-427.png" alt="upload successful"></p>
<h2 id="8-4-ing-实现-https"><a href="#8-4-ing-实现-https" class="headerlink" title="8.4 ing 实现 https"></a>8.4 <strong>ing 实现 https</strong></h2><ul>
<li>在 ingress 中引用 secret 资源，然后告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡的通道</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// 创建 secrets (证书文件提前准备好)</span><br><span class="line"><span class="comment"># kubectl create secret tls app-inadm-com -n inadm --key=app.inadm.com.key --cert=app.inadm.com.pem</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat ing-app-tls.yaml 			# 以 &quot;8.3.1 app应用&quot;为例</span></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-app-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  tls:                                  <span class="comment"># https</span></span><br><span class="line">  - hosts:</span><br><span class="line">    - app.inadm.com</span><br><span class="line">    secretName: app-inadm-com           <span class="comment"># secrets 资源名称</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: app.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: <span class="string">&quot;/&quot;</span></span><br><span class="line">        pathType: Prefix</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: svc-app-domain         <span class="comment"># svc name</span></span><br><span class="line">            port:</span><br><span class="line">              number: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-app-tls.yaml</span></span><br></pre></td></tr></table></figure>
<ul>
<li>浏览器访问: <a target="_blank" rel="noopener" href="https://app.inadm.com/">https://app.inadm.com</a></li>
</ul>
<h2 id="8-5-ing-自定义配置"><a href="#8-5-ing-自定义配置" class="headerlink" title="8.5 ing 自定义配置"></a>8.5 <strong>ing 自定义配置</strong></h2><ul>
<li>annotations: 局部  –&gt; server {} &#x3D;&#x3D;&gt; 认证、限速 …</li>
<li>configmap: 全局 –&gt; http {} &#x3D;&#x3D;&gt; 参数优化、压缩 …</li>
</ul>
<h3 id="8-5-1-自定义-basic-认证"><a href="#8-5-1-自定义-basic-认证" class="headerlink" title="8.5.1 自定义 basic 认证"></a>8.5.1 <strong><a target="_blank" rel="noopener" href="https://kubernetes.github.io/ingress-nginx/examples/auth/basic/">自定义 basic 认证</a></strong></h3><ul>
<li><a target="_blank" rel="noopener" href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">参考地址</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum -y install httpd-tools</span></span><br><span class="line"><span class="comment"># htpasswd -c auth admin                # 生成一个 auth 文件,名字必须是 auth</span></span><br><span class="line">New password: ink8s.com</span><br><span class="line">Re-<span class="built_in">type</span> new password: ink8s.com</span><br><span class="line"><span class="comment"># kubectl create secret generic basic-auth --from-file=auth -n inadm</span></span><br><span class="line"><span class="comment"># kubectl get secret basic-auth -n inadm -o yaml</span></span><br><span class="line"><span class="comment"># cat ing-auth-basic.yaml</span></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-tomcat-domain</span><br><span class="line">  namespace: inadm</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io/auth-type: basic                    <span class="comment"># 认证类型</span></span><br><span class="line">    nginx.ingress.kubernetes.io/auth-secret: basic-auth             <span class="comment"># 包含用户和密码的 secret 资源名称</span></span><br><span class="line">    nginx.ingress.kubernetes.io/auth-realm: <span class="string">&#x27;Please User password&#x27;</span>  <span class="comment"># 要显示的信息</span></span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: <span class="string">&quot;/&quot;</span></span><br><span class="line">        pathType: Prefix</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: svc-tomcat-domain         <span class="comment"># svc name</span></span><br><span class="line">            port:</span><br><span class="line">              number: 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f ing-auth-basic.yaml </span></span><br><span class="line">// http://tomcat.inadm.com</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-428.png" alt="upload successful"></p>
<h3 id="8-5-2-全局参数优化"><a href="#8-5-2-全局参数优化" class="headerlink" title="8.5.2 全局参数优化"></a>8.5.2 <strong><a target="_blank" rel="noopener" href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/">全局参数优化</a></strong></h3><ul>
<li>如果相对 ingress 进行全局配置，可以通过 configmap 资源来实现，Pod 默认加载了一个 ConfigMap 资源 “ingress-nginx”</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get cm -n ingress-nginx ingress-nginx-controller</span></span><br><span class="line">// 需要注意下划线都为中横线</span><br><span class="line"><span class="comment"># kubectl edit configmaps ingress-nginx-controller -n ingress-nginx</span></span><br><span class="line">data:</span><br><span class="line">  proxy-connect-timeout: <span class="string">&quot;120&quot;</span>            <span class="comment"># 与代理服务连接的超时时间，默认75s</span></span><br><span class="line">  proxy-read-timeout: <span class="string">&quot;120&quot;</span>               <span class="comment"># 从代理服务器读取响应的超时（秒）</span></span><br><span class="line">  upstream-keepalive-connections: <span class="string">&quot;500&quot;</span>   <span class="comment"># 缓存到后端的最大空闲连接数，默认320</span></span><br><span class="line">  upstream-keepalive-requests: <span class="string">&quot;1000&quot;</span>     <span class="comment"># 长连接最大支持的请求数，达到后则关闭连接，默认10000</span></span><br><span class="line">  upstream-keepalive-timeout: <span class="string">&quot;120&quot;</span>       <span class="comment"># 后端空闲连接数最大保留多长时间，默认60s</span></span><br><span class="line">  max-worker-connections: <span class="string">&quot;65535&quot;</span>         <span class="comment"># worker最大连接数，默认16384</span></span><br><span class="line">  max-worker-open-files: <span class="string">&quot;10240&quot;</span>          <span class="comment"># 进程可以打开的最大文件数。默认1024</span></span><br><span class="line">  ssl-protocols: <span class="string">&quot;TLSv1.2 TLSv1.3&quot;</span></span><br><span class="line">  ssl-session-cache: <span class="string">&quot;true&quot;</span>               <span class="comment"># 开启ssl缓存；</span></span><br><span class="line">  ssl-session-cache-size: <span class="string">&quot;30m&quot;</span>           <span class="comment"># ssl缓存大小；</span></span><br><span class="line">  ssl-session-timeout: <span class="string">&quot;1024m&quot;</span>            <span class="comment"># ssl缓存的超时时间</span></span><br><span class="line"></span><br><span class="line">// 验证</span><br><span class="line"><span class="comment"># kubectl get pod -n ingress-nginx        # 找到 ingress-nginx-controller POD 名</span></span><br><span class="line"><span class="comment"># kubectl exec -it -n ingress-nginx ingress-nginx-controller-6mv4t -- grep --color &quot;ssl_session_timeout&quot; /etc/nginx/nginx.conf </span></span><br><span class="line"><span class="comment"># kubectl exec -it -n ingress-nginx ingress-nginx-controller-6mv4t -- grep --color &quot;connections&quot; /etc/nginx/nginx.conf</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-429.png" alt="upload successful"></p>
<p><img src="/images/pasted-430.png" alt="upload successful"></p>
<h1 id="9-0-configmap"><a href="#9-0-configmap" class="headerlink" title="9.0 configmap"></a>9.0 <strong>configmap</strong></h1><h2 id="9-1-创建-cm"><a href="#9-1-创建-cm" class="headerlink" title="9.1 创建 cm"></a>9.1 <strong>创建 cm</strong></h2><h3 id="9-1-1-命令创建-cm"><a href="#9-1-1-命令创建-cm" class="headerlink" title="9.1.1 命令创建 cm"></a>9.1.1 <strong>命令创建 cm</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 1.使用 kubectl create configmap 命令 使用 --from-literal 选项给出键值对来创建 ConfigMap</span><br><span class="line"><span class="comment"># kubectl create configmap nginx-command-cm --from-literal=nginx.host=&#x27;0.0.0.0&#x27; --from-literal=nginx.port=&#x27;8899&#x27; -n inadm</span></span><br><span class="line"></span><br><span class="line">// 2.可以看到，cm 资源没有 spec 和 status，而是直接使用 data 字段嵌套数据</span><br><span class="line"><span class="comment"># kubectl get cm -n inadm nginx-command-cm -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  nginx.host: 0.0.0.0</span><br><span class="line">  nginx.port: <span class="string">&quot;8899&quot;</span></span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: <span class="string">&quot;xxx&quot;</span></span><br><span class="line">  name: nginx-command-cm</span><br><span class="line">  namespace: inadm</span><br><span class="line">  resourceVersion: <span class="string">&quot;2100281&quot;</span></span><br><span class="line">  uid: 52a58946-3fb9-4db0-91e2-27979a55757f</span><br></pre></td></tr></table></figure>

<h3 id="9-1-2-文件创建-cm"><a href="#9-1-2-文件创建-cm" class="headerlink" title="9.1.2 文件创建 cm"></a>9.1.2 <strong>文件创建 cm</strong></h3><ul>
<li>cm 资源也可以为应用程序提供大段配置，这些大段配置通常保存在一个或多个文件中，可以使用 kubectl create cm 命令，通过 –from-file 选项一次加载一个配置文件的内容为指定的键值。默认文件为 key，文件内容为 values</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cm-nginx-server.conf</span></span><br><span class="line">server &#123;</span><br><span class="line">    listen  80;</span><br><span class="line">    server_name app.inadm.com;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        root    /usr/share/nginx/html;</span><br><span class="line">        index   index.html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.cfg;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat cm-nginx-status.cfg</span></span><br><span class="line">location /ngx_status &#123;</span><br><span class="line">    stub_status;</span><br><span class="line">    access_log off;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 一个直接使用 cm-nginx-server.conf 作为 key 名称，另一个 status.cfg 作为键名称</span><br><span class="line"><span class="comment"># kubectl create cm nginx-conf --from-file=cm-nginx-server.conf --from-file=status.cfg=cm-nginx-status.cfg -n inadm</span></span><br><span class="line"><span class="comment"># kubectl describe -n inadm cm nginx-conf</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-435.png" alt="upload successful"></p>
<h3 id="9-1-3-目录创建-cm"><a href="#9-1-3-目录创建-cm" class="headerlink" title="9.1.3 目录创建 cm"></a>9.1.3 <strong>目录创建 cm</strong></h3><ul>
<li>对于配置文件较多且无需自定义键名称的场景，可以直接在 kubectl cm 命令的 –from-file 选项上附加一个目录路径就能将该目录下的所有文件创建于同一个 cm 资源中，各文件名即为键名称</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ls k8s-cm 目录中包含如下3个文件</span></span><br><span class="line">    cm-nginx-gzip.cfg</span><br><span class="line">    cm-nginx-server.conf    <span class="comment"># </span></span><br><span class="line">    cm-nginx-status.cfg     <span class="comment"># 基于上面以创建的文件继续使用</span></span><br><span class="line"><span class="comment"># cat cm-nginx-gzip.cfg</span></span><br><span class="line">    gzip    on;</span><br><span class="line"><span class="comment"># kubectl create cm cm-nginx-conf-dir --from-file=../k8s-cm/ -n inadm        # 目录中所有文件都保存到 cm-nginx-conf-dir 对象中</span></span><br></pre></td></tr></table></figure>

<h3 id="9-1-4-yaml-文件创建-cm"><a href="#9-1-4-yaml-文件创建-cm" class="headerlink" title="9.1.4 yaml 文件创建 cm"></a>9.1.4 <strong>yaml 文件创建 cm</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cm-app-config.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-app-config</span><br><span class="line">  namespace: inadm</span><br><span class="line">data:</span><br><span class="line">  host: 0.0.0.0</span><br><span class="line">  port: <span class="string">&quot;8888&quot;</span></span><br><span class="line">  nginx-server.conf: |</span><br><span class="line">    server &#123;</span><br><span class="line">        listen  80;</span><br><span class="line">        server_name app.inadm.com;</span><br><span class="line">    </span><br><span class="line">        location / &#123;</span><br><span class="line">            root    /usr/share/nginx/html;</span><br><span class="line">            index   index.html;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        include /etc/nginx/conf.d/*.cfg;</span><br><span class="line">    &#125;</span><br><span class="line">  nginx-status.conf: |</span><br><span class="line">    location /ngx_status &#123;</span><br><span class="line">        stub_status;</span><br><span class="line">        access_log off;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f cm-app-config.yaml </span></span><br><span class="line"><span class="comment"># kubectl describe -n inadm cm cm-app-config</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-433.png" alt="upload successful"></p>
<h2 id="9-2-引用-cm"><a href="#9-2-引用-cm" class="headerlink" title="9.2 引用 cm"></a>9.2 <strong>引用 cm</strong></h2><ul>
<li>通过环境变量引用 cm 键值</li>
<li>Pod 清单中除了使用 value 字段直接给定变量之外，还支持 valueFrom 字段嵌套 configMapKeyRef 来引用 ConfigMap 对象的键值，具体格式如下</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">env</span>:</span><br><span class="line">- name: &lt;string&gt;            <span class="comment"># 要赋值的环境变量名称</span></span><br><span class="line">  valueFrom:                <span class="comment"># 定义变量的引用</span></span><br><span class="line">    configMapkeyRef:        <span class="comment"># 变量来自于 configmap 对象</span></span><br><span class="line">      name: &lt;string&gt;        <span class="comment"># configmap 对象的名称(因为有很多 cm，需要制定具体的名称)</span></span><br><span class="line">      key: &lt;string&gt;         <span class="comment"># cm 的键名称</span></span><br><span class="line">// 这种方式赋值环境变量与直接赋值环境变量方式并无区别，它们都可以用于容器的启动脚本或直接传递给容器引用等</span><br></pre></td></tr></table></figure>

<h3 id="9-2-1-env-引用变量"><a href="#9-2-1-env-引用变量" class="headerlink" title="9.2.1 env 引用变量"></a>9.2.1 <strong>env 引用变量</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cm-env-conf.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-var-conf</span><br><span class="line">  namespace: inadm</span><br><span class="line">data:</span><br><span class="line">  demoapp.host: <span class="string">&quot;0.0.0.0&quot;</span></span><br><span class="line">  demoapp.port: <span class="string">&quot;8888&quot;</span></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-env-app</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: cm-pod-env</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: HOST</span><br><span class="line">      valueFrom:</span><br><span class="line">        configMapKeyRef:</span><br><span class="line">          name: cm-var-conf</span><br><span class="line">          key: demoapp.host</span><br><span class="line">    - name: PORT</span><br><span class="line">      valueFrom:</span><br><span class="line">        configMapKeyRef:</span><br><span class="line">          name: cm-var-conf</span><br><span class="line">          key: demoapp.port</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f cm-env-conf.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -n inadm cm-env-app -- netstat -nlaput            # 验证</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-434.png" alt="upload successful"></p>
<h3 id="9-2-2-挂载卷引用-cm"><a href="#9-2-2-挂载卷引用-cm" class="headerlink" title="9.2.2 挂载卷引用 cm"></a>9.2.2 <strong>挂载卷引用 cm</strong></h3><ul>
<li>使用环境变量方式导入 cm 对象中来源较长的文件内容，会导致占据过多的内存空间，同时也不支持内容的动态更新。</li>
<li>其次该类数据主要用于为容器提供配置文件，所以将其内容直接通过挂载方式进行引用，会是一种更好的选择</li>
<li>引用了 9.1.3</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// 引用整个存储卷：将 cm 对象的每个键名转为容器挂载点路径下的一个文件名，所以每个建明应该设计为对容器应用加载的配置文件名称</span><br><span class="line">// 1.启动一个 nginx_pod，然后将此前创建的 cm-nginx-conf-dir 引用至容器的 /etc/nginx/conf.d 目录中</span><br><span class="line"><span class="comment"># cat cm-nginx-all-volume.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-volume-all-cm</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: cm-nginx-volume</span><br><span class="line">    image: nginx</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    volumeMounts:                           <span class="comment"># 将 nginx-conf-dir 挂载到 /etc/nginx/conf.d 目录下</span></span><br><span class="line">    - name: nginx-conf-dir</span><br><span class="line">      mountPath: /etc/nginx/conf.d/</span><br><span class="line">  volumes:                                  <span class="comment"># nginx-conf-dir 内容来源于 cm 中的 cm-nginx-conf-dir 资源</span></span><br><span class="line">  - name: nginx-conf-dir</span><br><span class="line">    configMap:</span><br><span class="line">      name: cm-nginx-conf-dir               <span class="comment"># cm-nginx-conf-dir 资源中引用了 3 个配置文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f cm-nginx-all-volume.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it nginx-volume-all-cm -n inadm -- ls /etc/nginx/conf.d/</span></span><br><span class="line">    cm-nginx-gzip.cfg  cm-nginx-server.conf  cm-nginx-status.cfg</span><br><span class="line"><span class="comment"># curl pod_ip/ngx_status            # 访问 ngx_status，看是否可以打开状态页</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-436.png" alt="upload successful"></p>
<h3 id="9-2-3-引用存储卷部分键值"><a href="#9-2-3-引用存储卷部分键值" class="headerlink" title="9.2.3 引用存储卷部分键值"></a>9.2.3 <strong>引用存储卷部分键值</strong></h3><ul>
<li>有些应用场景中，用户可能期望仅向容器中挂载指定几个键，例如前面创建名为 cm-app-config 里面有 4 个键，其中 host、port 能为 demoapp 容器定义监听地址及端口，而 nginx-server.conf 、nginx-status.conf 能为 nginx 提供一个虚拟主机站点以及该虚拟站点的状态信息</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cm-app-config.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-app-config</span><br><span class="line">  namespace: inadm</span><br><span class="line">data:</span><br><span class="line">  host: 0.0.0.0             <span class="comment"># key: value</span></span><br><span class="line">  port: <span class="string">&quot;8888&quot;</span>              <span class="comment"># key: value</span></span><br><span class="line">  nginx-server.conf: |      <span class="comment"># key</span></span><br><span class="line">    server &#123;</span><br><span class="line">        listen  80;</span><br><span class="line">        server_name app.inadm.com;</span><br><span class="line">    </span><br><span class="line">        location / &#123;</span><br><span class="line">            root    /usr/share/nginx/html;</span><br><span class="line">            index   index.html;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        include /etc/nginx/conf.d/*.cfg;</span><br><span class="line">    &#125;</span><br><span class="line">  nginx-status.conf: |</span><br><span class="line">    location /ngx_status &#123;</span><br><span class="line">        stub_status;</span><br><span class="line">        access_log off;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cm-nginx-demoapp.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: demoapp-nginx-cm</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: cm-nginx-containers</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.16</span><br><span class="line">    <span class="comment"># imagePullPolicy: IfNotPresent</span></span><br><span class="line">    imagePullPolicy: Always</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: cm-ngx-conf</span><br><span class="line">      mountPath: /etc/nginx/conf.d/</span><br><span class="line">  - name: cn-demoapp-containers</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: PORT</span><br><span class="line">      valueFrom:</span><br><span class="line">        configMapKeyRef:</span><br><span class="line">          name: cm-app-config</span><br><span class="line">          key: port</span><br><span class="line">    - name: HOST</span><br><span class="line">      valueFrom:</span><br><span class="line">        configMapKeyRef:</span><br><span class="line">          name: cm-app-config</span><br><span class="line">          key: host</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cm-ngx-conf</span><br><span class="line">    configMap:</span><br><span class="line">      name: cm-app-config               <span class="comment"># cm-ngx-conf 引用 cm-app-config 的 cm 资源</span></span><br><span class="line">      items:                            <span class="comment"># 引用部分键采用 items</span></span><br><span class="line">      - key: nginx-server.conf</span><br><span class="line">        path: app.inadm.com.conf        <span class="comment"># 对应的键在挂载点目录映射的文件名称(必写)</span></span><br><span class="line">        mode: 0644                      <span class="comment"># 文件权限</span></span><br><span class="line">      - key: nginx-status.conf</span><br><span class="line">        path: nginx-status.cfg          <span class="comment"># include 引用的 *.cfg 所以这里必须是 .cfg 结尾</span></span><br><span class="line">        mode: 0644</span><br><span class="line"></span><br><span class="line">// 验证</span><br><span class="line"><span class="comment"># curl http://POD_IP/ngx_status</span></span><br><span class="line"><span class="comment"># kubectl exec -it demoapp-nginx-cm -c cn-demoapp-containers -n inadm -- netstat -nlaput</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-437.png" alt="upload successful"></p>
<h3 id="9-2-4-引用存储卷单个键值"><a href="#9-2-4-引用存储卷单个键值" class="headerlink" title="9.2.4 引用存储卷单个键值"></a>9.2.4 <strong>引用存储卷单个键值</strong></h3><ul>
<li>前面两种方式中，无论是装在 cm 对象中的所有文件还是部分文件，挂载点目录下原有的文件都会被隐藏。(打开刚在创建的 nginx 容器验证，看默认的 default.conf 配置文件是否还存在)</li>
<li>对于期望将 cm 对象提供的配置文件补充在挂载点目录下的需求来说，这种方法难以实现，好在我们可以通过容器上的 volumeMounts 字段 subpath 来解决</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// 1.运行一个 nginx_Pod，将 cm-app-config 中 nginx-server.conf、nginx-status.conf 挂载进来测试</span><br><span class="line"><span class="comment"># cat cm-nginx-subpath.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-nginx-pod-subpath</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cm-nginx-volume-subpath</span><br><span class="line">    configMap:</span><br><span class="line">      name: cm-app-config</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: cm-nginx-subpath-containers</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.16</span><br><span class="line">    imagePullPolicy: Always</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: cm-nginx-volume-subpath</span><br><span class="line">      mountPath: /etc/nginx/conf.d/app.inadm.com.conf               <span class="comment"># 挂载容器对应的路径(属于附加，不是覆盖)</span></span><br><span class="line">      subPath: nginx-server.conf</span><br><span class="line">    - name: cm-nginx-volume-subpath</span><br><span class="line">      mountPath: /etc/nginx/conf.d/app.inadm.com.cfg</span><br><span class="line">      subPath: nginx-status.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl exec -it cm-nginx-pod-subpath -c cm-nginx-subpath-containers -n inadm -- ls -l /etc/nginx/conf.d/	# 可以看到 default.conf 原文件</span></span><br><span class="line"><span class="comment"># curl HHost:app.inadm.com http://POD_IP/ngx_status</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-438.png" alt="upload successful"></p>
<h2 id="9-3-redis-结合-cm"><a href="#9-3-redis-结合-cm" class="headerlink" title="9.3 redis 结合 cm"></a>9.3 <strong>redis 结合 cm</strong></h2><ul>
<li>使用 redis 配置的值创建一个 cm 文档</li>
<li>创建一个 redis_pod，挂载并使用创建的 cm</li>
<li>验证配置已经被正确应用</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat cm-redis.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-redis</span><br><span class="line">  namespace: inadm</span><br><span class="line">data:</span><br><span class="line">  redis-cfg: |</span><br><span class="line">    <span class="built_in">bind</span> 0.0.0.0</span><br><span class="line">    port 6380</span><br><span class="line">    maxclients 200000</span><br><span class="line">    maxmemory 2mb</span><br><span class="line">    requirepass ink8s.com</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-pod-redis</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: redis-server-container</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/redis:latest</span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">    - redis-server</span><br><span class="line">    - <span class="string">&quot;/redis-master/redis.conf&quot;</span></span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: cm-redis-config                         <span class="comment"># cm-redis-config 挂载到 /redis-master/redis.conf</span></span><br><span class="line">      mountPath: /redis-master/redis.conf</span><br><span class="line">      subPath: redis-cfg</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cm-redis-config</span><br><span class="line">    configMap:</span><br><span class="line">      name: cm-redis</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm cm-pod-redis -- /bin/bash</span></span><br><span class="line">/data<span class="comment"># redis-cli -p 6380</span></span><br><span class="line">127.0.0.1:6380&gt; auth ink8s.com</span><br></pre></td></tr></table></figure>

<h1 id="10-0-secret"><a href="#10-0-secret" class="headerlink" title="10.0 secret"></a>10.0 <strong>secret</strong></h1><ul>
<li>secret 资源类别：<ul>
<li>docker-registry：用于认证 Docker Registry 的 Secret，以便于用户能使用私有容器镜像</li>
<li>generic：基于本地文件、目录或命令行创建的 Secret，一般用于存储密码、秘钥、等信息</li>
<li>tls：基于指定的公钥和私钥对来创建 TLS Secret，专用于 TLS 通信</li>
</ul>
</li>
</ul>
<h2 id="10-1-创建-secret"><a href="#10-1-创建-secret" class="headerlink" title="10.1 创建 secret"></a>10.1 <strong>创建 secret</strong></h2><h3 id="10-1-1-命令创建-secret"><a href="#10-1-1-命令创建-secret" class="headerlink" title="10.1.1 命令创建 secret"></a>10.1.1 <strong>命令创建 secret</strong></h3><ul>
<li>使用 secret 为容器中运行的服务提供用于认证的用户名和密码是一种常见的应用场景，像 MySQL 镜像就支持通过环境变量来设置管理员用户的默认密码</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 比如: 创建一个名为 mysql-root-auth 的 secret 资源，用户名使用 username 键名，密码使用 password 键名</span><br><span class="line"><span class="comment"># kubectl create secret generic mysql-root-auth -n inadm --from-literal=username=root --from-literal=password=ink8s.com</span></span><br><span class="line"><span class="comment"># kubectl get secret mysql-root-auth -n inadm -o yaml</span></span><br><span class="line"><span class="comment"># echo cm9vdA== | base64 -d</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-439.png" alt="upload successful"></p>
<h3 id="10-1-2-文件创建-Secret"><a href="#10-1-2-文件创建-Secret" class="headerlink" title="10.1.2 文件创建 Secret"></a>10.1.2 <strong>文件创建 Secret</strong></h3><ul>
<li>Secret 中包含 Pod 访问数据库所需的用户凭证，除了通过命令行创建，也可以通过文件方式创建。将用户名存储在文件 username.txt 中，将密码存储在文件 password.txt 中</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 1.准备好对应的用户名文件以及密码文件</span><br><span class="line">// <span class="string">&quot;-n&quot;</span> 表示文本末尾不包含额外换行符。因为 kubectl 读取文件并将内容编码为 <span class="built_in">base64</span> 时，多余的换行符也会被编码</span><br><span class="line"><span class="comment"># echo -n &#x27;admin&#x27; &gt; username.txt</span></span><br><span class="line"><span class="comment"># echo -n &#x27;ink8s.com&#x27; &gt; password.txt</span></span><br><span class="line"><span class="comment"># kubectl create secret generic mysql-inadm-auth -n inadm --from-file=username.txt --from-file=password.txt</span></span><br><span class="line">// 默认 key 名称为文件名。可以使用 --from-file=[key=]<span class="built_in">source</span> 来设置 key 名称</span><br><span class="line">    kubectl create secret generic mysql-inadm-auth -n inadm --from-file=user=username.txt --from-file=pass=password.txt</span><br><span class="line"><span class="comment"># kubectl get secrets mysql-inadm-auth -n inadm -o yaml</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-440.png" alt="upload successful"></p>
<h3 id="10-1-3-TLS-secret"><a href="#10-1-3-TLS-secret" class="headerlink" title="10.1.3 TLS secret"></a>10.1.3 <strong>TLS secret</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create secret tls app-inadm-com -n inadm --key=app.inadm.com.key --cert=app.inadm.com.pem</span></span><br><span class="line"><span class="comment"># kubectl get secret -n inadm app-inadm-com -o yaml</span></span><br><span class="line">// 同样可以使用 <span class="built_in">echo</span> xxx | <span class="built_in">base64</span> -d 反解析</span><br></pre></td></tr></table></figure>

<h3 id="10-1-4-docker-registry"><a href="#10-1-4-docker-registry" class="headerlink" title="10.1.4 docker registry"></a>10.1.4 <strong>docker registry</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create secret docker-registry image-auth \</span></span><br><span class="line">  --docker-server=harbor.inadm.com \</span><br><span class="line">  --docker-username=admin \</span><br><span class="line">  --docker-password=ink8s.com \</span><br><span class="line">  --docker-email=user@inadm.com \                            <span class="comment"># 可选</span></span><br><span class="line">  -n inadm</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl get secrets image-auth -n inadm</span></span><br><span class="line"></span><br><span class="line">// yaml 中需写</span><br><span class="line">...</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: image-auth</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="10-1-5-secret-清单"><a href="#10-1-5-secret-清单" class="headerlink" title="10.1.5 secret 清单"></a>10.1.5 <strong>secret 清单</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 1. 创建一个用户名 admin ，密码为 ink8s.com 的 Secret 对象，首先 用户名和密码做 <span class="built_in">base64</span> 编码</span><br><span class="line"><span class="comment"># echo -n &quot;admin&quot; | base64</span></span><br><span class="line">YWRtaW4=</span><br><span class="line"><span class="comment"># echo -n &quot;ink8s.com&quot; | base64</span></span><br><span class="line">aW5hZG0uY29t</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat secret-demo01</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-demo01</span><br><span class="line">  namespace: inadm</span><br><span class="line">data:</span><br><span class="line">  user: YWRtaW4=</span><br><span class="line">  pass: aW5rOHMuY29t</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f secret-demo01 </span></span><br><span class="line"><span class="comment"># kubectl describe secret -n inadm secret-demo01 </span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 2. 通过 Data 定义资源清单需要实现进行编码，如果使用 stringData 则直接输入明文信息，而后程序自动完成编码存储至 Data</span><br><span class="line"><span class="comment"># cat secret-demo02</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-demo02</span><br><span class="line">  namespace: inadm</span><br><span class="line">stringData:</span><br><span class="line">  username: <span class="string">&quot;admin&quot;</span></span><br><span class="line">  passname: <span class="string">&quot;ink8s.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f secret-demo02 </span></span><br><span class="line"><span class="comment"># kubectl describe secret -n inadm secret-demo02</span></span><br><span class="line"><span class="comment"># kubectl get secret -n inadm secret-demo02 -o yaml</span></span><br><span class="line"><span class="comment"># echo aW5rOHMuY29t | base64 -d</span></span><br></pre></td></tr></table></figure>

<h2 id="10-2-pod-引用-secret"><a href="#10-2-pod-引用-secret" class="headerlink" title="10.2 pod 引用 secret"></a>10.2 <strong>pod 引用 secret</strong></h2><h3 id="10-2-1-env-引用-Secret"><a href="#10-2-1-env-引用-Secret" class="headerlink" title="10.2.1 env 引用 Secret"></a>10.2.1 <strong>env 引用 Secret</strong></h3><ul>
<li>Pod 资源以环境变量方式获取 Secret 数据，存在两种方式<ul>
<li>将指定键传递给环境变量，一个一个传递，通过 env.valueFrom 字段实现</li>
<li>将 Secret 对象上的全部键一次性全部映射为容器的环境变量，通过 envFrom 字段实现</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// 将此前创建 secret-demo01 变量挨个传递给系统环境变量，将 secret-demo02 一次导入到系统环境变量</span><br><span class="line"><span class="comment"># cat secret-nginx.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-nignx-pod</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: secret-nginx-container</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.16</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: USER</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: secret-demo01       <span class="comment"># 引用 secret 对象名称</span></span><br><span class="line">          key: user                 <span class="comment"># 引用 secret 对象上的键，将其值传递给环境变量</span></span><br><span class="line">    - name: PASS</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: secret-demo01</span><br><span class="line">          key: pass</span><br><span class="line">    envFrom:                        <span class="comment"># 整体引用指定的 Secret 对象全部键名和键值</span></span><br><span class="line">    - prefix: NEW                   <span class="comment"># 将所有健明引用为环境变量时统一添加的前缀 (无需求，可不用)</span></span><br><span class="line">      secretRef:</span><br><span class="line">        name: secret-demo02</span><br><span class="line"></span><br><span class="line"><span class="comment"># kbuectl apply -f secret-nginx.yaml</span></span><br><span class="line"><span class="comment"># kubectl exec -it secret-nignx-pod -n inadm -- /bin/bash</span></span><br><span class="line">:/<span class="comment"># env | egrep -i &quot;(pass|user)&quot;</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-441.png" alt="upload successful"></p>
<h3 id="10-2-2-MySQL-注入密码"><a href="#10-2-2-MySQL-注入密码" class="headerlink" title="10.2.2 MySQL 注入密码"></a>10.2.2 <strong>MySQL 注入密码</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">// MySQL 运行时初始化 root 用户的密码，引用此前创建的 secret 对象 mysql-root-auth</span><br><span class="line"><span class="comment"># cat secret-mysql.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-mysql-pod</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: secret-nginx-container</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/mysql:5.7.37</span><br><span class="line">    <span class="built_in">env</span>:</span><br><span class="line">    - name: MYSQL_ROOT_PASSWORD         <span class="comment"># 设定 MySQL 超级管理员密码变量名称</span></span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: mysql-root-auth</span><br><span class="line">          key: password</span><br><span class="line">    - name: MYSQL_USER</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: mysql-inadm-auth</span><br><span class="line">          key: username.txt</span><br><span class="line">    - name: MYSQL_PASSWORD</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: mysql-inadm-auth</span><br><span class="line">          key: password.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f secret-mysql.yaml </span></span><br><span class="line">// 使用 mysql-root-auth 对象中的 password 字段值 inadm.com 作为密码进行数据库访问</span><br><span class="line"><span class="comment"># kubectl exec -it secret-mysql-pod -n inadm -- mysql -uroot -pink8s.com</span></span><br><span class="line">mysql&gt; show databases;</span><br></pre></td></tr></table></figure>

<h3 id="10-2-3-引用整个存储卷"><a href="#10-2-3-引用整个存储卷" class="headerlink" title="10.2.3 引用整个存储卷"></a>10.2.3 <strong>引用整个存储卷</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">// 引用整个存储卷：将 mysql-inadm-auth ,这个 secret 的每个键名转为容器挂载点路径下的一个文件名</span><br><span class="line"><span class="comment"># cat secret-volume.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-volume-nginx01</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  volumes:</span><br><span class="line">  - name: user-pass</span><br><span class="line">    secret:</span><br><span class="line">      secretName: mysql-inadm-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: secret-volume-container</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.16</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: user-pass             <span class="comment"># 将 user-pass 所有 key 挂载至 /app 目录下</span></span><br><span class="line">      mountPath: /app</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f secret-volume.yaml </span></span><br><span class="line"><span class="comment"># kubectl exec -it secret-volume-nginx01 -n inadm -- ls -l /app</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-442.png" alt="upload successful"></p>
<h3 id="10-2-4-引用部分存储卷"><a href="#10-2-4-引用部分存储卷" class="headerlink" title="10.2.4 引用部分存储卷"></a>10.2.4 <strong>引用部分存储卷</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">// 1.将 mysql-inadm-auth，这个 secret 的 username.txt 键挂载至容器的 /app 目录下，命名为 username.txt</span><br><span class="line">// 2.将 mysql-inadm-auth，这个 secret 的 password.txt 键挂载至容器的 /app 目录下，命名为 password.txt</span><br><span class="line"><span class="comment"># cat secret-volume-nginx02.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-volume-nginx02</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: secret-volume-container-02</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.16</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: volume-nginx02             <span class="comment"># 将 volume-nginx02 数据挂载至容器 /app 路径</span></span><br><span class="line">      mountPath: /app</span><br><span class="line">  volumes:</span><br><span class="line">  - name: volume-nginx02</span><br><span class="line">    secret:</span><br><span class="line">      secretName: mysql-inadm-auth</span><br><span class="line">      items:</span><br><span class="line">      - key: username.txt              <span class="comment"># key 挂载至 mountPath 定义路径下，并通过 path 命名为 username.txt</span></span><br><span class="line">        path: username.txt</span><br><span class="line">      - key: password.txt</span><br><span class="line">        path: password.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f secret-volume-nginx02.yaml</span></span><br><span class="line"><span class="comment"># kubectl exec -it secret-volume-nginx02 -n inadm -- cat /app/password.txt</span></span><br><span class="line">    ink8s.com        <span class="comment"># 进行了转译</span></span><br></pre></td></tr></table></figure>

<h2 id="10-3-nginx-基于-secret-实现-tls"><a href="#10-3-nginx-基于-secret-实现-tls" class="headerlink" title="10.3 nginx 基于 secret 实现 tls"></a>10.3 <strong>nginx 基于 secret 实现 tls</strong></h2><ul>
<li>场景说明:<ul>
<li>运行一个 nginx 容器</li>
<li>nginx 虚拟站点配置文件来源于 ConfigMap</li>
<li>nginx 虚拟站点需要使用 TLS 证书，来源于 Secret</li>
<li>验证 nginx 服务是否已提供 https 访问</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create secret tls web-inadm-com -n inadm --key=app.inadm.com.key --cert=app.inadm.com.pem</span></span><br><span class="line"><span class="comment"># cat secret-nginx-ssl.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: cm-nginx-ssl-vhost</span><br><span class="line">  namespace: inadm</span><br><span class="line">data:</span><br><span class="line">  app.inadm.com.conf: |</span><br><span class="line">    server &#123;</span><br><span class="line">        listen  443 ssl;</span><br><span class="line">        server_name app.inadm.com;</span><br><span class="line"></span><br><span class="line">        ssl_certificate /etc/nginx/certs/tls.crt;</span><br><span class="line">        ssl_certificate_key /etc/nginx/certs/tls.key;</span><br><span class="line">    </span><br><span class="line">        location / &#123;</span><br><span class="line">            root    /usr/share/nginx/html;</span><br><span class="line">            index   index.html;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        include /etc/nginx/conf.d/*.cfg;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen  80;</span><br><span class="line">        server_name app.inadm.com;</span><br><span class="line">        <span class="built_in">return</span> 301 https://$host<span class="variable">$request_uri</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ssl-vhost</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx-ssl-vhost-container</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.16</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: ngx-conf</span><br><span class="line">      mountPath: /etc/nginx/conf.d/</span><br><span class="line">    - name: ngx-tls</span><br><span class="line">      mountPath: /etc/nginx/certs/</span><br><span class="line">  volumes:</span><br><span class="line">  - name: ngx-conf</span><br><span class="line">    configMap:</span><br><span class="line">      name: cm-nginx-ssl-vhost</span><br><span class="line">      items:</span><br><span class="line">      - key: app.inadm.com.conf</span><br><span class="line">        path: app.inadm.com.conf</span><br><span class="line">        mode: 0644</span><br><span class="line">  - name: ngx-tls                       <span class="comment"># 定义虚拟主机 tls 文件来源</span></span><br><span class="line">    secret:</span><br><span class="line">      secretName: app-inadm-com</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f secret-nginx-ssl.yaml </span></span><br><span class="line"><span class="comment"># curl -I -k https://pod_ip</span></span><br><span class="line"><span class="comment"># curl -I -k -v https://pod_ip</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-443.png" alt="upload successful"></p>
<h1 id="11-0-statefulset"><a href="#11-0-statefulset" class="headerlink" title="11.0 statefulset"></a>11.0 <strong>statefulset</strong></h1><ul>
<li>StatefulSet 名称是固定，且创建时按照顺序进行创建，并固定对应的 Pod 名称</li>
<li>Headless Service 用来配置每个 Pod 的 DNS 名称，只要 Pod 名称不变化，他们的 DNS 就是稳定且持久的</li>
<li>VolumeClaimTemplate 作为分布式，每个 Pod 的数据时不一样的，所以每个 Pod 应该有自己的专有数据，所以他们不能使用同一个存储卷，应该使用各自自己的存储卷</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// StatefulSet 示例配置</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: &lt;string&gt;                            <span class="comment"># 资源名称</span></span><br><span class="line">  namespace: &lt;string&gt;</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &lt;string&gt;                     <span class="comment"># 相关 HeadlessService 名称</span></span><br><span class="line">  updateStrategy: &lt;object&gt;                  <span class="comment"># 更新策略</span></span><br><span class="line">    <span class="built_in">type</span>: &lt;string&gt;                          <span class="comment"># 更新类型，可以有 rollingUpdate 和 OnDelete</span></span><br><span class="line">    rollingUpdate: &lt;object&gt;                 <span class="comment"># 滚动更新</span></span><br><span class="line">      partition: &lt;<span class="built_in">integer</span>&gt;                  <span class="comment"># 分区策略，默认为 0</span></span><br><span class="line">  podManagementPolicy: &lt;string&gt;             <span class="comment"># Pod 管理策略，默认 OrderedReady 表示顺序创建 Pod，逆序删除</span></span><br><span class="line">                                            <span class="comment"># 另一种 Parallel 表示并行创建 Pod 和并行删除  Pod</span></span><br><span class="line">  replicas: &lt;<span class="built_in">integer</span>&gt;                       <span class="comment"># Pod 副本数</span></span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demo</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demo</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: demo-container</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">  volumeClaimTemplates:                             <span class="comment"># 存储卷申请模版</span></span><br><span class="line">    metadata:</span><br><span class="line">      name: data</span><br><span class="line">    spec:</span><br><span class="line">      accessMode: [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">      storageClassName: <span class="string">&quot;managed-nfs-storage&quot;</span></span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 2G</span><br></pre></td></tr></table></figure>

<h2 id="11-1-sts-实践"><a href="#11-1-sts-实践" class="headerlink" title="11.1 sts 实践"></a>11.1 <strong>sts 实践</strong></h2><ul>
<li>创建名为 web-svc 的 Headless Service 用来控制网络域名</li>
<li>创建名为 web 的 StatefulSet 有一个 spec，它表名将在独立的 3 个 Pod 副本中启动 nginx 容器</li>
<li>volumeClaimTemplates 将通过 PersistentVolumes 驱动来提供稳定的存储</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat sts-web-svc.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: web-svc</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  serviceName: web-svc</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.16</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: data</span><br><span class="line">      spec:</span><br><span class="line">        accessModes: [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">        storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 6Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f sts-web-svc.yaml</span></span><br><span class="line"><span class="comment"># kubectl get pvc -n inadm                                                 # pv 动态供应自动创建</span></span><br><span class="line"><span class="comment"># dig web-0.web-svc.inadm.svc.cluster.local @10.96.0.10 +short                # DNS 名称解析</span></span><br><span class="line"><span class="comment"># kubectl scale sts web -n inadm --replicas=5                                 # 扩展 sts，按照编号一个一个顺序创建出来</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-444.png" alt="upload successful"></p>
<h2 id="11-2-sts-更新策略"><a href="#11-2-sts-更新策略" class="headerlink" title="11.2 sts 更新策略"></a>11.2 <strong>sts 更新策略</strong></h2><ul>
<li>sts 更新支持两种策略 onDelete 和 RollingUpdate，在 .spec.updateStrategy 进行设定<ul>
<li>onDelete: 当 StatefulSet 的 .spec.updateStrategy.type 设置为 OnDelete 时，控制器将不会自动更新 StatefulSet 中的 Pod。用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的 .spec.template 的变动做出反应</li>
<li>RollingUpdate: RollingUpdate 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。这是默认的更新策略。StatefulSet 控制器会删除和重建 Pod。它将按照与 Pod 终止相同的顺序(从最大序号到最小序号)进行，每次更新一个 Pod</li>
</ul>
</li>
</ul>
<h3 id="11-2-1-滚动更新"><a href="#11-2-1-滚动更新" class="headerlink" title="11.2.1 滚动更新"></a>11.2.1 <strong>滚动更新</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">// 滚动更新：修改 yaml 为 RollingUpdate，并更新镜像版本</span><br><span class="line"><span class="comment"># cat sts-web.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: web-svc</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  serviceName: web-svc</span><br><span class="line">  replicas: 5</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  updateStrategy:                     <span class="comment"># 设定滚动更新</span></span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: harbor.inadm.com/ops_apps_tools/nginx:1.18            <span class="comment"># 调整版本，观察申请顺序</span></span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: data</span><br><span class="line">      spec:</span><br><span class="line">        accessModes: [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">        storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 6Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f sts-web.yaml</span></span><br><span class="line"><span class="comment"># kubectl get pod -n inadm -o wide -l app=nginx -w</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-446.png" alt="upload successful"></p>
<p><img src="/images/pasted-445.png" alt="upload successful"></p>
<h3 id="11-2-2-分区滚动更新"><a href="#11-2-2-分区滚动更新" class="headerlink" title="11.2.2 分区滚动更新"></a>11.2.2 <strong>分区滚动更新</strong></h3><ul>
<li>sts 的 RollingUpdate 滚动更新支持 Partition 分区更新，有点类似灰度发布模式<ul>
<li>设定 partition 为 3, sts 会检查是否有 Pod 的编号大于 3 ，如果有则更新大于等于 N 的 Pod</li>
<li>如果直接 partition 分区为 0，sts 会更新所有的镜像 (从最大序号到最小序号进行镜像更新)</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat sts-web.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: web-svc</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  serviceName: web-svc</span><br><span class="line">  replicas: 5</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  updateStrategy:                     <span class="comment"># 设定滚动更新</span></span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      partition: 3                    <span class="comment"># 设定更新分区</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: data</span><br><span class="line">      spec:</span><br><span class="line">        accessModes: [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">        storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 6Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pod -n inadm -l app=nginx -w -o wide            # 观察更新过程</span></span><br></pre></td></tr></table></figure>

<h1 id="12-0-授权与认证"><a href="#12-0-授权与认证" class="headerlink" title="12.0 授权与认证"></a>12.0 <strong>授权与认证</strong></h1><ul>
<li>为何需要认证：对于 k8s 系统来说，APIServer 肯定不是任何人都能轻易访问的，如果任何人都能轻易访问，意味着可以通过 kubectl 命令访问 APIServer，进而操作 k8s。也就意味着它能够在我们的系统上随便部署应用程序，甚至还会删除我们正在运行的应用程序，这是非常危险的。所以我们需要对用户进行<strong>身份认证</strong>，确保身份是合法的。</li>
<li>认证流程：任何客户端用户试图通过 APIServer 操作资源对象时，他们必须经历多个阶段的访问控制，才会被接受处理，其中包含认证、授权及准入控制<ul>
<li>认证：任何客户端访问，经过 API 操作之前，需要先完成认证操作，也就是进行<strong>身份认证</strong></li>
<li>授权：认证通过后仅代表它是一个合法的系统用户，但它是否拥有删除对应资源权限，需要进行<strong>授权检查</strong></li>
<li>准入控制：虽然我们有了权限，也可以创建 Pod 资源等各种资源，但创建 Pod 是否能够成功呢，假设 ops 名称空间显示最多创建 2 个 Pod，目前已经有了 2 个 Pod 了，那么这次的创建就会失败</li>
</ul>
</li>
<li>认证的方式：<ul>
<li><strong>UserAcconunt</strong>：使用 kubectl 创建资源，首先要进行客户端身份认证，所以客户端每次请求 APIServer 时都会携带上数字证书，用于认证 API-Server，当认证通过后，证书中的 Subject 将被识别为用户标识，其中的 CN 字段的值为用户名，字段 O 的值就是用户所属的组，例如：Subject： O&#x3D;ops,CN&#x3D;user01 用户名为 user01，用户的组为 ops</li>
<li><strong>ServiceAccount</strong>：有些情况下，期望在 Pod 内部能够访问 API-Server，获取集群的信息，甚至对集群进行改动。针对这种情况，k8s 提供了一种特殊的认证方式：ServiceAccount。默认情况下，创建的 Pod 如果没有指定 ServiceAccount 则系统默认提供一个 ServiceAccount，而后通过 mount 方式挂载到 Pod 文件系统，该 ServiceAccount 能通过 DownWardAPI 获取该 Pod 相关的一些元数据信息。当人也可以自行创建 ServiceAccount 来完成 API-Server 的身份认证，至于能否对集群进行改变，则需要看该 ServiceAccount 是否拥有权限</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 可以使用 <span class="built_in">echo</span> xxx | <span class="built_in">base64</span> -d 对 client-certificate-data 进行超管的解码</span><br><span class="line"><span class="comment"># cat /root/.kube/config                            # 对 &quot;client-certificate-data&quot; 进行解码</span></span><br><span class="line"><span class="comment"># echo xxx | base64 -d</span></span><br><span class="line"><span class="comment"># openssl x509 -in test.crt -text -noout            # 组&quot;O&quot;,用户&quot;CN&quot;</span></span><br><span class="line">// 进一步查看 kubernetes-admin 用户权限</span><br><span class="line"><span class="comment"># kubectl get clusterrole cluster-admin -o yaml</span></span><br><span class="line"><span class="comment"># kubectl get clusterrolebindings cluster-admin -o yaml</span></span><br></pre></td></tr></table></figure>

<h2 id="12-1-SA-认证实践"><a href="#12-1-SA-认证实践" class="headerlink" title="12.1 SA 认证实践"></a>12.1 <strong>SA 认证实践</strong></h2><ul>
<li><a target="_blank" rel="noopener" href="https://tooltt.com/jwt-decode/">在线 JWT Token 解析解码</a></li>
<li>使用默认 sa 认证 API：当创建 Pod 时，如果没有指定 SA，Pod 会注入对应名称空间的 default 服务账户。如果你查看 Pod 的原始 YAML（例如：kubectl get pod&#x2F;podname -o yaml），你可以看到 spec.serviceAccountName 字段已经被自动设置了。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -n inadm pod-test -o yaml | grep &quot;serviceAccountName&quot;</span></span><br><span class="line"><span class="comment"># cat sa-pod-test.yaml</span></span><br><span class="line">piVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-test</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: pod-test</span><br><span class="line">    image: harbor.inadm.com/inadm_kubernetes/demoapp:v1.0</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">spec:</span><br><span class="line">  serviceAccountName: default</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f sa-pod-test.yaml</span></span><br><span class="line"><span class="comment"># kubectl exec -it -n inadm pod-test -- /bin/sh</span></span><br><span class="line"><span class="variable">$cat</span> /var/run/secrets/kubernetes.io/serviceaccount/token		<span class="comment"># 令牌</span></span><br><span class="line">// Token 解码 https://tooltt.com/jwt-decode/</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-447.png" alt="upload successful"></p>
<p><img src="/images/pasted-448.png" alt="upload successful"></p>
<h3 id="12-1-1-创建-sa-并与-Pod-关联"><a href="#12-1-1-创建-sa-并与-Pod-关联" class="headerlink" title="12.1.1 创建 sa 并与 Pod 关联"></a>12.1.1 <strong>创建 sa 并与 Pod 关联</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 命令方式创建 SA</span><br><span class="line"><span class="comment"># kubectl create sa sa-test --namespace=default</span></span><br><span class="line"></span><br><span class="line">// 清单文件写法</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: sa-test</span><br><span class="line">  namespace: inadm</span><br><span class="line">automountServiceAccountToken: <span class="literal">true</span>            <span class="comment"># 自动挂载 API 凭据</span></span><br></pre></td></tr></table></figure>

<ul>
<li>测试 SA 用户能否认证 APIServer</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Pod 清单文件应用 SA</span><br><span class="line"><span class="comment"># cat sa-pod.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: sa-pod</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  serviceAccountName: sa-test                 <span class="comment"># 指定 Pod 运行时使用的 SA</span></span><br><span class="line">  automountServiceAccountToken: <span class="literal">true</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line"></span><br><span class="line">// 测试 SA 用户能否认证 APIServer</span><br><span class="line"><span class="comment"># kubectl exec -it sa-pod -- /bin/bash</span></span><br><span class="line"><span class="comment"># cd /var/run/secrets/kubernetes.io/serviceaccount</span></span><br><span class="line">// 虽然能认证，但是没有权限访问 (cdoe: 403)</span><br><span class="line"><span class="comment"># curl --cacert ca.crt -H &quot;Authorization: Bearer $(cat token)&quot; https://kubernetes/api/v1/namespaces/default</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-450.png" alt="upload successful"></p>
<ul>
<li>分配集群管理员权限，测试效果</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create rolebinding role-sa-admin --clusterrole=admin --serviceaccount=default:sa-test</span></span><br><span class="line">// 再次测试 SA 是否能访问 APIServer</span><br><span class="line"><span class="comment"># curl --cacert ca.crt -H &quot;Authorization: Bearer $(cat token)&quot; https://kubernetes/api/v1/namespaces/default</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-451.png" alt="upload successful"></p>
<h3 id="12-1-2-sa-添加私有仓库认证"><a href="#12-1-2-sa-添加私有仓库认证" class="headerlink" title="12.1.2 sa 添加私有仓库认证"></a>12.1.2 <strong>sa 添加私有仓库认证</strong></h3><ul>
<li>通过 ServiceAccountName 来完成私有仓库认证，可以不使用 imagePullSecret，因为在 ServiceAccount 中有 Image Pull Secrets 这个字段，可以将认证仓库信息给附加进去</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// 1.secret 认证使用 harbor-auth</span><br><span class="line">// 2.创建 SA，定义名称，然后将镜像拉取 Secret 添加到该账号下</span><br><span class="line"><span class="comment"># cat sa-pod.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: sa-pod-auth</span><br><span class="line">  namespace: inadm</span><br><span class="line">imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: sa-pod-auth</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  serviceAccount: sa-pod-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: sa-pod-auth</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f sa-pod.yaml</span></span><br></pre></td></tr></table></figure>

<h2 id="12-2-k8s-基于-user-认证"><a href="#12-2-k8s-基于-user-认证" class="headerlink" title="12.2 k8s 基于 user 认证"></a>12.2 <strong>k8s 基于 user 认证</strong></h2><ul>
<li>kubeconfig 作用：由于 APIServer 是基于无状态 HTTP&#x2F;HTTPS 协议实现，所以每次与集群进行交互都需要进行身份认证，通常都是使用证书进行认证，其认证所需要的信息都会存放在 kubeconfig 文件中</li>
<li>客户端程序课通过默认路径 –kubeconfig 选项 或 KUBECONFIG 环境变量定义要加载的 kubeocnfig 文件，从而能够在每次的请求通过 API-Server 的认证</li>
<li>kubeconfig 文件格式：kubeconfig 文件主要分为四部分：cluster、users、context、current-context<ul>
<li>cluster：集群以列表形式定义在 cluster 配置段中，每个列表项代表一个 kubernetes 集群，并拥有名称标识</li>
<li>users：访问集群的身份都定义在 users 配置段中，每个列表项代表一个能够认证到某个 kubernetes 集群的凭据</li>
<li>context：将 user 与 cluster 二者之间的映射关系进行绑定，而后定义在 context 配置中</li>
<li>current-context：用于指定当前集群默认使用的 context，表示当前正在使用哪个用户操作哪个集群</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// 默认使用 kubeadm 初始化 kubernetes 集群过程中，在 master 节点上生成的 /etc/kubernetes/admin.conf 文件就是一个 kubeconfig 格式的文件，它由 kubeadm 命令自动生成，可由 kubectl 加载后接入当前集群的 API Server。</span><br><span class="line">// 默认 kubectl 加载的 kubeconfig 文件默认路径为 <span class="variable">$HOME</span>/.kube/config，当然也可以通过 --kubeconfig 选项，或 KUBECONFIG 环境变量将其修改为其它路径</span><br><span class="line"></span><br><span class="line">// kubectl config view 命令能打印 kubeconfig 文件内容，其中包含了集群列表，用户列表，上下文列表，以及当前使用的上下文 context 等</span><br><span class="line"><span class="comment"># kubectl config view</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:                                         <span class="comment"># 集群列表</span></span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: DATA+OMITTED</span><br><span class="line">    server: https://k8s-vip.inadm.com:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:                                         <span class="comment"># 映射关系</span></span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kubernetes-admin</span><br><span class="line">  name: kubernetes-admin@kubernetes</span><br><span class="line">current-context: kubernetes-admin@kubernetes      <span class="comment"># 当前正在使用的</span></span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line"><span class="built_in">users</span>:                                            <span class="comment"># 用户列表</span></span><br><span class="line">- name: kubernetes-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: REDACTED</span><br><span class="line">    client-key-data: REDACTED</span><br></pre></td></tr></table></figure>

<h3 id="12-2-1-自定义-kubeconfig"><a href="#12-2-1-自定义-kubeconfig" class="headerlink" title="12.2.1 自定义 kubeconfig"></a>12.2.1 <strong>自定义 kubeconfig</strong></h3><ul>
<li>创建一个 UserAccount 用户，然后加入到 kubeconfig 文件中，最后通过 kubectl 加载 kubeconfig 文件中对应的证书信息，然后尝试认证到 APServer</li>
<li>UserAccount 用户不可以直接创建，需要创建证书文件，在证书申请文件中填写好证书对应的 CN，也就是我们的用户名称，而后经由 APIServer 信任的 CA (&#x2F;etc&#x2F;kubernetes&#x2F;api&#x2F;ca.crt) 签署证书请求文件。最后与 APIServer 进行认证时，APIServer 会获取证书中的 CN，以判断该用户是否是合法的。 </li>
<li>加入已有集群配置文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// 创建证书私钥文件</span><br><span class="line"><span class="comment"># mkdir -p /root/.certs</span></span><br><span class="line"><span class="comment"># (umask 077; openssl genrsa -out /root/.certs/user01.key 2048)</span></span><br><span class="line"></span><br><span class="line">// 创建证书签署请求文件，-subj 选项中的 CN 的值将被 APIServer 识别为用户名，O 的值将被识别为用户组</span><br><span class="line"><span class="comment"># openssl req -new -key /root/.certs/user01.key -out /root/.certs/user01.csr -subj &quot;/CN=user01/O=devops&quot;</span></span><br><span class="line"></span><br><span class="line">// 使用 kubernetes-ca 的身份对文件进行签署</span><br><span class="line"><span class="comment"># openssl x509 -req -days 3650 -in /root/.certs/user01.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out /root/.certs/user01.crt</span></span><br><span class="line"></span><br><span class="line">// 根据 x509 数字证书及私钥创建身份凭据</span><br><span class="line"><span class="comment"># kubectl config set-credentials user01 --client-certificate=/root/.certs/user01.crt --client-key=/root/.certs/user01.key --embed-certs=true</span></span><br><span class="line"></span><br><span class="line">// 配置 context 上下文，以 user01 身份凭据访问已定义的 kubernetes 集群，context 名称为 user01@kubernets</span><br><span class="line"><span class="comment"># kubectl config set-context user01@kubernetes --cluster=kubernetes --user=user01</span></span><br><span class="line"></span><br><span class="line">// 将当前上下文切换为 user01@kubernetes</span><br><span class="line"><span class="comment"># kubectl config use-context user01@kubernetes</span></span><br><span class="line"></span><br><span class="line">// 测试 user01 用户是否能通过 API Server 认证</span><br><span class="line"><span class="comment"># kubectl get ns</span></span><br><span class="line">Error from server (Forbidden): namespaces is forbidden: User <span class="string">&quot;user01&quot;</span> cannot list resource <span class="string">&quot;namespaces&quot;</span> <span class="keyword">in</span> API group <span class="string">&quot;&quot;</span> at the cluster scope</span><br><span class="line">/ 虽然报错，但 user01 用户已被 API Server 正确失败</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config get-contexts 					# 查看所有上下文</span></span><br><span class="line"><span class="comment"># kubectl config current-context 				# 查看当前使用的上下文</span></span><br><span class="line"><span class="comment"># kubectl config use-context kubernetes-admin@kubernetes	# 切换上下文</span></span><br></pre></td></tr></table></figure>

<h3 id="12-2-2-创建新的集群配置文件"><a href="#12-2-2-创建新的集群配置文件" class="headerlink" title="12.2.2 创建新的集群配置文件"></a>12.2.2 <strong>创建新的集群配置文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 1.添加集群配置，设定集群名称，设定 APIServer 地址，以及 APIServer 信任的 CA 证书</span><br><span class="line"><span class="comment"># kubectl config set-cluster inadm_k8s --kubeconfig=/tmp/config --server=&quot;https://k8s-vip.inadm.com:6443&quot; --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true</span></span><br><span class="line"></span><br><span class="line">// 2.添加身份凭据，使用 CA 已签署的客户端证书即可</span><br><span class="line"><span class="comment"># kubectl config set-credentials user01 --kubeconfig=/tmp/config --client-certificate=/root/.certs/user01.crt --client-key=/root/.certs/user01.key --embed-certs=true</span></span><br><span class="line"></span><br><span class="line">// 3.将 user01 用户与 inadm_k8s 集群建立映射关系</span><br><span class="line"><span class="comment"># kubectl config set-context user01@inadm_k8s --cluster=inadm_k8s --user=user01 --kubeconfig=/tmp/config</span></span><br><span class="line"></span><br><span class="line">// 4.设定默认上下文为 user01@inadm_k8s</span><br><span class="line"><span class="comment"># kubectl config use-context user01@inadm_k8s --kubeconfig=/tmp/config</span></span><br><span class="line"></span><br><span class="line">// 5. user01 用户为设置任何授权，但它能够被系统识别为 user01 用户，这表示身份认证时正常；后续就可以授权了</span><br><span class="line"><span class="comment"># kubectl get nodes --kubeconfig=/tmp/config --context=&quot;user01@inadm_k8s&quot;</span></span><br><span class="line">Error from server (Forbidden): nodes is forbidden: User <span class="string">&quot;user01&quot;</span> cannot list resource <span class="string">&quot;nodes&quot;</span> <span class="keyword">in</span> API group <span class="string">&quot;&quot;</span> at the cluster scope</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]<span class="comment"># kubectl config current-context                # 查看 kubectl 的当前上下文</span></span><br><span class="line">    user01@kubernetes</span><br></pre></td></tr></table></figure>

<h2 id="12-3-RBAC-授权"><a href="#12-3-RBAC-授权" class="headerlink" title="12.3 RBAC 授权"></a>12.3 <strong>RBAC 授权</strong></h2><ul>
<li>RBAC 基于角色的访问控制；其实就是将资源的操作权限授予给指定的角色，而后将用户加入该角色，那么该用户则拥有了对角色的权限</li>
<li>举例：希望 user01 用户能够获取所有 Pod 的列表<ul>
<li>首先定义角色，然后定义角色权限规则，资源：Pod，操作权限：get,list</li>
<li>然后定义角色绑定，将 user01 绑定至该角色，而后 user01 就拥有了该角色的权限，进而就能够获取 Pod 信息</li>
</ul>
</li>
</ul>
<p><img src="/images/pasted-452.png" alt="upload successful"></p>
<ul>
<li>RBAC 角色与集群角色：k8s 系统的 RBAC 授权插件将角色分为了 Role 和 ClusterRole 两类：<ul>
<li>Role：仅作用于名称空间级别，用于承载名称空间级别内的资源权限集合</li>
<li>ClusterRole：作用于集群范围，能够同时承载名称空间和集群级别的资源权限集合</li>
</ul>
</li>
<li>k8s 利用 Role 和  ClusterRole 两类角色来赋予对应的权限，同时也需要用到另外两类资源 Rolebinding 和 ClusterRolebinding 来完成用户与角色的绑定关系</li>
</ul>
<p><img src="/images/pasted-453.png" alt="upload successful"></p>
<ul>
<li>注意：RoleBinding除了可以绑定 Role 以外，还可以绑定 ClusterRole，但它的权限还是限制在名称空间级别<ul>
<li>这种方式有着特定的应用场景<ul>
<li>比如：希望在三个名称空间中都创建一个管理员身份，那么我们就需要创建3个 role 和 3 个 rolebinding</li>
<li>但是：我们可以定义一个 clusterrole，然后通过 rolebind 绑定就完成了，也就不需要重复创建很多的 role</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/images/pasted-454.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Role &lt;-- Rolebinding --&gt; User</span><br><span class="line">User --&gt; Rolebinding --&gt; ClusterRole</span><br></pre></td></tr></table></figure>

<h3 id="12-3-1-RBAC-场景-1"><a href="#12-3-1-RBAC-场景-1" class="headerlink" title="12.3.1 RBAC 场景-1"></a>12.3.1 <strong>RBAC 场景-1</strong></h3><ul>
<li>场景说明： 赋予 user01 用户对 default 名称空间拥有 Pod 的读取权限</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 创建 role 角色，设定对应的规则</span><br><span class="line"></span><br><span class="line">// 命令方式</span><br><span class="line"><span class="comment"># kubectl create role default-pod-reader --resource=pod --verb=get,list,watch --namespace=default --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"></span><br><span class="line">// yaml 方式</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: default-pod-reader</span><br><span class="line">  namespace: default</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">&quot;&quot;</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br></pre></td></tr></table></figure>

<ul>
<li>创建 rolebinding 角色绑定，将 user01 绑定至对应的 role 上</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 命令方式</span><br><span class="line"><span class="comment"># kubectl create rolebinding user01-default-pod-reader --role=default-pod-reader --user=user01 --namespace=default --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"></span><br><span class="line">// yaml 方式</span><br><span class="line">apiVersion: rabc.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: user01-default-pod-reader</span><br><span class="line">  namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rabc.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: default-pod-reader</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup:</span><br><span class="line">  kind: User</span><br><span class="line">  name: user01</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 user01 用户验证权限</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod --context=&quot;user01@kubernetes&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="12-3-2-RBAC-场景-2"><a href="#12-3-2-RBAC-场景-2" class="headerlink" title="12.3.2 RBAC 场景-2"></a>12.3.2 <strong>RBAC 场景-2</strong></h3><ul>
<li>场景说明：赋予 user01 用户对所有名称空间拥有 Pod 的读取权限 （ClusterRole、ClusterRoleBinding）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// 创建 clusterrole</span><br><span class="line"></span><br><span class="line">// 命令方式</span><br><span class="line"><span class="comment"># kubectl create clusterrole cluster-pod-reader --resource=pod --verb=get,list,watch --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"></span><br><span class="line">// yaml 方式</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: cluster-pod-reader</span><br><span class="line">rule:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">&quot;&quot;</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br></pre></td></tr></table></figure>

<ul>
<li>创建 clusterrolebinding，并绑定 user01 用户至对应的 clusterrole</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 命令方式</span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding cluster-pod-reader-user01 --clusterrole=cluster-pod-reader --user=user01 --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"></span><br><span class="line">// yaml 方式</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: cluster-pod-reader-user01</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-pod-reader</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: User</span><br><span class="line">  name: user01</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 user01 用户验证权限</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod --context=&quot;user01@kubernetes&quot;</span></span><br><span class="line"><span class="comment"># kubectl get pod -n kube-system --context=&quot;user01@kubernetes&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="12-3-3-RBAC-场景-3"><a href="#12-3-3-RBAC-场景-3" class="headerlink" title="12.3.3 RBAC 场景-3"></a>12.3.3 <strong>RBAC 场景-3</strong></h3><ul>
<li>场景说明: 赋予 user01 用户对 default 名称空间拥有管理员权限<ul>
<li>系统内置了一个 ClusterRole: admin 的集群管理员，我们可以通过 rolebinding 引用 ClusterRole: admin 集群角色，该引用会造成用户仅对 rolebinding 所在的名称空间有管理员权限。（因为 rolebinding 仅能作用在空间）</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 删除此前的绑定，避免权限受到干扰</span><br><span class="line"><span class="comment"># kubectl delete clusterrolebindings cluster-pod-reader-user01 --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"><span class="comment"># kubectl delete rolebindings user01-default-pod-reader --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"></span><br><span class="line">// 创建 RoleBinding 引用 Cluster-role</span><br><span class="line"><span class="comment"># kubectl create rolebinding default-admin-user01 --clusterrole=admin --user=user01 --namespace=default --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line"></span><br><span class="line">// 验证 user01 用户权限</span><br><span class="line"><span class="comment"># kubectl get pod</span></span><br><span class="line"><span class="comment"># kubectl delete pod sa-pod</span></span><br><span class="line"></span><br><span class="line">// 能对 default 名称空间进行 增 删 改，对其它名称空间无权限</span><br><span class="line"><span class="comment"># kubectl get pod -n ingress-nginx --context=&quot;user01@kubernetes&quot;</span></span><br><span class="line">Error from server (Forbidden): pods is forbidden: User <span class="string">&quot;user01&quot;</span> cannot list resource <span class="string">&quot;pods&quot;</span> <span class="keyword">in</span> API group <span class="string">&quot;&quot;</span> <span class="keyword">in</span> the namespace <span class="string">&quot;ingress-nginx&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="12-3-4-内置ClusterRole"><a href="#12-3-4-内置ClusterRole" class="headerlink" title="12.3.4 内置ClusterRole"></a>12.3.4 <strong>内置ClusterRole</strong></h3><ul>
<li>k8s 系统内置了一组默认的 ClusterRole 和 ClusterRoleBinding 资源预留给系统使用，其中大多数都以 system: 为前缀。还有一些不易 system: 为前缀的 ClusterRole 是面向用户设计的，比如：集群管理员角色 cluster-admin、admin、edit、view 掌握这些默认的内置角色资源有助于按需创建用户并分配相应的权限</li>
</ul>
<p><img src="/images/pasted-468.png" alt="upload successful"></p>
<ul>
<li>Cluster-admin 分析：内置的 cluster-admin 资源拥有管理集群所有资源的权限，而内置的 cluster-admin 将该角色分配给了 system:master 组，这意味着所有加入该组的用户将自动具有集群的超级管理员权限</li>
<li>在使用 kubeadm 安装集群时，它自动创建配置文件 &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf 中定义的用户为 kubernetes-admin，而该用户使用数字证书， Subject 属性值为 &#x2F;o&#x3D;system:master。所以 API Server 会在成功验证该用户的身份之后将其识别为 system:master 用户组成员</li>
</ul>
<p><img src="/images/pasted-469.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// 分析过程如下：</span><br><span class="line"></span><br><span class="line">1. 查看 cluster-admin 角色的绑定关系，可以看到 cluster-admin 这个角色绑定的 system.master</span><br><span class="line"><span class="comment"># kubectl get clusterrolebinding cluster-admin -o yaml --context=&quot;kubernetes-admin@kubernetes&quot;</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">&quot;true&quot;</span></span><br><span class="line">  creationTimestamp: <span class="string">&quot;xxx&quot;</span></span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  resourceVersion: <span class="string">&quot;172&quot;</span></span><br><span class="line">  uid: adf208fd-91ba-4766-89dc-76fbc2901b00</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:masters</span><br><span class="line">  </span><br><span class="line">2. system:master 这个组名称并非认为定义，而是证书生成的</span><br><span class="line"><span class="comment"># echo &quot;KUBERNETES-ADMIN_USER_CRT&quot; | base64 -d &gt; 1.txt</span></span><br><span class="line"><span class="comment"># openssl x509 -in 1.txt -text -noout            # 注意：在创建证书时，可以将用户绑定到 system:master 组，用户会自动集成组的权限</span></span><br></pre></td></tr></table></figure>

<h4 id="12-3-4-1-Cluster-admin-实践"><a href="#12-3-4-1-Cluster-admin-实践" class="headerlink" title="12.3.4.1 Cluster-admin 实践"></a>12.3.4.1 <strong>Cluster-admin 实践</strong></h4><ul>
<li>创建一个 ops 用户，然后将该用户加入到 system:masters 组中，验证是否拥有集群管理员权限</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1. 创建证书私有文件</span><br><span class="line"><span class="comment"># mkdir -p /root/.certs</span></span><br><span class="line"><span class="comment"># (umask 077; openssl genrsa -out /root/.certs/ops.key 2048)</span></span><br><span class="line"></span><br><span class="line">2. 创建证书签署请求文件，CN 为指定的用户名，O 为指定的组名称</span><br><span class="line"><span class="comment"># openssl req -new -key /root/.certs/ops.key -out /root/.certs/ops.csr -subj &quot;/CN=ops/O=system:masters&quot;</span></span><br><span class="line"></span><br><span class="line">3. 使用 kubernetes-ca 的身份对文件进行签署</span><br><span class="line"><span class="comment"># openssl x509 -req -days 3650 -in /root/.certs/ops.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out /root/.certs/ops.crt</span></span><br><span class="line"></span><br><span class="line">4. 根据 x509 数字证书及私钥创建身份凭据</span><br><span class="line"><span class="comment"># kubectl config set-credentials ops --client-certificate=/root/.certs/ops.crt --client-key=/root/.certs/ops.key --embed-certs=true</span></span><br><span class="line"></span><br><span class="line">5. 配置 context 上下文，以 user01 身份凭据访问已定义的 kuberntes 集群，context 名称为 user01@kubernetes</span><br><span class="line"><span class="comment"># kubectl config set-context ops@kubernetes --cluster=kubernetes --user=ops</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">6. 测试 user01 是否具备超级管理员权限</span><br><span class="line"><span class="comment"># kubectl get nodes --context=&quot;ops@kubernetes&quot;</span></span><br><span class="line"><span class="comment"># kubectl get all --context=&quot;ops@kubernetes&quot;</span></span><br></pre></td></tr></table></figure>


<h1 id="13-0-准入控制"><a href="#13-0-准入控制" class="headerlink" title="13.0 准入控制"></a>13.0 <strong>准入控制</strong></h1><ul>
<li>APIServer 中准入控制器是以插件形式存在的，他们会拦截所有已完成认证的用户，且与资源创建、更新、删除操作相关的请求</li>
<li>通过 APIServer 启用的准入控制插件，然后强制实现定义的功能，比如：对象的语义验证、以及设置确实字段的默认值等 k8s 通过 ResourceQuota 以及 LImitRange 准入控制器，可以为多租户或多项目的集群环境提供资源配额与限制</li>
</ul>
<h2 id="13-1-ResoucesQuta"><a href="#13-1-ResoucesQuta" class="headerlink" title="13.1 ResoucesQuta"></a>13.1 <strong>ResoucesQuta</strong></h2><ul>
<li><p>资源配额：当系统存在多个用户或团队共享具有固定节点的 k8s 集群时，一般会根据不同团队创建不同的命名空间，但可能会出现某个应用将该命名空间的 CPU 或 内存耗尽的情况，无法保证其公平分配原则。可以通过 ResourceQuotas 资源配额来解决这个问题。ResourceQuota 主要是对每个命名空间的资源使用总量设定限制</p>
<ul>
<li>它可以限制命名空间中某种类型对象的所创建的总数进行限制</li>
<li>也可以限制命名空间中 Pod 可以使用的 CPU 或 内存资源的总上线</li>
</ul>
</li>
<li><p>场景-1：当用户在命名空间下创建资源（如：Pod、Service 等）时，k8s 的配额会跟踪集群的资源使用情况，以确保使用的资源用量不超过 ResourceQuota 中定义的资源限额。如果资源创建或更新请求违反了配额约束，那么该请求会报错（HTTP 403 FORBIDDEN），并在消息中给出违反的信息描述</p>
</li>
<li><p>场景-2：如果命名空间下的计算资源（如 cpu 和 memory）的配额被启用，则用户必须为这些资源设定（requests）和（limit），否则配额系统将决绝 Pod 的创建。提示：可使用LimitRange 准入控制器来为没有设置计算资源的 Pod 设置默认值</p>
</li>
<li><p>配额策略示例：在具有 32GiB 内存和 16C 资源的集群中</p>
<ul>
<li>允许 A 团队使用 20GiB 内存和 10C 的资源</li>
<li>允许 B 团队使用 10GiB 内存和 4C 的资源</li>
<li>预留 2GiB 内存和 2C 资源供将来分配</li>
</ul>
</li>
<li><p>限制 test 命名空间使用 1C 和 1GiB 内存。允许 prod 命名空间使用任意数量，当集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，k8s 系统会遵循先到先得的原则。但不管是自愿竞争还是配额修改，都不会影响已经创建的资源使用对象</p>
</li>
</ul>
<h3 id="13-1-1-计算资源配额"><a href="#13-1-1-计算资源配额" class="headerlink" title="13.1.1 计算资源配额"></a>13.1.1 <strong>计算资源配额</strong></h3><ul>
<li>用户可以对给定命名空间下的可被请求的 计算资源 总量进行限制</li>
</ul>
<escape>
<table>
  <tr><th>资源名称</th><th>描述</th></tr>
  <tr><td>requests.cpu</td><td>所有非终止状态的 Pod，其 CPU  申请的总量不能超过该值</td></tr>
  <tr><td>requests.memory</td><td>所有非终止状态的 Pod，其 内存 申请的总量不能超过该值</td></tr>
  <tr><td>1limits.cpu</td><td>所有非终止状态的 Pod，其 CPU 运行期间限制总量不能超过该值</td></tr>
  <tr><td>limits.memory</td><td>所有非终止状态的 Pod，其 内存 运行期间限制总量不能超过该值</td></tr>
</table>   
</escape>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 示例</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-cpu-demo</span><br><span class="line">  namespace: ops</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    requests.cpu: <span class="string">&quot;1&quot;</span>                <span class="comment"># 在该命名空间中所有 Pod 的 CPU  申请总和不能超过 1C</span></span><br><span class="line">    requests.memory: 1Gi             <span class="comment"># 在该命名空间中所有 Pod 的内存申请总和不能超过 1GiB</span></span><br><span class="line">    limits.cpu: <span class="string">&quot;2&quot;</span>                  <span class="comment"># 在该名称空间中所有 Pod 的 CPU 限制总和不能超过 2C</span></span><br><span class="line">    limits.memory: 2Gi               <span class="comment"># 在该名称空间中所有 Pod 的内存限制总和不能超过 2GiB</span></span><br></pre></td></tr></table></figure>

<h3 id="13-1-2-存储资源配额"><a href="#13-1-2-存储资源配额" class="headerlink" title="13.1.2 存储资源配额"></a>13.1.2 <strong>存储资源配额</strong></h3><ul>
<li>用户可以对指定命名空间下的存储资源总量进行限制。此外，还可以根据相关的存储类（Storage Class）来限制存储资源的消耗</li>
</ul>
<escape>
<table>
  <tr><th>资源名称</th><th>描述</th></tr>
  <tr><td>requests.storage</td><td>所有 PVC 存储资源的需求总量不能超过该值</td></tr>
  <tr><td>persistentvolumeclaims 3</td><td>在该命名空间中所允许的 PVC 总数量</td></tr>
  <tr><td>< storage-class-name >.storageclass.storage.k8s.io/requests.storage</td><td>与 < storage-class-name > 相关的 PVC，存储请求的总和不能超过该值</td></tr>
  <tr><td>< storage-class-name >.storageclass.storage.k8s.io/persistentvolumeclaims</td><td>与 < storage-class-name > 相关的 PVC，可以存在的 PVC 总数</td></tr>
</table>   
</escape>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 示例</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: storage-demo</span><br><span class="line">  namespace: ops</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    requests.storage: 500Gi                                     <span class="comment"># 总共可以申请的 PVC 总量</span></span><br><span class="line">    persistentvolumeclaims: 20                                  <span class="comment"># 命名空间中总共能申请的 PVC 数量</span></span><br><span class="line">    nfs.storageclass.storage.k8s.io/requests.storage: 100Gi     <span class="comment"># 对 nfs 存储类型限制 100G 的申请</span></span><br><span class="line">    gfs:storageclass.storage.k8s.io/requests.storage: 100Gi     <span class="comment"># 对 gfs 存储类型限制 100Gi 的申请</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># kubectl get storageclass					# storageclass NAME 查看</span></span><br></pre></td></tr></table></figure>

<h3 id="13-1-3-对象数量配额"><a href="#13-1-3-对象数量配额" class="headerlink" title="13.1.3 对象数量配额"></a>13.1.3 <strong>对象数量配额</strong></h3><ul>
<li>当使用 count&#x2F;* 资源配额时，如果对象存在于服务器存储中，则会根据配额管理资源。这些类型的配额有助于防止存储资源耗尽。例如<ul>
<li>用户可能想根据服务器的存储能力来对服务器中 secret 的数量进行配额限制，集群中存在过多的 secret 实际上回导致服务器和控制器无法启动</li>
<li>用户可以选择对 Job 进行配额管理，以防止配置不当的 CronJob 在某命名空间中创建太多 Job 而导致集群拒绝服务</li>
</ul>
</li>
</ul>
<escape>
<table>
  <tr><th>资源名称</th><th>描述</th></tr>
  <tr><td>configmaps</td><td>在该名称空间中允许存在的 ConfigMap 总数上限</td></tr>
  <tr><td>persistentvolumeclaims</td><td>在该名称空间中允许存在的 PVC 的总数上限</td></tr>
  <tr><td>pods</td><td>在该名称空间中允许存在的非终止状态的 Pod 总数上限。包含(Filed, Succeeded)</td></tr>
  <tr><td>resourcequotas</td><td>在该名称空间中允许存在的 ResourceQuota 总数上线</td></tr>
  <tr><td>services</td><td>在该名称空间中允许存在的 service 总数上限</td></tr>
  <tr><td>rservies.loadbalancers</td><td>在该名称空间中允许存在的 LoadBalancer 类型的 Service 总数上限</td></tr>
  <tr><td>services.nodeports</td><td>在该名称空间中允许存在的 NodePort 类型的 Service 总数上限</td></tr>
  <tr><td>secrets</td><td>在该名称空间中允许存在的 Secret 总数上限</td></tr>
</table>   
</escape>

<h3 id="13-1-4-计算资源配置"><a href="#13-1-4-计算资源配置" class="headerlink" title="13.1.4 计算资源配置"></a>13.1.4 <strong>计算资源配置</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1. 创建一个名称空间，以便创建的资源和集群的其余资源相隔离</span><br><span class="line"><span class="comment"># kubectl create ns quota-ns</span></span><br><span class="line"></span><br><span class="line">2. 创建清单示例文件</span><br><span class="line"><span class="comment"># cat quota-mem-cpu.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: mem-cpu-demo-1</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    requests.cpu: <span class="string">&quot;1&quot;</span>                <span class="comment"># 在该命名空间中所有 Pod 的 CPU  申请总和不能超过 1C</span></span><br><span class="line">    requests.memory: 1Gi             <span class="comment"># 在该命名空间中所有 Pod 的内存申请总和不能超过 1GiB</span></span><br><span class="line">    limits.cpu: <span class="string">&quot;2&quot;</span>                  <span class="comment"># 在该名称空间中所有 Pod 的 CPU 限制总和不能超过 2C</span></span><br><span class="line">    limits.memory: 2Gi               <span class="comment"># 在该名称空间中所有 Pod 的内存限制总和不能超过 2GiB</span></span><br><span class="line"></span><br><span class="line">3. 查看 RequestsQuta 配置详情</span><br><span class="line"><span class="comment"># kubectl get resourcequota -n quota-ns</span></span><br><span class="line">NAME             AGE    REQUEST                                     LIMIT</span><br><span class="line">mem-cpu-demo-1   14s   requests.cpu: 0/1, requests.memory: 0/1Gi   limits.cpu: 0/2, limits.memory: 0/2Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl describe quota -n quota-ns        # Used 使用多少；Hard 限制多少</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-455.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">4. 创建第一个 Pod，申请 400m的 CPU，最大限制 800m。申请 600Mi 内存，最大限制 800Mi</span><br><span class="line"><span class="comment"># kubectl create secret docker-registry harbor-auth \</span></span><br><span class="line">  --docker-server=harbor.inadm.com \</span><br><span class="line">  --docker-username=admin \</span><br><span class="line">  --docker-password=ink8s.com \</span><br><span class="line">  --docker-email=user01@inadm.com \</span><br><span class="line">  -n quota-ns</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pod-quota-mem-cpu-1.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-demo-1</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;400m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;600Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;800m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;800Mi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-quota-mem-cpu-1.yaml</span></span><br><span class="line"></span><br><span class="line">5. 检查 ResourceQuotas</span><br><span class="line"><span class="comment"># kubectl describe -n quota-ns resourcequotas </span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-456.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">6. 创建第二个 Pod，申请 400m的 CPU，最大限制 800m。申请 700Mi 的内存，最大限制 1Gi</span><br><span class="line"><span class="comment"># cat pod-quota-mem-cpu-2.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-demo-2</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: <span class="string">&quot;400m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;700Mi&quot;</span></span><br><span class="line">      limits:</span><br><span class="line">        cpu: <span class="string">&quot;800m&quot;</span></span><br><span class="line">        memory: <span class="string">&quot;1Gi&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-quota-mem-cpu-2.yaml</span></span><br><span class="line">Error from server (Forbidden): error when creating <span class="string">&quot;pod-quota-mem-cpu-2.yaml&quot;</span>: pods <span class="string">&quot;pod-demo-2&quot;</span> is forbidden: exceeded quota: mem-cpu-demo-1, requested: requests.memory=700Mi, used: requests.memory=600Mi, limited: requests.memory=1Gi</span><br><span class="line"></span><br><span class="line">7. 可以看到 Pod 的内存请求为 700Mi。注意新的内存请求与已经使用的内存请求之和超过了内存请求的配额：600MiB + 700MiB &gt; 1GiB，所以无法创建，提示内存溢出</span><br><span class="line"></span><br><span class="line">8. 如果 Pod 没有设定 requests 和 limits 则报错。但可以通过 limitRange 来设定默认的 requests 和 limits</span><br></pre></td></tr></table></figure>

<h3 id="13-1-5-存储资源配置"><a href="#13-1-5-存储资源配置" class="headerlink" title="13.1.5 存储资源配置"></a>13.1.5 <strong><a target="_blank" rel="noopener" href="http://kubernetes.io/zh-cn/docs/concepts/policy/resource-quotas/">存储资源配置</a></strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1. 创建资源清单文件</span><br><span class="line"><span class="comment"># cat quota-storage.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: storage-demo</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    requests.storage: 10Gi			<span class="comment"># 所有存储类型的总容量</span></span><br><span class="line">    persistentvolumeclaims: 3			<span class="comment"># 所有存储类型的PVC总数</span></span><br><span class="line">    csi-rbd-sc.storageclass.storage.k8s.io/requests.storage: 8Gi</span><br><span class="line">    csi-rbd-sc.storageclass.storage.k8s.io/persistentvolumeclaims: 2</span><br><span class="line">    cephfs.storageclass.storage.k8s.io/requests.storage: 5Gi</span><br><span class="line"></span><br><span class="line">// managed-nfs-storage 动态</span><br><span class="line"><span class="comment"># kubectl get storageclass                    # 查看动态 storage 名称</span></span><br><span class="line">// nfs-storage 静态</span><br><span class="line"><span class="comment"># kubectl get pv                              # 查看静态 STORAGECLASS 名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (managed-nfs-storage + nfs-storage) 总容量不能超过总限制 10Gi，总申请条目不允许超过 3 条</span></span><br><span class="line"><span class="comment"># managed-nfs-storage 类型的 PVC 申请总和不能超过 8G，并 PVC 条目不允许超过 2</span></span><br><span class="line"><span class="comment"># nfs-storage 类型的 PVC 申请总和不能超过 5G</span></span><br><span class="line"></span><br><span class="line">2. 查看状态</span><br><span class="line"><span class="comment"># kubectl describe resourcequotas -n quota-ns                        # 由于没有静态 &quot;nfs-storage&quot;，所以没有实际执行过程</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-458.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">3. 创建第一个 PVC 申请，类型为 csi-rbd-sc，申请 5Gi</span><br><span class="line"><span class="comment"># cat pvc-quota-storage-1.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-demo-1</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pvc-quota-storage-1.yaml</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">4. 创建第二个 PVC 申请，先申请 3Gi，然后申请 2Gi，最后申请 1Gi</span><br><span class="line"><span class="comment"># cat pvc-quota-storage-2.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-demo-2</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 3Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pvc-quota-storage-2.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat pvc-quota-storage-3.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-demo-3</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pvc-quota-storage-3.yaml</span></span><br><span class="line"></span><br><span class="line">// 第 1 个 PVC 会申请成功，第 2 个PVC也会申请成功，第 3 个 PVC 会申请失败。(首先超过了 PVC 最大的申请数量，同时申请的容量也超过了最大的限制)</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-459.png" alt="upload successful"></p>
<h3 id="13-1-5-对象数量限制"><a href="#13-1-5-对象数量限制" class="headerlink" title="13.1.5 对象数量限制"></a>13.1.5 <strong>对象数量限制</strong></h3><ul>
<li>count&#x2F;&lt; resource &gt;.&lt; group &gt;: 用于非核心（core）组的资源，比如 Ingress</li>
<li>count&#x2F;&lt; resource &gt;: 用于核心组的资源</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 创建资源清单</span><br><span class="line"><span class="comment"># cat quota-counts-1.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: count-demo</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    count/pods: 2</span><br><span class="line">    count/deployments.apps: 2</span><br><span class="line">    count/services: 2</span><br><span class="line">    count/ingresses.networking.k8s.io: 1</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-460.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">2. 创建一个 Deployment，副本数为 1，然后创建 Service 以及 Ingress 资源</span><br><span class="line"><span class="comment"># cat quota-count-2.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 10Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 20m</span><br><span class="line">            memory: 20Mi</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-svc</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-nginx</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: app.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: <span class="string">&quot;/&quot;</span></span><br><span class="line">        pathType: Prefix</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: nginx-svc</span><br><span class="line">            port:</span><br><span class="line">              number: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl describe resourcequotas -n quota-ns</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-461.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">3. 创建一个 Deploy，Pod 副本数为 1，然后创建 Service 以及 Ingress 资源</span><br><span class="line"><span class="comment"># cat quota-counts-3.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-2</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-2</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-auth</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx-2</span><br><span class="line">        image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 10Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 20m</span><br><span class="line">            memory: 20Mi</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-svc-2</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx-2</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ing-nginx-2</span><br><span class="line">  namespace: quota-ns</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx.inadm.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: <span class="string">&quot;/&quot;</span></span><br><span class="line">        pathType: Prefix</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: nginx-svc-2</span><br><span class="line">            port:</span><br><span class="line">              number: 80</span><br><span class="line"></span><br><span class="line">4. 应用 yaml 文件，发现无法创建 Ingress，因为限制该名称空间最多创建一个 Ingress</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-462.png" alt="upload successful"></p>
<h2 id="13-2-LimitRange"><a href="#13-2-LimitRange" class="headerlink" title="13.2 LimitRange"></a>13.2 <strong>LimitRange</strong></h2><ul>
<li>ResourceQuota 可以对名称空间资源总使用量进行限制。但可能会出现一个 Pod 申请的 CPU 或 内存设定非常大，从而将该名称空间中的可用资源被某个 Pod 所耗尽，所以可以通过 LimitRange 来限制名称空间中的每个 Pod 或每个 Container 的最大资源、最小资源。如果创建 Pod 没有给定对应的 resources 字段，还可以设定默认值进行自动填充</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">// limitrange 示例</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: LimitRange</span><br><span class="line">metadata:</span><br><span class="line">  name: mylimits</span><br><span class="line">spec:</span><br><span class="line">  limits:</span><br><span class="line">  - <span class="built_in">type</span>: Pod               <span class="comment"># 针对 Pod 资源（一个 Pod 可以有多个 Container）</span></span><br><span class="line">    max:                    <span class="comment"># Pod 最大能申请的资源大小</span></span><br><span class="line">      cpu: <span class="string">&quot;2&quot;</span></span><br><span class="line">      memory: 2Gi</span><br><span class="line">    min:                    <span class="comment"># Pod 最低申请的资源大小</span></span><br><span class="line">      cpu: 200m</span><br><span class="line">      memory: 6Mi</span><br><span class="line">  - <span class="built_in">type</span>: Container</span><br><span class="line">    default:                <span class="comment"># 默认容器未指定资源限制，为该容器提供默认的 Limits</span></span><br><span class="line">      cpu: 300m</span><br><span class="line">      memory: 200Mi</span><br><span class="line">    defaultRequest:         <span class="comment"># 默认容器未指定资源请求，为该容器提供默认的 Requests</span></span><br><span class="line">      cpu: 200m</span><br><span class="line">      memory: 100Mi</span><br><span class="line">    max:                    <span class="comment"># 容器能指定的最大资源申请</span></span><br><span class="line">      cpu: <span class="string">&quot;1&quot;</span></span><br><span class="line">      memory: 1Gi</span><br><span class="line">    min:                    <span class="comment"># 容器申请时，最低申请的资源大小，低于该大小则报错</span></span><br><span class="line">      cpu: 100m</span><br><span class="line">      memory: 3Mi</span><br><span class="line">                            <span class="comment"># Limits 与 Requests 的比例值不能超过</span></span><br><span class="line">    maxLimitRequestRatio:   <span class="comment"># Requests 和 limits 比率，公式: Limits/Requests ≤ maxLimitRequestRatio</span></span><br><span class="line">      cpu: 5</span><br><span class="line">      memory: 4</span><br></pre></td></tr></table></figure>

<h3 id="13-2-1-LimitRange-限制场景-1"><a href="#13-2-1-LimitRange-限制场景-1" class="headerlink" title="13.2.1 LimitRange 限制场景-1"></a>13.2.1 <strong>LimitRange 限制场景-1</strong></h3><ul>
<li>为名称空间中未指定 Resources 字段的 Pod 资源，设定默认的 requests 和 limits</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1. 创建 LimitRange 资源限制</span><br><span class="line"><span class="comment"># cat limitrange-cpumem.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: LimitRange</span><br><span class="line">metadata:</span><br><span class="line">  name: limitrange-mem-1</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  limits:</span><br><span class="line">  - <span class="built_in">type</span>: Container</span><br><span class="line">    default:</span><br><span class="line">      memory: 256Mi</span><br><span class="line">      cpu: 200m</span><br><span class="line">    defaultRequest:</span><br><span class="line">      memory: 128Mi</span><br><span class="line">      cpu: 100m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f limitrange-cpumem.yaml </span></span><br><span class="line"></span><br><span class="line">2. 查看 limitrange 详情</span><br><span class="line"><span class="comment"># kubectl describe limitranges -n inadm</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-463.png" alt="upload successful"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">3. 创建一个 Pod，并且在 Pod 中不申明任何的 cpu 与 内存 的 Requests 和 Limits</span><br><span class="line"><span class="comment"># cat pod-cpu-mem.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-cpu-mem-demo-1</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f pod-cpu-mem.yaml </span></span><br><span class="line"></span><br><span class="line">4. 查看 Pod 的默认 Requests 和 Limits</span><br><span class="line"><span class="comment"># kubectl get -n inadm pod pod-cpu-mem-demo-1 -o yaml</span></span><br><span class="line">// 默认情况什么都不指定，使用LimitRange提供的  request和limits字段值</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-464.png" alt="upload successful"></p>
<h3 id="13-2-2-LimitRange-限制场景-2"><a href="#13-2-2-LimitRange-限制场景-2" class="headerlink" title="13.2.2 LimitRange 限制场景-2"></a>13.2.2 <strong>LimitRange 限制场景-2</strong></h3><ul>
<li>创建容器时，如果指定了容器的 limits，而没有指定它的 requests 会怎么样 ？</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1. 创建 Pod 的 yaml</span><br><span class="line"><span class="comment"># cat pod-mem.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-mem-demo-2</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">    resources:</span><br><span class="line">      limits:             <span class="comment"># 指定 Limits 字段</span></span><br><span class="line">        memory: 64Mi</span><br><span class="line">        cpu: 50m</span><br><span class="line"></span><br><span class="line">2. 输出结果显示，容器的 CPU、内存，对应的 Requests 被设置为对应的 Limits 想通值</span><br><span class="line"><span class="comment"># kubectl get -n inadm pod pod-mem-demo-2 -o yaml</span></span><br><span class="line">// 如果创建Pod自行指定了limits，而没有指定request。   它会使用limits的值作为request的值； 二者一样</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-465.png" alt="upload successful"></p>
<h3 id="13-2-3-LimitRange-限制场景-3"><a href="#13-2-3-LimitRange-限制场景-3" class="headerlink" title="13.2.3 LimitRange 限制场景-3"></a>13.2.3 <strong>LimitRange 限制场景-3</strong></h3><ul>
<li>声明容器的 CPU 和 内存 请求而不声明内存限制会怎样 ？</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1. 创建 Pod 的 yaml</span><br><span class="line"><span class="comment"># cat pod-mem.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-mem-demo-3</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 32Mi</span><br><span class="line">        cpu: 30m</span><br><span class="line"></span><br><span class="line">2. 输出结果显示，所创建 Pod，容器的 Requests 为 Pod 清单中声明的值。而容器的 CPU与内存 的 Limits 被设置为 200m，256Mi，此值是该名称空间的默认内存限制值</span><br><span class="line"><span class="comment"># kubectl get -n inadm pod pod-mem-demo-3 -o yaml</span></span><br><span class="line">// 如果创建Pod自行指定了requests，而没有指定limits。 request会使用用户自行指定的，而limits则会使用limitrange 中 default字段提供的值作为limits的值；</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-466.png" alt="upload successful"></p>
<h3 id="13-2-4-LimitRange-限制场景-4"><a href="#13-2-4-LimitRange-限制场景-4" class="headerlink" title="13.2.4 LimitRange 限制场景-4"></a>13.2.4 <strong>LimitRange 限制场景-4</strong></h3><ul>
<li>通过 LimitRange，限制 Pod 创建时最小申请的资源（Request），以及 Pod 运行期间最大能使用的资源（Limits）</li>
<li>这样就可以将 ResourceQuota 和 LimitRange 结合起来，从而避免因某一个 Pod 超额申请资源而造成名称空间资源被耗尽</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">1. 创建 LimitRange</span><br><span class="line"><span class="comment"># cat limitrange-mem.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: LimitRange</span><br><span class="line">metadata:</span><br><span class="line">  name: limitrange-mem-1</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  limits:</span><br><span class="line">  - <span class="built_in">type</span>: Container</span><br><span class="line">    default:</span><br><span class="line">      memory: 256Mi</span><br><span class="line">      cpu: 200m</span><br><span class="line">    defaultRequest:</span><br><span class="line">      memory: 128Mi</span><br><span class="line">      cpu: 100m</span><br><span class="line">    max:</span><br><span class="line">      cpu: 400m</span><br><span class="line">      memory: 2048Mi</span><br><span class="line">    min:</span><br><span class="line">      cpu: 10m</span><br><span class="line">      memory: 64Mi</span><br><span class="line"></span><br><span class="line">2. 创建 Pod</span><br><span class="line"><span class="comment"># cat pod-mem.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-mem-demo-4</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: harbor-auth</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: harbor.inadm.com/ops_apps_tools/nginx:1.18</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 32Mi</span><br><span class="line">        cpu: 10m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f 13-pod-mem.yaml </span></span><br><span class="line">Error from server (Forbidden): error when creating <span class="string">&quot;pod-mem.yaml&quot;</span>: pods <span class="string">&quot;pod-mem-demo-4&quot;</span> is forbidden: minimum memory usage per Container is 64Mi, but request is 32Mi</span><br><span class="line">// 因为 requests 的 memory 不满足最低申请的 64Mi，所以无法创建 Pod</span><br></pre></td></tr></table></figure>

<h3 id="13-2-5-LimitRange-限制存储"><a href="#13-2-5-LimitRange-限制存储" class="headerlink" title="13.2.5 LimitRange 限制存储"></a>13.2.5 <strong>LimitRange 限制存储</strong></h3><ul>
<li>场景说明：<ul>
<li>使用 ResourceQuota 限制名称空间中创建的 PVC 总容量，以及 PVC 的总数量</li>
<li>然后使用 LimitRange 限制每个 PVC 申请的最小容量和最大容量</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">1. 创建 ResourceQuotas 限制 PVC 的总容量以及创建的总数量</span><br><span class="line"><span class="comment"># ct resource-storage-limitrange.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: resource-storage</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    persistentvolumeclaims: 3</span><br><span class="line">    requests.storage: <span class="string">&quot;3Gi&quot;</span></span><br><span class="line"></span><br><span class="line">2. 创建 LimitRange 限制最大和最小</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: LimitRange</span><br><span class="line">metadata:</span><br><span class="line">  name: limitrange-storage</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  limits:</span><br><span class="line">  - <span class="built_in">type</span>: PersistentVolumeClaim</span><br><span class="line">    min:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">    max:</span><br><span class="line">      storage: 2Gi</span><br><span class="line"></span><br><span class="line">3. 创建 pvc 申请</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: limit-pvc</span><br><span class="line">  namespace: inadm</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: <span class="string">&quot;csi-rbd-sc&quot;</span></span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWritemany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 100Mi</span><br><span class="line"></span><br><span class="line">4. 请求 100Mi 存储空间的 PVC 将被拒绝，因为它们没有达到最小的申请 10Gi</span><br></pre></td></tr></table></figure>

<p><img src="/images/pasted-467.png" alt="upload successful"></p>

    

    
    <div class="page-reward">
      <a href="javascript:void(0);" class="page-reward-btn tooltip-top" target="_self">
        <div class="tooltip tooltip-east">
          <span class="tooltip-item">
            微信
          </span>
          <span class="tooltip-content">
            <span class="tooltip-text">
              <span class="tooltip-inner">
                <p class="reward-p"><i class="icon icon-quo-left"></i>加微信,入技术群<i
                    class="icon icon-quo-right"></i></p>
                <div class="reward-box">
                  
                  
                  <div class="reward-box-item">
                    <img class="reward-img" src="/img/wechat.jpg">
                    <span class="reward-type">微信</span>
                  </div>
                  
                </div>
              </span>
            </span>
          </span>
        </div>
      </a>
    </div>
    

    
    <div class="declare"> 
      <ul class="post-copyright">
        <li>
          <strong>本文作者：</strong>
          weixj@inadm.com
        </li>
        <li>
          <strong>本文链接：</strong>
          <a href="https://www.ink8s.com/2025/07/17/k8s-资源对象/" title="kubernetes 资源对象" target="_blank">https://www.ink8s.com/2025/07/17/k8s-资源对象/</a>
        </li>
        
        <li>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，均采用 <a href="https://github.com/JoeyBling/hexo-theme-yilia-plus/blob/master/LICENSE" rel="external nofollow" target="_blank">MIT</a> 许可协议。转载请注明出处！
        </li>
        
      </ul>
    </div>
    

  </div>
  <div class="article-info article-info-index">
    
    
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">Linux-k8s</a>
        		</li>
      		
		</ul>
	</div>

    

    

    
    
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <!-- <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a> -->
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
     <!-- <img src="//pan.baidu.com/share/qrcode?url=https://www.ink8s.com/2025/07/17/k8s-%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/" alt="微信分享二维码"> -->
    </div>
</div>

<div class="mask js-mask"></div>

    
    <div class="clearfix"></div>
  </div>
  </div>
</article>


<nav id="article-nav">
  
  
    <a href="/2025/07/17/ceph-%E4%B8%8E-kubernetes-%E9%9B%86%E6%88%90/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">ceph 与 kubernetes 集成</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-0-k8s-%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text">1.0 k8s 基础应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-deploy"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 deploy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-pod-%E9%95%9C%E5%83%8F%E6%8B%89%E5%8F%96%E7%AD%96%E7%95%A5"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 pod 镜像拉取策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E8%8E%B7%E5%8F%96%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 获取私有仓库镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-%E5%88%9B%E5%BB%BA-secret-%E8%AE%A4%E8%AF%81"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.3.1 创建 secret 认证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-2-yaml-%E9%83%A8%E7%BD%B2%E9%AA%8C%E8%AF%81"><span class="toc-number">1.3.2.</span> <span class="toc-text">1.3.2 yaml 部署验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E4%BC%A0%E9%80%92%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 传递环境变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AE%B9%E5%99%A8%E5%91%BD%E4%BB%A4%E4%B8%8E%E5%8F%82%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 自定义容器命令与参数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-0-Pod-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F"><span class="toc-number">2.</span> <span class="toc-text">2.0 Pod 生命周期</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AE%B9%E5%99%A8"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 初始化容器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-%E5%9C%BA%E6%99%AF1-%E7%AB%AF%E5%8F%A3%E6%A3%80%E6%9F%A5"><span class="toc-number">2.1.1.</span> <span class="toc-text">2.1.1 场景1-端口检查</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E5%9C%BA%E6%99%AF2-%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">2.1.2.</span> <span class="toc-text">2.1.2 场景2-内核参数优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E9%92%A9%E5%AD%90%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 钩子函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E9%92%A9%E5%AD%90%E7%A4%BA%E4%BE%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 钩子示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E9%92%A9%E5%AD%90%E5%9C%BA%E6%99%AF1"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 钩子场景1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-%E9%92%A9%E5%AD%90%E5%9C%BA%E6%99%AF2"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.2.3 钩子场景2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E6%A3%80%E6%B5%8B%E6%8E%A2%E9%92%88"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 检测探针</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-%E6%8E%A2%E9%92%88%E6%A3%80%E6%B5%8B%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.3.1.</span> <span class="toc-text">2.3.1 探针检测类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-%E6%8E%A2%E9%92%88%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.3.2 探针检查机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-3-startupProbe"><span class="toc-number">2.3.3.</span> <span class="toc-text">2.3.3 startupProbe</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-1-exec"><span class="toc-number">2.3.3.1.</span> <span class="toc-text">2.3.3.1 exec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-2-httpGet"><span class="toc-number">2.3.3.2.</span> <span class="toc-text">2.3.3.2 httpGet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-3-tcpSocket"><span class="toc-number">2.3.3.3.</span> <span class="toc-text">2.3.3.3 tcpSocket</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-4-livenessProbe"><span class="toc-number">2.3.4.</span> <span class="toc-text">2.3.4 livenessProbe</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-1-exec"><span class="toc-number">2.3.4.1.</span> <span class="toc-text">2.3.4.1 exec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-1-httpGet"><span class="toc-number">2.3.4.2.</span> <span class="toc-text">2.3.4.1 httpGet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-2-tcpSocket"><span class="toc-number">2.3.4.3.</span> <span class="toc-text">2.3.4.2 tcpSocket</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-5-readinessProbe"><span class="toc-number">2.3.5.</span> <span class="toc-text">2.3.5 readinessProbe</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-1-exec"><span class="toc-number">2.3.5.1.</span> <span class="toc-text">2.3.5.1 exec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-2-httpGet"><span class="toc-number">2.3.5.2.</span> <span class="toc-text">2.3.5.2 httpGet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-3-tcpSocket"><span class="toc-number">2.3.5.3.</span> <span class="toc-text">2.3.5.3 tcpSocket</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-0-Pod-%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">3.</span> <span class="toc-text">3.0 Pod 资源限制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-cpu-%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 cpu 资源限制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-cpu-%E8%AF%B7%E6%B1%82%E5%92%8C%E9%99%90%E5%88%B6"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.1.1 cpu 请求和限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E8%B6%85%E8%BF%87%E8%8A%82%E7%82%B9-cpu-%E8%AF%B7%E6%B1%82"><span class="toc-number">3.1.2.</span> <span class="toc-text">3.1.2 超过节点 cpu 请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E4%B8%8D%E6%8C%87%E5%AE%9Acpu-limits"><span class="toc-number">3.1.3.</span> <span class="toc-text">3.1.2 不指定cpu limits</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%86%85%E5%AD%98%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 内存资源限制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-mem-%E8%AF%B7%E6%B1%82%E5%92%8C%E9%99%90%E5%88%B6"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 mem 请求和限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E8%B6%85%E8%BF%87%E5%AE%B9%E5%99%A8mem%E9%99%90%E5%88%B6%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 超过容器mem限制的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E8%B6%85%E8%BF%87%E8%8A%82%E7%82%B9mem%E5%88%86%E9%85%8D"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3 超过节点mem分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E6%9C%AA%E6%8C%87%E5%AE%9A%E5%86%85%E5%AD%98%E9%99%90%E5%88%B6"><span class="toc-number">3.2.4.</span> <span class="toc-text">3.2.4 未指定内存限制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Qos-%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 Qos 服务质量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E5%88%9B%E5%BB%BAGuaranteed%E7%9A%84pod"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.3.1 创建Guaranteed的pod</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E5%88%9B%E5%BB%BABurstables%E7%9A%84pod"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.3.2 创建Burstables的pod</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E5%88%9B%E5%BB%BABestEffort%E7%9A%84pod"><span class="toc-number">3.3.3.</span> <span class="toc-text">3.3.2 创建BestEffort的pod</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-%E5%88%9B%E5%BB%BA%E5%A4%9A%E4%B8%AA%E5%AE%B9%E5%99%A8pod"><span class="toc-number">3.3.4.</span> <span class="toc-text">3.3.3 创建多个容器pod</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-downward-api"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 downward api</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E5%8F%AF%E6%B3%A8%E5%85%A5%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">3.4.1.</span> <span class="toc-text">3.4.1 可注入元数据信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%B3%A8%E5%85%A5%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">3.4.2.</span> <span class="toc-text">3.4.2 环境变量注入元数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-%E5%AD%98%E5%82%A8%E5%8D%B7%E6%B3%A8%E5%85%A5%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">3.4.3.</span> <span class="toc-text">3.4.3 存储卷注入元数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-4-%E4%B8%BA%E6%B3%A8%E5%86%8C%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%85%A5pod%E5%90%8D%E7%A7%B0"><span class="toc-number">3.4.4.</span> <span class="toc-text">3.4.4 为注册服务注入pod名称</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-5-%E4%B8%BATomcat%E6%B3%A8%E5%85%A5%E5%AF%B9%E5%86%85%E5%AD%98%E9%99%90%E5%88%B6"><span class="toc-number">3.4.5.</span> <span class="toc-text">3.4.5 为Tomcat注入对内存限制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-0-deployment"><span class="toc-number">4.</span> <span class="toc-text">4.0 deployment</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-replicaset"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 replicaset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-deploy"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 deploy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-svc"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 svc</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-hpa"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 hpa</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-recreate-%E9%87%8D%E5%BB%BA%E7%AD%96%E7%95%A5"><span class="toc-number">4.5.</span> <span class="toc-text">4.5 recreate 重建策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-rollingupdate"><span class="toc-number">4.6.</span> <span class="toc-text">4.6 rollingupdate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7-rollout"><span class="toc-number">4.7.</span> <span class="toc-text">4.7 rollout</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-8-deploy-%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5"><span class="toc-number">4.8.</span> <span class="toc-text">4.8 deploy 更新策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-1-maxSurge"><span class="toc-number">4.8.1.</span> <span class="toc-text">4.8.1 maxSurge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-2-maxUnavailable"><span class="toc-number">4.8.2.</span> <span class="toc-text">4.8.2 maxUnavailable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-3-surge-%E5%92%8C-unavailable"><span class="toc-number">4.8.3.</span> <span class="toc-text">4.8.3 surge 和 unavailable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-4-paused-%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0"><span class="toc-number">4.8.4.</span> <span class="toc-text">4.8.4 paused 暂停更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-5-minreadyseconds"><span class="toc-number">4.8.5.</span> <span class="toc-text">4.8.5 minreadyseconds</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-6-revisionhistorylimit"><span class="toc-number">4.8.6.</span> <span class="toc-text">4.8.6 revisionhistorylimit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-7-progressdeadlineSeconds"><span class="toc-number">4.8.7.</span> <span class="toc-text">4.8.7 progressdeadlineSeconds</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-9-deploy-%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83"><span class="toc-number">4.9.</span> <span class="toc-text">4.9 deploy 灰度发布</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-1-deploy-v1-0"><span class="toc-number">4.9.1.</span> <span class="toc-text">4.9.1 deploy v1.0</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-2-deploy-v1-1"><span class="toc-number">4.9.2.</span> <span class="toc-text">4.9.2 deploy v1.1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-3-%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="toc-number">4.9.3.</span> <span class="toc-text">4.9.3 实现方式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-0-daemonset"><span class="toc-number">5.</span> <span class="toc-text">5.0 daemonset</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E9%83%A8%E7%BD%B2-node-exporter"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 部署 node_exporter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-ds-%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 ds 更新策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1-RollingUpdate"><span class="toc-number">5.3.1.</span> <span class="toc-text">5.3.1 RollingUpdate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2-OnDelete"><span class="toc-number">5.3.2.</span> <span class="toc-text">5.3.2 OnDelete</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-0-job%E3%80%81cronjob"><span class="toc-number">6.</span> <span class="toc-text">6.0 job、cronjob</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-job-%E5%9F%BA%E7%A1%80%E8%B5%84%E6%BA%90"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 job 基础资源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-job-%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 job 示例代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E5%B9%B6%E8%A1%8C%E8%AF%BB%E5%8F%96RabbitMQ%E6%95%B0%E6%8D%AE%E6%BC%94%E7%A4%BA"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 并行读取RabbitMQ数据演示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-1-%E5%88%9B%E5%BB%BA-RabbitMQ-%E6%9C%8D%E5%8A%A1"><span class="toc-number">6.3.1.</span> <span class="toc-text">6.3.1 创建 RabbitMQ 服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-2-%E6%B6%88%E6%81%AF%E5%8F%91%E5%B8%83%E8%80%85"><span class="toc-number">6.3.2.</span> <span class="toc-text">6.3.2 消息发布者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-3-%E6%B6%88%E6%81%AF%E8%AE%A2%E9%98%85"><span class="toc-number">6.3.3.</span> <span class="toc-text">6.3.3 消息订阅</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E5%B9%B6%E8%A1%8C%E8%AF%BB%E5%8F%96redis%E6%95%B0%E6%8D%AE"><span class="toc-number">6.4.</span> <span class="toc-text">6.4 并行读取redis数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-1-%E5%88%9B%E5%BB%BA-Redis-%E6%9C%8D%E5%8A%A1"><span class="toc-number">6.4.1.</span> <span class="toc-text">6.4.1 创建 Redis 服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-2-%E6%B6%88%E6%81%AF%E5%8F%91%E5%B8%83%E8%80%85"><span class="toc-number">6.4.2.</span> <span class="toc-text">6.4.2 消息发布者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-3-%E6%B6%88%E6%81%AF%E8%AE%A2%E9%98%85"><span class="toc-number">6.4.3.</span> <span class="toc-text">6.4.3 消息订阅</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-contjob"><span class="toc-number">6.5.</span> <span class="toc-text">6.5 contjob</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-1-CrobJob-%E5%9F%BA%E7%A1%80%E8%B5%84%E6%BA%90"><span class="toc-number">6.5.1.</span> <span class="toc-text">6.5.1 CrobJob 基础资源</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-%E6%AF%8F%E5%88%86%E9%92%9F%E4%BB%8Eredis%E9%98%9F%E5%88%97%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">6.6.</span> <span class="toc-text">6.6 每分钟从redis队列获取数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-1-%E6%B6%88%E6%81%AF%E5%8F%91%E5%B8%83%E8%80%85"><span class="toc-number">6.6.1.</span> <span class="toc-text">6.6.1 消息发布者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-2-%E6%B6%88%E6%81%AF%E5%8F%91%E5%B8%83%E8%80%85"><span class="toc-number">6.6.2.</span> <span class="toc-text">6.6.2 消息发布者</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-0-service"><span class="toc-number">7.</span> <span class="toc-text">7.0 service</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 配置示例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-1-ClusterIP"><span class="toc-number">7.1.1.</span> <span class="toc-text">7.1.1 ClusterIP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-2-NodePort"><span class="toc-number">7.1.2.</span> <span class="toc-text">7.1.2 NodePort</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-3-ExternalName"><span class="toc-number">7.1.3.</span> <span class="toc-text">7.1.3 ExternalName</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E8%87%AA%E5%AE%9A%E4%B9%89endpoint"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 自定义endpoint</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-service%E7%9B%B8%E5%85%B3%E5%AD%97%E6%AE%B5"><span class="toc-number">7.3.</span> <span class="toc-text">7.3 service相关字段</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-1-sessionAffinity"><span class="toc-number">7.3.1.</span> <span class="toc-text">7.3.1 sessionAffinity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-2-externalTrafficPolicy"><span class="toc-number">7.3.2.</span> <span class="toc-text">7.3.2 externalTrafficPolicy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-3-internalTrafficPolicy"><span class="toc-number">7.3.3.</span> <span class="toc-text">7.3.3 internalTrafficPolicy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-4-publishNotReadyAddresses"><span class="toc-number">7.3.4.</span> <span class="toc-text">7.3.4 publishNotReadyAddresses</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4-coredns-%E7%AD%96%E7%95%A5"><span class="toc-number">7.4.</span> <span class="toc-text">7.4 coredns 策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-1-ClusterFirst"><span class="toc-number">7.4.1.</span> <span class="toc-text">7.4.1 ClusterFirst</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-2-ClusterFirstWithHostNet"><span class="toc-number">7.4.2.</span> <span class="toc-text">7.4.2 ClusterFirstWithHostNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-3-Default"><span class="toc-number">7.4.3.</span> <span class="toc-text">7.4.3 Default</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-4-None"><span class="toc-number">7.4.4.</span> <span class="toc-text">7.4.4 None</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-5-headless"><span class="toc-number">7.5.</span> <span class="toc-text">7.5 headless</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-0-ingress"><span class="toc-number">8.</span> <span class="toc-text">8.0 ingress</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-ingress-%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">8.1.</span> <span class="toc-text">8.1 ingress 资源清单</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-ingress-%E5%9F%BA%E4%BA%8E-url-%E5%AE%9E%E7%8E%B0%E8%B7%AF%E7%94%B1"><span class="toc-number">8.2.</span> <span class="toc-text">8.2 ingress 基于 url 实现路由</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-1-demoapp-%E5%BA%94%E7%94%A8"><span class="toc-number">8.2.1.</span> <span class="toc-text">8.2.1 demoapp 应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-2-tomcat-%E5%BA%94%E7%94%A8"><span class="toc-number">8.2.2.</span> <span class="toc-text">8.2.2 tomcat 应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-3-ing-%E9%85%8D%E7%BD%AE"><span class="toc-number">8.2.3.</span> <span class="toc-text">8.2.3 ing 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-4-%E9%AA%8C%E8%AF%81"><span class="toc-number">8.2.4.</span> <span class="toc-text">8.2.4 验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-%E5%A4%9A%E5%9F%9F%E5%90%8D"><span class="toc-number">8.3.</span> <span class="toc-text">8.3 多域名</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-1-app-%E5%BA%94%E7%94%A8"><span class="toc-number">8.3.1.</span> <span class="toc-text">8.3.1 app 应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-2-tomcat-%E5%BA%94%E7%94%A8"><span class="toc-number">8.3.2.</span> <span class="toc-text">8.3.2 tomcat 应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-2-%E9%AA%8C%E8%AF%81"><span class="toc-number">8.3.3.</span> <span class="toc-text">8.3.2 验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-ing-%E5%AE%9E%E7%8E%B0-https"><span class="toc-number">8.4.</span> <span class="toc-text">8.4 ing 实现 https</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-ing-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE"><span class="toc-number">8.5.</span> <span class="toc-text">8.5 ing 自定义配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-1-%E8%87%AA%E5%AE%9A%E4%B9%89-basic-%E8%AE%A4%E8%AF%81"><span class="toc-number">8.5.1.</span> <span class="toc-text">8.5.1 自定义 basic 认证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-2-%E5%85%A8%E5%B1%80%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">8.5.2.</span> <span class="toc-text">8.5.2 全局参数优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-0-configmap"><span class="toc-number">9.</span> <span class="toc-text">9.0 configmap</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-%E5%88%9B%E5%BB%BA-cm"><span class="toc-number">9.1.</span> <span class="toc-text">9.1 创建 cm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-1-%E5%91%BD%E4%BB%A4%E5%88%9B%E5%BB%BA-cm"><span class="toc-number">9.1.1.</span> <span class="toc-text">9.1.1 命令创建 cm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-2-%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA-cm"><span class="toc-number">9.1.2.</span> <span class="toc-text">9.1.2 文件创建 cm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-3-%E7%9B%AE%E5%BD%95%E5%88%9B%E5%BB%BA-cm"><span class="toc-number">9.1.3.</span> <span class="toc-text">9.1.3 目录创建 cm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-4-yaml-%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA-cm"><span class="toc-number">9.1.4.</span> <span class="toc-text">9.1.4 yaml 文件创建 cm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-%E5%BC%95%E7%94%A8-cm"><span class="toc-number">9.2.</span> <span class="toc-text">9.2 引用 cm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-1-env-%E5%BC%95%E7%94%A8%E5%8F%98%E9%87%8F"><span class="toc-number">9.2.1.</span> <span class="toc-text">9.2.1 env 引用变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-2-%E6%8C%82%E8%BD%BD%E5%8D%B7%E5%BC%95%E7%94%A8-cm"><span class="toc-number">9.2.2.</span> <span class="toc-text">9.2.2 挂载卷引用 cm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-3-%E5%BC%95%E7%94%A8%E5%AD%98%E5%82%A8%E5%8D%B7%E9%83%A8%E5%88%86%E9%94%AE%E5%80%BC"><span class="toc-number">9.2.3.</span> <span class="toc-text">9.2.3 引用存储卷部分键值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-4-%E5%BC%95%E7%94%A8%E5%AD%98%E5%82%A8%E5%8D%B7%E5%8D%95%E4%B8%AA%E9%94%AE%E5%80%BC"><span class="toc-number">9.2.4.</span> <span class="toc-text">9.2.4 引用存储卷单个键值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-redis-%E7%BB%93%E5%90%88-cm"><span class="toc-number">9.3.</span> <span class="toc-text">9.3 redis 结合 cm</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-0-secret"><span class="toc-number">10.</span> <span class="toc-text">10.0 secret</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-%E5%88%9B%E5%BB%BA-secret"><span class="toc-number">10.1.</span> <span class="toc-text">10.1 创建 secret</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-1-%E5%91%BD%E4%BB%A4%E5%88%9B%E5%BB%BA-secret"><span class="toc-number">10.1.1.</span> <span class="toc-text">10.1.1 命令创建 secret</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-2-%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA-Secret"><span class="toc-number">10.1.2.</span> <span class="toc-text">10.1.2 文件创建 Secret</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-3-TLS-secret"><span class="toc-number">10.1.3.</span> <span class="toc-text">10.1.3 TLS secret</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-4-docker-registry"><span class="toc-number">10.1.4.</span> <span class="toc-text">10.1.4 docker registry</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-5-secret-%E6%B8%85%E5%8D%95"><span class="toc-number">10.1.5.</span> <span class="toc-text">10.1.5 secret 清单</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-pod-%E5%BC%95%E7%94%A8-secret"><span class="toc-number">10.2.</span> <span class="toc-text">10.2 pod 引用 secret</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-1-env-%E5%BC%95%E7%94%A8-Secret"><span class="toc-number">10.2.1.</span> <span class="toc-text">10.2.1 env 引用 Secret</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-2-MySQL-%E6%B3%A8%E5%85%A5%E5%AF%86%E7%A0%81"><span class="toc-number">10.2.2.</span> <span class="toc-text">10.2.2 MySQL 注入密码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-3-%E5%BC%95%E7%94%A8%E6%95%B4%E4%B8%AA%E5%AD%98%E5%82%A8%E5%8D%B7"><span class="toc-number">10.2.3.</span> <span class="toc-text">10.2.3 引用整个存储卷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-4-%E5%BC%95%E7%94%A8%E9%83%A8%E5%88%86%E5%AD%98%E5%82%A8%E5%8D%B7"><span class="toc-number">10.2.4.</span> <span class="toc-text">10.2.4 引用部分存储卷</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-nginx-%E5%9F%BA%E4%BA%8E-secret-%E5%AE%9E%E7%8E%B0-tls"><span class="toc-number">10.3.</span> <span class="toc-text">10.3 nginx 基于 secret 实现 tls</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-0-statefulset"><span class="toc-number">11.</span> <span class="toc-text">11.0 statefulset</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-sts-%E5%AE%9E%E8%B7%B5"><span class="toc-number">11.1.</span> <span class="toc-text">11.1 sts 实践</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-sts-%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5"><span class="toc-number">11.2.</span> <span class="toc-text">11.2 sts 更新策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-1-%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0"><span class="toc-number">11.2.1.</span> <span class="toc-text">11.2.1 滚动更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-2-%E5%88%86%E5%8C%BA%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0"><span class="toc-number">11.2.2.</span> <span class="toc-text">11.2.2 分区滚动更新</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-0-%E6%8E%88%E6%9D%83%E4%B8%8E%E8%AE%A4%E8%AF%81"><span class="toc-number">12.</span> <span class="toc-text">12.0 授权与认证</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-SA-%E8%AE%A4%E8%AF%81%E5%AE%9E%E8%B7%B5"><span class="toc-number">12.1.</span> <span class="toc-text">12.1 SA 认证实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-1-%E5%88%9B%E5%BB%BA-sa-%E5%B9%B6%E4%B8%8E-Pod-%E5%85%B3%E8%81%94"><span class="toc-number">12.1.1.</span> <span class="toc-text">12.1.1 创建 sa 并与 Pod 关联</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-2-sa-%E6%B7%BB%E5%8A%A0%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E8%AE%A4%E8%AF%81"><span class="toc-number">12.1.2.</span> <span class="toc-text">12.1.2 sa 添加私有仓库认证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-k8s-%E5%9F%BA%E4%BA%8E-user-%E8%AE%A4%E8%AF%81"><span class="toc-number">12.2.</span> <span class="toc-text">12.2 k8s 基于 user 认证</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-1-%E8%87%AA%E5%AE%9A%E4%B9%89-kubeconfig"><span class="toc-number">12.2.1.</span> <span class="toc-text">12.2.1 自定义 kubeconfig</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-2-%E5%88%9B%E5%BB%BA%E6%96%B0%E7%9A%84%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">12.2.2.</span> <span class="toc-text">12.2.2 创建新的集群配置文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-RBAC-%E6%8E%88%E6%9D%83"><span class="toc-number">12.3.</span> <span class="toc-text">12.3 RBAC 授权</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-1-RBAC-%E5%9C%BA%E6%99%AF-1"><span class="toc-number">12.3.1.</span> <span class="toc-text">12.3.1 RBAC 场景-1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-2-RBAC-%E5%9C%BA%E6%99%AF-2"><span class="toc-number">12.3.2.</span> <span class="toc-text">12.3.2 RBAC 场景-2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-3-RBAC-%E5%9C%BA%E6%99%AF-3"><span class="toc-number">12.3.3.</span> <span class="toc-text">12.3.3 RBAC 场景-3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-4-%E5%86%85%E7%BD%AEClusterRole"><span class="toc-number">12.3.4.</span> <span class="toc-text">12.3.4 内置ClusterRole</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-3-4-1-Cluster-admin-%E5%AE%9E%E8%B7%B5"><span class="toc-number">12.3.4.1.</span> <span class="toc-text">12.3.4.1 Cluster-admin 实践</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-0-%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6"><span class="toc-number">13.</span> <span class="toc-text">13.0 准入控制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-ResoucesQuta"><span class="toc-number">13.1.</span> <span class="toc-text">13.1 ResoucesQuta</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-1-%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E9%85%8D%E9%A2%9D"><span class="toc-number">13.1.1.</span> <span class="toc-text">13.1.1 计算资源配额</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-2-%E5%AD%98%E5%82%A8%E8%B5%84%E6%BA%90%E9%85%8D%E9%A2%9D"><span class="toc-number">13.1.2.</span> <span class="toc-text">13.1.2 存储资源配额</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-3-%E5%AF%B9%E8%B1%A1%E6%95%B0%E9%87%8F%E9%85%8D%E9%A2%9D"><span class="toc-number">13.1.3.</span> <span class="toc-text">13.1.3 对象数量配额</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-4-%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E9%85%8D%E7%BD%AE"><span class="toc-number">13.1.4.</span> <span class="toc-text">13.1.4 计算资源配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-5-%E5%AD%98%E5%82%A8%E8%B5%84%E6%BA%90%E9%85%8D%E7%BD%AE"><span class="toc-number">13.1.5.</span> <span class="toc-text">13.1.5 存储资源配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-5-%E5%AF%B9%E8%B1%A1%E6%95%B0%E9%87%8F%E9%99%90%E5%88%B6"><span class="toc-number">13.1.6.</span> <span class="toc-text">13.1.5 对象数量限制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-LimitRange"><span class="toc-number">13.2.</span> <span class="toc-text">13.2 LimitRange</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-1-LimitRange-%E9%99%90%E5%88%B6%E5%9C%BA%E6%99%AF-1"><span class="toc-number">13.2.1.</span> <span class="toc-text">13.2.1 LimitRange 限制场景-1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-2-LimitRange-%E9%99%90%E5%88%B6%E5%9C%BA%E6%99%AF-2"><span class="toc-number">13.2.2.</span> <span class="toc-text">13.2.2 LimitRange 限制场景-2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-3-LimitRange-%E9%99%90%E5%88%B6%E5%9C%BA%E6%99%AF-3"><span class="toc-number">13.2.3.</span> <span class="toc-text">13.2.3 LimitRange 限制场景-3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-4-LimitRange-%E9%99%90%E5%88%B6%E5%9C%BA%E6%99%AF-4"><span class="toc-number">13.2.4.</span> <span class="toc-text">13.2.4 LimitRange 限制场景-4</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-5-LimitRange-%E9%99%90%E5%88%B6%E5%AD%98%E5%82%A8"><span class="toc-number">13.2.5.</span> <span class="toc-text">13.2.5 LimitRange 限制存储</span></a></li></ol></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>










  <div id="giteement-ctn"></div>
  
    <link rel="stylesheet" href="/lib/default.css">
    <script src="/lib/giteement.browser.js"></script>
  
  <script>
  <!-- id: "kubernetes 资源对象" -->
  var giteement = new Giteement({
    id: '20250717213649',
    owner: 'weixj90',
    repo: 'ink8s-msg',
    backcall_uri: 'https://www.ink8s.com',
    oauth_uri: 'https://cors-anywhere.herokuapp.com/https://gitee.com/oauth/token',
    oauth: {
      client_id: '50776f958ea15d74ac6d512eeadb5d45fce926378764764cb8d6998f95617e18',
      client_secret: '5b53bcab62e8a4c917eab413aa2dc40ec6b2b69344411c51bf11285bbe734d8c'
    },
  })
  giteement.render('giteement-ctn')
  </script>








          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2023-2025 <a href="https://www.ink8s.com/" target="_blank">weixj@inadm.com</a>
    	</div>
      	<div class="footer-right">
			
			
      		GitHub:<a href="https://github.com/JoeyBling/hexo-theme-yilia-plus" target="_blank">hexo-theme-yilia-plus</a> by Litten
      	</div>
    </div>
  </div>
  
  
	<script src="/lib/busuanzi.pure.js"></script>
	
  
  
	
	<span id="busuanzi_container_site_pv" style="display:none">
		本站总访问量<span id="busuanzi_value_site_pv"></span>次	
	        <span class="post-meta-divider" >|</span>
	</span>
  	<span id="busuanzi_container_site_uv" style='display:none'>
  		本站访客数<span id="busuanzi_value_site_uv"></span>人
  	</span>
  
</footer>

    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: true
	}
</script>

<script>!function(r){function e(t){if(i[t])return i[t].exports;var n=i[t]={exports:{},id:t,loaded:!1};return r[t].call(n.exports,n,n.exports,e),n.loaded=!0,n.exports}var i={};e.m=r,e.c=i,e.p="./",e(0)}([function(t,n,r){r(208),t.exports=r(205)},function(t,n,r){var d=r(3),y=r(46),g=r(26),b=r(27),x=r(47),m="prototype",S=function(t,n,r){var e,i,o,u,c=t&S.F,f=t&S.G,a=t&S.S,s=t&S.P,l=t&S.B,h=f?d:a?d[n]||(d[n]={}):(d[n]||{})[m],v=f?y:y[n]||(y[n]={}),p=v[m]||(v[m]={});for(e in f&&(r=n),r)o=((i=!c&&h&&void 0!==h[e])?h:r)[e],u=l&&i?x(o,d):s&&"function"==typeof o?x(Function.call,o):o,h&&b(h,e,o,t&S.U),v[e]!=o&&g(v,e,u),s&&p[e]!=o&&(p[e]=o)};d.core=y,S.F=1,S.G=2,S.S=4,S.P=8,S.B=16,S.W=32,S.U=64,S.R=128,t.exports=S},function(t,n,r){var e=r(5);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n,r){var e=r(118)("wks"),i=r(79),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(49),i=Math.min;t.exports=function(t){return 0<t?i(e(t),9007199254740991):0}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(174),o=r(53),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(20)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(24);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(22),i=r(60),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(96),i=r(34);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(40)("wks"),i=r(25),o=r(6).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(51);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){var r=t.exports={version:"2.6.9"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(18);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=!0},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(11),i=r(75);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var o=r(3),u=r(26),c=r(30),f=r(79)("src"),e=r(219),i="toString",a=(""+e).split(i);r(46).inspectSource=function(t){return e.call(t)},(t.exports=function(t,n,r,e){var i="function"==typeof r;i&&(c(r,"name")||u(r,"name",n)),t[n]!==r&&(i&&(c(r,f)||u(r,f,t[n]?""+t[n]:a.join(String(n)))),t===o?t[n]=r:e?t[n]?t[n]=r:u(t,n,r):(delete t[n],u(t,n,r)))})(Function.prototype,i,function(){return"function"==typeof this&&this[f]||e.call(this)})},function(t,n,r){var e=r(1),i=r(4),u=r(51),c=/"/g,o=function(t,n,r,e){var i=String(u(t)),o="<"+n;return""!==r&&(o+=" "+r+'="'+String(e).replace(c,"&quot;")+'"'),o+">"+i+"</"+n+">"};t.exports=function(n,t){var r={};r[n]=t(o),e(e.P+e.F*i(function(){var t=""[n]('"');return t!==t.toLowerCase()||3<t.split('"').length}),"String",r)}},function(t,n,r){var e=r(65),i=r(35);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(117),i=r(75),o=r(33),u=r(53),c=r(30),f=r(174),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(30),i=r(17),o=r(154)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(116),i=r(51);t.exports=function(t){return e(i(t))}},function(t,n){t.exports=function(t){if(null==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(9),o=r(16)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(25);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(19),i=r(6),o="__core-js_shared__",u=i[o]||(i[o]={});(t.exports=function(t,n){return u[t]||(u[t]=void 0!==n?n:{})})("versions",[]).push({version:e.version,mode:r(23)?"pure":"global",copyright:"© 2019 Denis Pushkarev (zloirock.ru)"})},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(0<t?e:r)(t)}},function(t,n,r){var i=r(18);t.exports=function(t,n){if(!i(t))return t;var r,e;if(n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;if("function"==typeof(r=t.valueOf)&&!i(e=r.call(t)))return e;if(!n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(6),i=r(19),o=r(23),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(16)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){var r=t.exports={version:"2.6.9"};"number"==typeof __e&&(__e=r)},function(t,n,r){var o=r(21);t.exports=function(e,i,t){if(o(e),void 0===i)return e;switch(t){case 1:return function(t){return e.call(i,t)};case 2:return function(t,n){return e.call(i,t,n)};case 3:return function(t,n,r){return e.call(i,t,n,r)}}return function(){return e.apply(i,arguments)}}},function(t,n,r){"use strict";var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(0<t?e:r)(t)}},function(t,n,r){var x=r(47),m=r(116),S=r(17),w=r(8),e=r(138);t.exports=function(l,t){var h=1==l,v=2==l,p=3==l,d=4==l,y=6==l,g=5==l||y,b=t||e;return function(t,n,r){for(var e,i,o=S(t),u=m(o),c=x(n,r,3),f=w(u.length),a=0,s=h?b(t,f):v?b(t,0):void 0;a<f;a++)if((g||a in u)&&(i=c(e=u[a],a,o),l))if(h)s[a]=i;else if(i)switch(l){case 3:return!0;case 5:return e;case 6:return a;case 2:s.push(e)}else if(d)return!1;return y?-1:p||d?d:s}}},function(t,n){t.exports=function(t){if(null==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var i=r(1),o=r(46),u=r(4);t.exports=function(t,n){var r=(o.Object||{})[t]||Object[t],e={};e[t]=n(r),i(i.S+i.F*u(function(){r(1)}),"Object",e)}},function(t,n,r){var i=r(5);t.exports=function(t,n){if(!i(t))return t;var r,e;if(n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;if("function"==typeof(r=t.valueOf)&&!i(e=r.call(t)))return e;if(!n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var d=r(6),y=r(19),g=r(93),b=r(13),x=r(9),m="prototype",S=function(t,n,r){var e,i,o,u=t&S.F,c=t&S.G,f=t&S.S,a=t&S.P,s=t&S.B,l=t&S.W,h=c?y:y[n]||(y[n]={}),v=h[m],p=c?d:f?d[n]:(d[n]||{})[m];for(e in c&&(r=n),r)(i=!u&&p&&void 0!==p[e])&&x(h,e)||(o=i?p[e]:r[e],h[e]=c&&"function"!=typeof p[e]?r[e]:s&&i?g(o,d):l&&p[e]==o?function(e){var t=function(t,n,r){if(this instanceof e){switch(arguments.length){case 0:return new e;case 1:return new e(t);case 2:return new e(t,n)}return new e(t,n,r)}return e.apply(this,arguments)};return t[m]=e[m],t}(o):a&&"function"==typeof o?g(Function.call,o):o,a&&((h.virtual||(h.virtual={}))[e]=o,t&S.R&&v&&!v[e]&&b(v,e,o)))};S.F=1,S.G=2,S.S=4,S.P=8,S.B=16,S.W=32,S.U=64,S.R=128,t.exports=S},function(t,n,r){var e=r(34);t.exports=function(t){return Object(e(t))}},function(t,n,r){var o=r(196),e=r(1),i=r(118)("metadata"),u=i.store||(i.store=new(r(200))),c=function(t,n,r){var e=u.get(t);if(!e){if(!r)return;u.set(t,e=new o)}var i=e.get(n);if(!i){if(!r)return;e.set(n,i=new o)}return i};t.exports={store:u,map:c,has:function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},get:function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},set:function(t,n,r,e){c(r,e,!0).set(t,n)},keys:function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},key:function(t){return void 0===t||"symbol"==typeof t?t:String(t)},exp:function(t){e(e.S,"Reflect",t)}}},function(t,n,r){"use strict";if(r(10)){var g=r(68),b=r(3),x=r(4),m=r(1),S=r(132),e=r(159),h=r(47),w=r(70),i=r(75),_=r(26),o=r(76),u=r(49),O=r(8),E=r(194),c=r(78),f=r(53),a=r(30),M=r(81),P=r(5),v=r(17),p=r(145),j=r(72),F=r(32),A=r(73).f,d=r(161),s=r(79),l=r(7),y=r(50),I=r(120),L=r(119),N=r(162),T=r(82),k=r(125),R=r(77),C=r(137),D=r(166),G=r(11),W=r(31),U=G.f,V=W.f,B=b.RangeError,q=b.TypeError,z=b.Uint8Array,K="ArrayBuffer",H="Shared"+K,J="BYTES_PER_ELEMENT",$="prototype",Y=Array[$],X=e.ArrayBuffer,Q=e.DataView,Z=y(0),tt=y(2),nt=y(3),rt=y(4),et=y(5),it=y(6),ot=I(!0),ut=I(!1),ct=N.values,ft=N.keys,at=N.entries,st=Y.lastIndexOf,lt=Y.reduce,ht=Y.reduceRight,vt=Y.join,pt=Y.sort,dt=Y.slice,yt=Y.toString,gt=Y.toLocaleString,bt=l("iterator"),xt=l("toStringTag"),mt=s("typed_constructor"),St=s("def_constructor"),wt=S.CONSTR,_t=S.TYPED,Ot=S.VIEW,Et="Wrong length!",Mt=y(1,function(t,n){return It(L(t,t[St]),n)}),Pt=x(function(){return 1===new z(new Uint16Array([1]).buffer)[0]}),jt=!!z&&!!z[$].set&&x(function(){new z(1).set({})}),Ft=function(t,n){var r=u(t);if(r<0||r%n)throw B("Wrong offset!");return r},At=function(t){if(P(t)&&_t in t)return t;throw q(t+" is not a typed array!")},It=function(t,n){if(!(P(t)&&mt in t))throw q("It is not a typed array constructor!");return new t(n)},Lt=function(t,n){return Nt(L(t,t[St]),n)},Nt=function(t,n){for(var r=0,e=n.length,i=It(t,e);r<e;)i[r]=n[r++];return i},Tt=function(t,n,r){U(t,n,{get:function(){return this._d[r]}})},kt=function(t){var n,r,e,i,o,u,c=v(t),f=arguments.length,a=1<f?arguments[1]:void 0,s=void 0!==a,l=d(c);if(null!=l&&!p(l)){for(u=l.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(s&&2<f&&(a=h(a,arguments[2],2)),n=0,r=O(c.length),i=It(this,r);n<r;n++)i[n]=s?a(c[n],n):c[n];return i},Rt=function(){for(var t=0,n=arguments.length,r=It(this,n);t<n;)r[t]=arguments[t++];return r},Ct=!!z&&x(function(){gt.call(new z(1))}),Dt=function(){return gt.apply(Ct?dt.call(At(this)):At(this),arguments)},Gt={copyWithin:function(t,n){return D.call(At(this),t,n,2<arguments.length?arguments[2]:void 0)},every:function(t){return rt(At(this),t,1<arguments.length?arguments[1]:void 0)},fill:function(t){return C.apply(At(this),arguments)},filter:function(t){return Lt(this,tt(At(this),t,1<arguments.length?arguments[1]:void 0))},find:function(t){return et(At(this),t,1<arguments.length?arguments[1]:void 0)},findIndex:function(t){return it(At(this),t,1<arguments.length?arguments[1]:void 0)},forEach:function(t){Z(At(this),t,1<arguments.length?arguments[1]:void 0)},indexOf:function(t){return ut(At(this),t,1<arguments.length?arguments[1]:void 0)},includes:function(t){return ot(At(this),t,1<arguments.length?arguments[1]:void 0)},join:function(t){return vt.apply(At(this),arguments)},lastIndexOf:function(t){return st.apply(At(this),arguments)},map:function(t){return Mt(At(this),t,1<arguments.length?arguments[1]:void 0)},reduce:function(t){return lt.apply(At(this),arguments)},reduceRight:function(t){return ht.apply(At(this),arguments)},reverse:function(){for(var t,n=this,r=At(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(At(this),t,1<arguments.length?arguments[1]:void 0)},sort:function(t){return pt.call(At(this),t)},subarray:function(t,n){var r=At(this),e=r.length,i=c(t,e);return new(L(r,r[St]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,O((void 0===n?e:c(n,e))-i))}},Wt=function(t,n){return Lt(this,dt.call(At(this),t,n))},Ut=function(t){At(this);var n=Ft(arguments[1],1),r=this.length,e=v(t),i=O(e.length),o=0;if(r<i+n)throw B(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(At(this))},keys:function(){return ft.call(At(this))},values:function(){return ct.call(At(this))}},Bt=function(t,n){return P(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return Bt(t,n=f(n,!0))?i(2,t[n]):V(t,n)},zt=function(t,n,r){return!(Bt(t,n=f(n,!0))&&P(r)&&a(r,"value"))||a(r,"get")||a(r,"set")||r.configurable||a(r,"writable")&&!r.writable||a(r,"enumerable")&&!r.enumerable?U(t,n,r):(t[n]=r.value,t)};wt||(W.f=qt,G.f=zt),m(m.S+m.F*!wt,"Object",{getOwnPropertyDescriptor:qt,defineProperty:zt}),x(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Kt=o({},Gt);o(Kt,Vt),_(Kt,bt,Vt.values),o(Kt,{slice:Wt,set:Ut,constructor:function(){},toString:yt,toLocaleString:Dt}),Tt(Kt,"buffer","b"),Tt(Kt,"byteOffset","o"),Tt(Kt,"byteLength","l"),Tt(Kt,"length","e"),U(Kt,xt,{get:function(){return this[_t]}}),t.exports=function(t,l,n,o){var h=t+((o=!!o)?"Clamped":"")+"Array",r="get"+t,u="set"+t,v=b[h],c=v||{},e=v&&F(v),i=!v||!S.ABV,f={},a=v&&v[$],p=function(t,i){U(t,i,{get:function(){return t=i,(n=this._d).v[r](t*l+n.o,Pt);var t,n},set:function(t){return n=i,r=t,e=this._d,o&&(r=(r=Math.round(r))<0?0:255<r?255:255&r),void e.v[u](n*l+e.o,r,Pt);var n,r,e},enumerable:!0})};i?(v=n(function(t,n,r,e){w(t,v,h,"_d");var i,o,u,c,f=0,a=0;if(P(n)){if(!(n instanceof X||(c=M(n))==K||c==H))return _t in n?Nt(v,n):kt.call(v,n);i=n,a=Ft(r,l);var s=n.byteLength;if(void 0===e){if(s%l)throw B(Et);if((o=s-a)<0)throw B(Et)}else if(s<(o=O(e)*l)+a)throw B(Et);u=o/l}else u=E(n),i=new X(o=u*l);for(_(t,"_d",{b:i,o:a,l:o,e:u,v:new Q(i)});f<u;)p(t,f++)}),a=v[$]=j(Kt),_(a,"constructor",v)):x(function(){v(1)})&&x(function(){new v(-1)})&&k(function(t){new v,new v(null),new v(1.5),new v(t)},!0)||(v=n(function(t,n,r,e){var i;return w(t,v,h),P(n)?n instanceof X||(i=M(n))==K||i==H?void 0!==e?new c(n,Ft(r,l),e):void 0!==r?new c(n,Ft(r,l)):new c(n):_t in n?Nt(v,n):kt.call(v,n):new c(E(n))}),Z(e!==Function.prototype?A(c).concat(A(e)):A(c),function(t){t in v||_(v,t,c[t])}),v[$]=a,g||(a.constructor=v));var s=a[bt],d=!!s&&("values"==s.name||null==s.name),y=Vt.values;_(v,mt,!0),_(a,_t,h),_(a,Ot,!0),_(a,St,v),(o?new v(1)[xt]==h:xt in a)||U(a,xt,{get:function(){return h}}),f[h]=v,m(m.G+m.W+m.F*(v!=c),f),m(m.S,h,{BYTES_PER_ELEMENT:l}),m(m.S+m.F*x(function(){c.of.call(v,1)}),h,{from:kt,of:Rt}),J in a||_(a,J,l),m(m.P,h,Gt),R(h),m(m.P+m.F*jt,h,{set:Ut}),m(m.P+m.F*!d,h,Vt),g||a.toString==yt||(a.toString=yt),m(m.P+m.F*x(function(){new v(1).slice()}),h,{slice:Wt}),m(m.P+m.F*(x(function(){return[1,2].toLocaleString()!=new v([1,2]).toLocaleString()})||!x(function(){a.toLocaleString.call([1,2])})),h,{toLocaleString:Dt}),T[h]=d?s:y,g||d||_(a,bt,y)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(18),i=r(6).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(20)(function(){return 7!=Object.defineProperty(r(59)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var x=r(23),m=r(54),S=r(66),w=r(13),_=r(36),O=r(98),E=r(38),M=r(104),P=r(16)("iterator"),j=!([].keys&&"next"in[].keys()),F="values",A=function(){return this};t.exports=function(t,n,r,e,i,o,u){O(r,n,e);var c,f,a,s=function(t){if(!j&&t in p)return p[t];switch(t){case"keys":case F:return function(){return new r(this,t)}}return function(){return new r(this,t)}},l=n+" Iterator",h=i==F,v=!1,p=t.prototype,d=p[P]||p["@@iterator"]||i&&p[i],y=d||s(i),g=i?h?s("entries"):y:void 0,b="Array"==n&&p.entries||d;if(b&&((a=M(b.call(new t)))!==Object.prototype&&a.next&&(E(a,l,!0),x||"function"==typeof a[P]||w(a,P,A))),h&&d&&d.name!==F&&(v=!0,y=function(){return d.call(this)}),x&&!u||!j&&!v&&p[P]||w(p,P,y),_[n]=y,_[l]=A,i)if(c={values:h?y:s(F),keys:o?y:s("keys"),entries:g},u)for(f in c)f in p||S(p,f,c[f]);else m(m.P+m.F*(j||v),n,c);return c}},function(t,n,e){var i=e(22),o=e(101),u=e(35),c=e(39)("IE_PROTO"),f=function(){},a="prototype",s=function(){var t,n=e(59)("iframe"),r=u.length;for(n.style.display="none",e(95).appendChild(n),n.src="javascript:",(t=n.contentWindow.document).open(),t.write("<script>document.F=Object<\/script>"),t.close(),s=t.F;r--;)delete s[a][u[r]];return s()};t.exports=Object.create||function(t,n){var r;return null!==t?(f[a]=i(t),r=new f,f[a]=null,r[c]=t):r=s(),void 0===n?r:o(r,n)}},function(t,n,r){var e=r(65),i=r(35).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var u=r(9),c=r(15),f=r(92)(!1),a=r(39)("IE_PROTO");t.exports=function(t,n){var r,e=c(t),i=0,o=[];for(r in e)r!=a&&u(e,r)&&o.push(r);for(;n.length>i;)u(e,r=n[i++])&&(~f(o,r)||o.push(r));return o}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;null==i[e]&&r(26)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n){t.exports=!1},function(t,n,r){var e=r(79)("meta"),i=r(5),o=r(30),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=t.exports={KEY:e,NEED:!1,fastKey:function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},getWeak:function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},onFreeze:function(t){return a&&l.NEED&&f(t)&&!o(t,e)&&s(t),t}}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n,r){var h=r(47),v=r(177),p=r(145),d=r(2),y=r(8),g=r(161),b={},x={};(n=t.exports=function(t,n,r,e,i){var o,u,c,f,a=i?function(){return t}:g(t),s=h(r,e,n?2:1),l=0;if("function"!=typeof a)throw TypeError(t+" is not iterable!");if(p(a)){for(o=y(t.length);l<o;l++)if((f=n?s(d(u=t[l])[0],u[1]):s(t[l]))===b||f===x)return f}else for(c=a.call(t);!(u=c.next()).done;)if((f=v(c,s,u.value,n))===b||f===x)return f}).BREAK=b,n.RETURN=x},function(t,n,e){var i=e(2),o=e(183),u=e(141),c=e(154)("IE_PROTO"),f=function(){},a="prototype",s=function(){var t,n=e(140)("iframe"),r=u.length;for(n.style.display="none",e(143).appendChild(n),n.src="javascript:",(t=n.contentWindow.document).open(),t.write("<script>document.F=Object<\/script>"),t.close(),s=t.F;r--;)delete s[a][u[r]];return s()};t.exports=Object.create||function(t,n){var r;return null!==t?(f[a]=i(t),r=new f,f[a]=null,r[c]=t):r=s(),void 0===n?r:o(r,n)}},function(t,n,r){var e=r(185),i=r(141).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(185),i=r(141);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n,r){var i=r(27);t.exports=function(t,n,r){for(var e in n)i(t,e,n[e],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(49),i=Math.max,o=Math.min;t.exports=function(t,n){return(t=e(t))<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(5);t.exports=function(t,n){if(!e(t)||t._t!==n)throw TypeError("Incompatible receiver, "+n+" required!");return t}},function(t,n,r){var i=r(45),o=r(7)("toStringTag"),u="Arguments"==i(function(){return arguments}());t.exports=function(t){var n,r,e;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=function(t,n){try{return t[n]}catch(t){}}(n=Object(t),o))?r:u?i(n):"Object"==(e=i(n))&&"function"==typeof n.callee?"Arguments":e}},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(30),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var u=r(1),e=r(51),c=r(4),f=r(157),i="["+f+"]",o=RegExp("^"+i+i+"*"),a=RegExp(i+i+"*$"),s=function(t,n,r){var e={},i=c(function(){return!!f[t]()||"​"!="​"[t]()}),o=e[t]=i?n(l):f[t];r&&(e[r]=o),u(u.P+u.F*i,"String",e)},l=s.trim=function(t,n){return t=String(e(t)),1&n&&(t=t.replace(o,"")),2&n&&(t=t.replace(a,"")),t};t.exports=s},function(t,n,r){t.exports={default:r(88),__esModule:!0}},function(t,n,r){t.exports={default:r(89),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=e(r(86)),o=e(r(85)),u="function"==typeof o.default&&"symbol"==typeof i.default?function(t){return typeof t}:function(t){return t&&"function"==typeof o.default&&t.constructor===o.default&&t!==o.default.prototype?"symbol":typeof t};n.default="function"==typeof o.default&&"symbol"===u(i.default)?function(t){return void 0===t?"undefined":u(t)}:function(t){return t&&"function"==typeof o.default&&t.constructor===o.default&&t!==o.default.prototype?"symbol":void 0===t?"undefined":u(t)}},function(t,n,r){r(111),r(109),r(112),r(113),t.exports=r(19).Symbol},function(t,n,r){r(110),r(114),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var f=r(15),a=r(107),s=r(106);t.exports=function(c){return function(t,n,r){var e,i=f(t),o=a(i.length),u=s(r,o);if(c&&n!=n){for(;u<o;)if((e=i[u++])!=e)return!0}else for(;u<o;u++)if((c||u in i)&&i[u]===n)return c||u||0;return!c&&-1}}},function(t,n,r){var o=r(90);t.exports=function(e,i,t){if(o(e),void 0===i)return e;switch(t){case 1:return function(t){return e.call(i,t)};case 2:return function(t,n){return e.call(i,t,n)};case 3:return function(t,n,r){return e.call(i,t,n,r)}}return function(){return e.apply(i,arguments)}}},function(t,n,r){var c=r(29),f=r(64),a=r(37);t.exports=function(t){var n=c(t),r=f.f;if(r)for(var e,i=r(t),o=a.f,u=0;i.length>u;)o.call(t,e=i[u++])&&n.push(e);return n}},function(t,n,r){var e=r(6).document;t.exports=e&&e.documentElement},function(t,n,r){var e=r(58);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(58);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(62),i=r(24),o=r(38),u={};r(13)(u,r(16)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(25)("meta"),i=r(18),o=r(9),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(20)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=t.exports={KEY:e,NEED:!1,fastKey:function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},getWeak:function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},onFreeze:function(t){return a&&l.NEED&&f(t)&&!o(t,e)&&s(t),t}}},function(t,n,r){var u=r(14),c=r(22),f=r(29);t.exports=r(12)?Object.defineProperties:function(t,n){c(t);for(var r,e=f(n),i=e.length,o=0;o<i;)u.f(t,r=e[o++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(24),o=r(15),u=r(42),c=r(9),f=r(60),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(15),i=r(63).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?function(t){try{return i(t)}catch(t){return u.slice()}}(t):i(e(t))}},function(t,n,r){var e=r(9),i=r(55),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var f=r(41),a=r(34);t.exports=function(c){return function(t,n){var r,e,i=String(a(t)),o=f(n),u=i.length;return o<0||u<=o?c?"":void 0:(r=i.charCodeAt(o))<55296||56319<r||o+1===u||(e=i.charCodeAt(o+1))<56320||57343<e?c?i.charAt(o):r:c?i.slice(o,o+2):e-56320+(r-55296<<10)+65536}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return(t=e(t))<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return 0<t?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(91),i=r(99),o=r(36),u=r(15);t.exports=r(61)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):i(0,"keys"==n?r:"values"==n?t[r]:[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(105)(!0);r(61)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(6),u=r(9),i=r(12),o=r(54),c=r(66),f=r(100).KEY,a=r(20),s=r(40),l=r(38),h=r(25),v=r(16),p=r(44),d=r(43),y=r(94),g=r(97),b=r(22),x=r(18),m=r(55),S=r(15),w=r(42),_=r(24),O=r(62),E=r(103),M=r(102),P=r(64),j=r(14),F=r(29),A=M.f,I=j.f,L=E.f,N=e.Symbol,T=e.JSON,k=T&&T.stringify,R="prototype",C=v("_hidden"),D=v("toPrimitive"),G={}.propertyIsEnumerable,W=s("symbol-registry"),U=s("symbols"),V=s("op-symbols"),B=Object[R],q="function"==typeof N&&!!P.f,z=e.QObject,K=!z||!z[R]||!z[R].findChild,H=i&&a(function(){return 7!=O(I({},"a",{get:function(){return I(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=A(B,n);e&&delete B[n],I(t,n,r),e&&t!==B&&I(B,n,e)}:I,J=function(t){var n=U[t]=O(N[R]);return n._k=t,n},$=q&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===B&&Y(V,n,r),b(t),n=w(n,!0),b(r),u(U,n)?(r.enumerable?(u(t,C)&&t[C][n]&&(t[C][n]=!1),r=O(r,{enumerable:_(0,!1)})):(u(t,C)||I(t,C,_(1,{})),t[C][n]=!0),H(t,n,r)):I(t,n,r)},X=function(t,n){b(t);for(var r,e=y(n=S(n)),i=0,o=e.length;i<o;)Y(t,r=e[i++],n[r]);return t},Q=function(t){var n=G.call(this,t=w(t,!0));return!(this===B&&u(U,t)&&!u(V,t))&&(!(n||!u(this,t)||!u(U,t)||u(this,C)&&this[C][t])||n)},Z=function(t,n){if(t=S(t),n=w(n,!0),t!==B||!u(U,n)||u(V,n)){var r=A(t,n);return!r||!u(U,n)||u(t,C)&&t[C][n]||(r.enumerable=!0),r}},tt=function(t){for(var n,r=L(S(t)),e=[],i=0;r.length>i;)u(U,n=r[i++])||n==C||n==f||e.push(n);return e},nt=function(t){for(var n,r=t===B,e=L(r?V:S(t)),i=[],o=0;e.length>o;)!u(U,n=e[o++])||r&&!u(B,n)||i.push(U[n]);return i};q||(c((N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var n=h(0<arguments.length?arguments[0]:void 0),r=function(t){this===B&&r.call(V,t),u(this,C)&&u(this[C],n)&&(this[C][n]=!1),H(this,n,_(1,t))};return i&&K&&H(B,n,{configurable:!0,set:r}),J(n)})[R],"toString",function(){return this._k}),M.f=Z,j.f=Y,r(63).f=E.f=tt,r(37).f=Q,P.f=nt,i&&!r(23)&&c(B,"propertyIsEnumerable",Q,!0),p.f=function(t){return J(v(t))}),o(o.G+o.W+o.F*!q,{Symbol:N});for(var rt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),et=0;rt.length>et;)v(rt[et++]);for(var it=F(v.store),ot=0;it.length>ot;)d(it[ot++]);o(o.S+o.F*!q,"Symbol",{for:function(t){return u(W,t+="")?W[t]:W[t]=N(t)},keyFor:function(t){if(!$(t))throw TypeError(t+" is not a symbol!");for(var n in W)if(W[n]===t)return n},useSetter:function(){K=!0},useSimple:function(){K=!1}}),o(o.S+o.F*!q,"Object",{create:function(t,n){return void 0===n?O(t):X(O(t),n)},defineProperty:Y,defineProperties:X,getOwnPropertyDescriptor:Z,getOwnPropertyNames:tt,getOwnPropertySymbols:nt});var ut=a(function(){P.f(1)});o(o.S+o.F*ut,"Object",{getOwnPropertySymbols:function(t){return P.f(m(t))}}),T&&o(o.S+o.F*(!q||a(function(){var t=N();return"[null]"!=k([t])||"{}"!=k({a:t})||"{}"!=k(Object(t))})),"JSON",{stringify:function(t){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);if(r=n=e[1],(x(n)||void 0!==t)&&!$(t))return g(n)||(n=function(t,n){if("function"==typeof r&&(n=r.call(this,t,n)),!$(n))return n}),e[1]=n,k.apply(T,e)}}),N[R][D]||r(13)(N[R],D,N[R].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(108);for(var e=r(6),i=r(13),o=r(36),u=r(16)("toStringTag"),c="CSSRuleList,CSSStyleDeclaration,CSSValueList,ClientRectList,DOMRectList,DOMStringList,DOMTokenList,DataTransferItemList,FileList,HTMLAllCollection,HTMLCollection,HTMLFormElement,HTMLSelectElement,MediaList,MimeTypeArray,NamedNodeMap,NodeList,PaintRequestList,Plugin,PluginArray,SVGLengthList,SVGNumberList,SVGPathSegList,SVGPointList,SVGStringList,SVGTransformList,SourceBufferList,StyleSheetList,TextTrackCueList,TextTrackList,TouchList".split(","),f=0;f<c.length;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(46),i=r(3),o="__core-js_shared__",u=i[o]||(i[o]={});(t.exports=function(t,n){return u[t]||(u[t]=void 0!==n?n:{})})("versions",[]).push({version:e.version,mode:r(68)?"pure":"global",copyright:"© 2019 Denis Pushkarev (zloirock.ru)"})},function(t,n,r){var i=r(2),o=r(21),u=r(7)("species");t.exports=function(t,n){var r,e=i(t).constructor;return void 0===e||null==(r=i(e)[u])?n:o(r)}},function(t,n,r){var f=r(33),a=r(8),s=r(78);t.exports=function(c){return function(t,n,r){var e,i=f(t),o=a(i.length),u=s(r,o);if(c&&n!=n){for(;u<o;)if((e=i[u++])!=e)return!0}else for(;u<o;u++)if((c||u in i)&&i[u]===n)return c||u||0;return!c&&-1}}},function(t,n,r){"use strict";var g=r(3),b=r(1),x=r(27),m=r(76),S=r(69),w=r(71),_=r(70),O=r(5),E=r(4),M=r(125),P=r(83),j=r(144);t.exports=function(e,t,n,r,i,o){var u=g[e],c=u,f=i?"set":"add",a=c&&c.prototype,s={},l=function(t){var r=a[t];x(a,t,"delete"==t?function(t){return!(o&&!O(t))&&r.call(this,0===t?0:t)}:"has"==t?function(t){return!(o&&!O(t))&&r.call(this,0===t?0:t)}:"get"==t?function(t){return o&&!O(t)?void 0:r.call(this,0===t?0:t)}:"add"==t?function(t){return r.call(this,0===t?0:t),this}:function(t,n){return r.call(this,0===t?0:t,n),this})};if("function"==typeof c&&(o||a.forEach&&!E(function(){(new c).entries().next()}))){var h=new c,v=h[f](o?{}:-0,1)!=h,p=E(function(){h.has(1)}),d=M(function(t){new c(t)}),y=!o&&E(function(){for(var t=new c,n=5;n--;)t[f](n,n);return!t.has(-0)});d||(((c=t(function(t,n){_(t,c,e);var r=j(new u,t,c);return null!=n&&w(n,i,r[f],r),r})).prototype=a).constructor=c),(p||y)&&(l("delete"),l("has"),i&&l("get")),(y||v)&&l(f),o&&a.clear&&delete a.clear}else c=r.getConstructor(t,e,i,f),m(c.prototype,n),S.NEED=!0;return P(c,e),s[e]=c,b(b.G+b.W+b.F*(c!=u),s),o||r.setStrong(c,e,i),c}},function(t,n,r){"use strict";r(197);var s=r(27),l=r(26),h=r(4),v=r(51),p=r(7),d=r(152),y=p("species"),g=!h(function(){var t=/./;return t.exec=function(){var t=[];return t.groups={a:"7"},t},"7"!=="".replace(t,"$<a>")}),b=function(){var t=/(?:)/,n=t.exec;t.exec=function(){return n.apply(this,arguments)};var r="ab".split(t);return 2===r.length&&"a"===r[0]&&"b"===r[1]}();t.exports=function(r,t,n){var e=p(r),o=!h(function(){var t={};return t[e]=function(){return 7},7!=""[r](t)}),i=o?!h(function(){var t=!1,n=/a/;return n.exec=function(){return t=!0,null},"split"===r&&(n.constructor={},n.constructor[y]=function(){return n}),n[e](""),!t}):void 0;if(!o||!i||"replace"===r&&!g||"split"===r&&!b){var u=/./[e],c=n(v,e,""[r],function(t,n,r,e,i){return n.exec===d?o&&!i?{done:!0,value:u.call(n,r,e)}:{done:!0,value:t.call(r,n,e)}:{done:!1}}),f=c[0],a=c[1];s(String.prototype,r,f),l(RegExp.prototype,e,2==t?function(t,n){return a.call(t,this,n)}:function(t){return a.call(t,this)})}}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){var e=r(5),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var o=r(7)("iterator"),u=!1;try{var e=[7][o]();e.return=function(){u=!0},Array.from(e,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!u)return!1;var r=!1;try{var e=[7],i=e[o]();i.next=function(){return{done:r=!0}},e[o]=function(){return i},t(e)}catch(t){}return r}},function(t,n,r){"use strict";t.exports=r(68)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){"use strict";var i=r(81),o=RegExp.prototype.exec;t.exports=function(t,n){var r=t.exec;if("function"==typeof r){var e=r.call(t,n);if("object"!=typeof e)throw new TypeError("RegExp exec method returned something other than an Object or null");return e}if("RegExp"!==i(t))throw new TypeError("RegExp#exec called on incompatible receiver");return o.call(t,n)}},function(t,n,r){"use strict";var e=r(1),u=r(21),c=r(47),f=r(71);t.exports=function(t){e(e.S,t,{from:function(t){var n,r,e,i,o=arguments[1];return u(this),(n=void 0!==o)&&u(o),null==t?new this:(r=[],n?(e=0,i=c(o,arguments[2],2),f(t,!1,function(t){r.push(i(t,e++))})):f(t,!1,r.push,r),new this(r))}})}},function(t,n,r){"use strict";var e=r(1);t.exports=function(t){e(e.S,t,{of:function(){for(var t=arguments.length,n=new Array(t);t--;)n[t]=arguments[t];return new this(n)}})}},function(t,n,r){var f=r(49),a=r(51);t.exports=function(c){return function(t,n){var r,e,i=String(a(t)),o=f(n),u=i.length;return o<0||u<=o?c?"":void 0:(r=i.charCodeAt(o))<55296||56319<r||o+1===u||(e=i.charCodeAt(o+1))<56320||57343<e?c?i.charAt(o):r:c?i.slice(o,o+2):e-56320+(r-55296<<10)+65536}}},function(t,n,r){for(var e,i=r(3),o=r(26),u=r(79),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n,r){var e=r(3).navigator;t.exports=e&&e.userAgent||""},function(t,n){"use strict";var r,e={versions:(r=window.navigator.userAgent,{trident:-1<r.indexOf("Trident"),presto:-1<r.indexOf("Presto"),webKit:-1<r.indexOf("AppleWebKit"),gecko:-1<r.indexOf("Gecko")&&-1==r.indexOf("KHTML"),mobile:!!r.match(/AppleWebKit.*Mobile.*/),ios:!!r.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:-1<r.indexOf("Android")||-1<r.indexOf("Linux"),iPhone:-1<r.indexOf("iPhone")||-1<r.indexOf("Mac"),iPad:-1<r.indexOf("iPad"),webApp:-1==r.indexOf("Safari"),weixin:-1==r.indexOf("MicroMessenger")})};t.exports=e},function(t,n,r){"use strict";var e,i=r(87),l=(e=i)&&e.__esModule?e:{default:e},h=function(){function n(t,n,r){return n||r?String.fromCharCode(n||r):o[t]||t}function r(t){return s[t]}var e=/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,i=/['<> "&]/g,o={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},u=/\u00a0/g,c=/<br\s*\/?>/gi,f=/\r?\n/g,a=/\s/g,s={};for(var t in o)s[o[t]]=t;return o["&apos;"]="'",s["'"]="&#39;",{encode:function(t){return t?(""+t).replace(i,r).replace(f,"<br/>").replace(a,"&nbsp;"):""},decode:function(t){return t?(""+t).replace(c,"\n").replace(e,n).replace(u," "):""},encodeBase16:function(t){if(!t)return t;for(var n=[],r=0,e=(t+="").length;r<e;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;for(var n=[],r=0,e=(t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")})).length;r<e;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;for(var n=[],r=0,e=(t+="").length;r<e;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;n<r;n++)t[n]=h.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,l.default)(t)))for(var e in t)t[e]=h.encodeObject(t[e]);else if("string"==typeof t)return h.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=h},function(t,n,r){"use strict";var e=r(131)(!0);t.exports=function(t,n,r){return n+(r?e(t,n).length:1)}},function(t,n,r){"use strict";var c=r(17),f=r(78),a=r(8);t.exports=function(t){for(var n=c(this),r=a(n.length),e=arguments.length,i=f(1<e?arguments[1]:void 0,r),o=2<e?arguments[2]:void 0,u=void 0===o?r:f(o,r);i<u;)n[i++]=t;return n}},function(t,n,r){var e=r(215);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(11),i=r(75);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(5),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(n){var r=/./;try{"/./"[n](r)}catch(t){try{return r[e]=!1,!"/./"[n](r)}catch(n){}}return!0}},function(t,n,r){var e=r(3).document;t.exports=e&&e.documentElement},function(t,n,r){var o=r(5),u=r(153).set;t.exports=function(t,n,r){var e,i=n.constructor;return i!==r&&"function"==typeof i&&(e=i.prototype)!==r.prototype&&o(e)&&u&&u(t,e),t}},function(t,n,r){var e=r(82),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){"use strict";var e=r(72),i=r(75),o=r(83),u={};r(26)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var x=r(68),m=r(1),S=r(27),w=r(26),_=r(82),O=r(146),E=r(83),M=r(32),P=r(7)("iterator"),j=!([].keys&&"next"in[].keys()),F="values",A=function(){return this};t.exports=function(t,n,r,e,i,o,u){O(r,n,e);var c,f,a,s=function(t){if(!j&&t in p)return p[t];switch(t){case"keys":case F:return function(){return new r(this,t)}}return function(){return new r(this,t)}},l=n+" Iterator",h=i==F,v=!1,p=t.prototype,d=p[P]||p["@@iterator"]||i&&p[i],y=d||s(i),g=i?h?s("entries"):y:void 0,b="Array"==n&&p.entries||d;if(b&&((a=M(b.call(new t)))!==Object.prototype&&a.next&&(E(a,l,!0),x||"function"==typeof a[P]||w(a,P,A))),h&&d&&d.name!==F&&(v=!0,y=function(){return d.call(this)}),x&&!u||!j&&!v&&p[P]||w(p,P,y),_[n]=y,_[l]=A,i)if(c={values:h?y:s(F),keys:o?y:s("keys"),entries:g},u)for(f in c)f in p||S(p,f,c[f]);else m(m.P+m.F*(j||v),n,c);return c}},function(t,n){var r=Math.expm1;t.exports=!r||22025.465794806718<r(10)||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:-1e-6<t&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var c=r(3),f=r(158).set,a=c.MutationObserver||c.WebKitMutationObserver,s=c.process,l=c.Promise,h="process"==r(45)(s);t.exports=function(){var r,e,i,t=function(){var t,n;for(h&&(t=s.domain)&&t.exit();r;){n=r.fn,r=r.next;try{n()}catch(t){throw r?i():e=void 0,t}}e=void 0,t&&t.enter()};if(h)i=function(){s.nextTick(t)};else if(!a||c.navigator&&c.navigator.standalone)if(l&&l.resolve){var n=l.resolve(void 0);i=function(){n.then(t)}}else i=function(){f.call(c,t)};else{var o=!0,u=document.createTextNode("");new a(t).observe(u,{characterData:!0}),i=function(){u.data=o=!o}}return function(t){var n={fn:t,next:void 0};e&&(e.next=n),r||(r=n,i()),e=n}}},function(t,n,r){"use strict";function e(t){var r,e;this.promise=new t(function(t,n){if(void 0!==r||void 0!==e)throw TypeError("Bad Promise constructor");r=t,e=n}),this.resolve=i(r),this.reject=i(e)}var i=r(21);t.exports.f=function(t){return new e(t)}},function(t,n,r){"use strict";var e,i,u=r(115),c=RegExp.prototype.exec,f=String.prototype.replace,o=c,a="lastIndex",s=(e=/a/,i=/b*/g,c.call(e,"a"),c.call(i,"a"),0!==e[a]||0!==i[a]),l=void 0!==/()??/.exec("")[1];(s||l)&&(o=function(t){var n,r,e,i,o=this;return l&&(r=new RegExp("^"+o.source+"$(?!\\s)",u.call(o))),s&&(n=o[a]),e=c.call(o,t),s&&e&&(o[a]=o.global?e.index+e[0].length:n),l&&e&&1<e.length&&f.call(e[0],r,function(){for(i=1;i<arguments.length-2;i++)void 0===arguments[i]&&(e[i]=void 0)}),e}),t.exports=o},function(t,n,i){var r=i(5),e=i(2),o=function(t,n){if(e(t),!r(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,r,e){try{(e=i(47)(Function.call,i(31).f(Object.prototype,"__proto__").set,2))(t,[]),r=!(t instanceof Array)}catch(t){r=!0}return function(t,n){return o(t,n),r?t.__proto__=n:e(t,n),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(118)("keys"),i=r(79);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(124),i=r(51);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var i=r(49),o=r(51);t.exports=function(t){var n=String(o(this)),r="",e=i(t);if(e<0||e==1/0)throw RangeError("Count can't be negative");for(;0<e;(e>>>=1)&&(n+=n))1&e&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(47),c=r(175),f=r(143),a=r(140),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=s.Dispatch,y=0,g={},b="onreadystatechange",x=function(){var t=+this;if(g.hasOwnProperty(t)){var n=g[t];delete g[t],n()}},m=function(t){x.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return g[++y]=function(){c("function"==typeof t?t:Function(t),n)},e(y),y},v=function(t){delete g[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(x,t,1))}:d&&d.now?e=function(t){d.now(u(x,t,1))}:p?(o=(i=new p).port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=b in a("script")?function(t){f.appendChild(a("script"))[b]=function(){f.removeChild(this),x.call(t)}}:function(t){setTimeout(u(x,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";function e(t,n,r){var e,i,o,u=new Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?W(2,-24)-W(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for((t=G(t))!=t||t===C?(i=t!=t?1:0,e=f):(e=U(V(t)/B),t*(o=W(2,-e))<1&&(e--,o*=2),2<=(t+=1<=e+a?s/o:s*W(2,1-a))*o&&(e++,o/=2),f<=e+a?(i=0,e=f):1<=e+a?(i=(t*o-1)*W(2,n),e+=a):(i=t*W(2,a-1)*W(2,n),e=0));8<=n;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;0<c;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u}function i(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;0<c;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;0<c;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-C:C;e+=W(2,n),s-=u}return(a?-1:1)*e*W(2,s-n)}function o(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]}function u(t){return[255&t]}function c(t){return[255&t,t>>8&255]}function f(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]}function a(t){return e(t,52,8)}function s(t){return e(t,23,4)}function l(t,n,r){M(t[I],n,{get:function(){return this[r]}})}function h(t,n,r,e){var i=O(+r);if(i+n>t[H])throw R(L);var o=t[K]._b,u=i+t[J],c=o.slice(u,u+n);return e?c:c.reverse()}function v(t,n,r,e,i,o){var u=O(+r);if(u+n>t[H])throw R(L);for(var c=t[K]._b,f=u+t[J],a=e(+i),s=0;s<n;s++)c[f+s]=a[o?s:n-s-1]}var p=r(3),d=r(10),y=r(68),g=r(132),b=r(26),x=r(76),m=r(4),S=r(70),w=r(49),_=r(8),O=r(194),E=r(73).f,M=r(11).f,P=r(137),j=r(83),F="ArrayBuffer",A="DataView",I="prototype",L="Wrong index!",N=p[F],T=p[A],k=p.Math,R=p.RangeError,C=p.Infinity,D=N,G=k.abs,W=k.pow,U=k.floor,V=k.log,B=k.LN2,q="byteLength",z="byteOffset",K=d?"_b":"buffer",H=d?"_l":q,J=d?"_o":z;if(g.ABV){if(!m(function(){N(1)})||!m(function(){new N(-1)})||m(function(){return new N,new N(1.5),new N(NaN),N.name!=F})){for(var $,Y=(N=function(t){return S(this,N),new D(O(t))})[I]=D[I],X=E(D),Q=0;X.length>Q;)($=X[Q++])in N||b(N,$,D[$]);y||(Y.constructor=N)}var Z=new T(new N(2)),tt=T[I].setInt8;Z.setInt8(0,2147483648),Z.setInt8(1,2147483649),!Z.getInt8(0)&&Z.getInt8(1)||x(T[I],{setInt8:function(t,n){tt.call(this,t,n<<24>>24)},setUint8:function(t,n){tt.call(this,t,n<<24>>24)}},!0)}else N=function(t){S(this,N,F);var n=O(t);this._b=P.call(new Array(n),0),this[H]=n},T=function(t,n,r){S(this,T,A),S(t,N,A);var e=t[H],i=w(n);if(i<0||e<i)throw R("Wrong offset!");if(e<i+(r=void 0===r?e-i:_(r)))throw R("Wrong length!");this[K]=t,this[J]=i,this[H]=r},d&&(l(N,q,"_l"),l(T,"buffer","_b"),l(T,q,"_l"),l(T,z,"_o")),x(T[I],{getInt8:function(t){return h(this,1,t)[0]<<24>>24},getUint8:function(t){return h(this,1,t)[0]},getInt16:function(t){var n=h(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=h(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return o(h(this,4,t,arguments[1]))},getUint32:function(t){return o(h(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return i(h(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return i(h(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){v(this,1,t,u,n)},setUint8:function(t,n){v(this,1,t,u,n)},setInt16:function(t,n){v(this,2,t,c,n,arguments[2])},setUint16:function(t,n){v(this,2,t,c,n,arguments[2])},setInt32:function(t,n){v(this,4,t,f,n,arguments[2])},setUint32:function(t,n){v(this,4,t,f,n,arguments[2])},setFloat32:function(t,n){v(this,4,t,s,n,arguments[2])},setFloat64:function(t,n){v(this,8,t,a,n,arguments[2])}});j(N,F),j(T,A),b(T[I],g.VIEW,!0),n[F]=N,n[A]=T},function(t,n,r){var e=r(3),i=r(46),o=r(68),u=r(195),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(81),i=r(7)("iterator"),o=r(82);t.exports=r(46).getIteratorMethod=function(t){if(null!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(67),i=r(178),o=r(82),u=r(33);t.exports=r(147)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):i(0,"keys"==n?r:"values"==n?t[r]:[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){t.exports=function(t,n){t.classList?t.classList.add(n):t.className+=" "+n}},function(t,n){t.exports=function(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var a=r(17),s=r(78),l=r(8);t.exports=[].copyWithin||function(t,n){var r=a(this),e=l(r.length),i=s(t,e),o=s(n,e),u=2<arguments.length?arguments[2]:void 0,c=Math.min((void 0===u?e:s(u,e))-o,e-i),f=1;for(o<i&&i<o+c&&(f=-1,o+=c-1,i+=c-1);0<c--;)o in r?r[i]=r[o]:delete r[i],i+=f,o+=f;return r}},function(t,n,r){var e=r(71);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var s=r(21),l=r(17),h=r(116),v=r(8);t.exports=function(t,n,r,e,i){s(n);var o=l(t),u=h(o),c=v(o.length),f=i?c-1:0,a=i?-1:1;if(r<2)for(;;){if(f in u){e=u[f],f+=a;break}if(f+=a,i?f<0:c<=f)throw TypeError("Reduce of empty array with no initial value")}for(;i?0<=f:f<c;f+=a)f in u&&(e=n(e,u[f],f,o));return e}},function(t,n,r){"use strict";var o=r(21),u=r(5),c=r(175),f=[].slice,a={};t.exports=Function.bind||function(n){var r=o(this),e=f.call(arguments,1),i=function(){var t=e.concat(f.call(arguments));return this instanceof i?function(t,n,r){if(!(n in a)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";a[n]=Function("F,a","return new F("+e.join(",")+")")}return a[n](t,r)}(r,t.length,t):c(r,t,n)};return u(r.prototype)&&(i.prototype=r.prototype),i}},function(t,n,r){"use strict";var u=r(11).f,c=r(72),f=r(76),a=r(47),s=r(70),l=r(71),e=r(147),i=r(178),o=r(77),h=r(10),v=r(69).fastKey,p=r(80),d=h?"_s":"size",y=function(t,n){var r,e=v(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,o,r,e){var i=t(function(t,n){s(t,i,o,"_i"),t._t=o,t._i=c(null),t._f=void 0,t._l=void 0,t[d]=0,null!=n&&l(n,r,t[e],t)});return f(i.prototype,{clear:function(){for(var t=p(this,o),n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=p(this,o),r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){p(this,o);for(var n,r=a(t,1<arguments.length?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(p(this,o),t)}}),h&&u(i.prototype,"size",{get:function(){return p(this,o)[d]}}),i},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=v(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,r,n){e(t,r,function(t,n){this._t=p(t,r),this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?i(0,"keys"==n?r.k:"values"==n?r.v:[r.k,r.v]):(t._t=void 0,i(1))},n?"entries":"values",!n,!0),o(r)}}},function(t,n,r){var e=r(81),i=r(167);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var u=r(76),c=r(69).getWeak,i=r(2),f=r(5),a=r(70),s=r(71),e=r(50),l=r(30),h=r(80),o=e(5),v=e(6),p=0,d=function(t){return t._l||(t._l=new y)},y=function(){this.a=[]},g=function(t,n){return o(t.a,function(t){return t[0]===n})};y.prototype={get:function(t){var n=g(this,t);if(n)return n[1]},has:function(t){return!!g(this,t)},set:function(t,n){var r=g(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(n){var t=v(this.a,function(t){return t[0]===n});return~t&&this.a.splice(t,1),!!~t}},t.exports={getConstructor:function(t,r,e,i){var o=t(function(t,n){a(t,o,r,"_i"),t._t=r,t._i=p++,t._l=void 0,null!=n&&s(n,e,t[i],t)});return u(o.prototype,{delete:function(t){if(!f(t))return!1;var n=c(t);return!0===n?d(h(this,r)).delete(t):n&&l(n,this._i)&&delete n[this._i]},has:function(t){if(!f(t))return!1;var n=c(t);return!0===n?d(h(this,r)).has(t):n&&l(n,this._i)}}),o},def:function(t,n,r){var e=c(i(n),!0);return!0===e?d(t).set(n,r):e[t._i]=r,t},ufstore:d}},function(t,n,r){"use strict";var p=r(123),d=r(5),y=r(8),g=r(47),b=r(7)("isConcatSpreadable");t.exports=function t(n,r,e,i,o,u,c,f){for(var a,s,l=o,h=0,v=!!c&&g(c,f,3);h<i;){if(h in e){if(a=v?v(e[h],h,r):e[h],s=!1,d(a)&&(s=void 0!==(s=a[b])?!!s:p(a)),s&&0<u)l=t(n,r,a,y(a.length),l,u-1)-1;else{if(9007199254740991<=l)throw TypeError();n[l]=a}l++}h++}return l}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(140)("div"),"a",{get:function(){return 7}}).a})},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(5),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var o=r(2);t.exports=function(t,n,r,e){try{return e?n(o(r)[0],r[1]):n(r)}catch(n){var i=t.return;throw void 0!==i&&o(i.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var o=r(149),e=Math.pow,u=e(2,-52),c=e(2,-23),f=e(2,127)*(2-c),a=e(2,-126);t.exports=Math.fround||function(t){var n,r,e=Math.abs(t),i=o(t);return e<a?i*(e/a/c+1/u-1/u)*a*c:f<(r=(n=(1+c/u)*e)-(n-e))||r!=r?i*(1/0):i*r}},function(t,n){t.exports=Math.log1p||function(t){return-1e-8<(t=+t)&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n){t.exports=Math.scale||function(t,n,r,e,i){return 0===arguments.length||t!=t||n!=n||r!=r||e!=e||i!=i?NaN:t===1/0||t===-1/0?t:(t-n)*(i-e)/(r-n)+e}},function(t,n,r){"use strict";var h=r(10),v=r(74),p=r(127),d=r(117),y=r(17),g=r(116),i=Object.assign;t.exports=!i||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=i({},t)[r]||Object.keys(i({},n)).join("")!=e})?function(t,n){for(var r=y(t),e=arguments.length,i=1,o=p.f,u=d.f;i<e;)for(var c,f=g(arguments[i++]),a=o?v(f).concat(o(f)):v(f),s=a.length,l=0;l<s;)c=a[l++],h&&!u.call(f,c)||(r[c]=f[c]);return r}:i},function(t,n,r){var u=r(11),c=r(2),f=r(74);t.exports=r(10)?Object.defineProperties:function(t,n){c(t);for(var r,e=f(n),i=e.length,o=0;o<i;)u.f(t,r=e[o++],n[r]);return t}},function(t,n,r){var e=r(33),i=r(73).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?function(t){try{return i(t)}catch(t){return u.slice()}}(t):i(e(t))}},function(t,n,r){var u=r(30),c=r(33),f=r(120)(!1),a=r(154)("IE_PROTO");t.exports=function(t,n){var r,e=c(t),i=0,o=[];for(r in e)r!=a&&u(e,r)&&o.push(r);for(;n.length>i;)u(e,r=n[i++])&&(~f(o,r)||o.push(r));return o}},function(t,n,r){var f=r(10),a=r(74),s=r(33),l=r(117).f;t.exports=function(c){return function(t){for(var n,r=s(t),e=a(r),i=e.length,o=0,u=[];o<i;)n=e[o++],f&&!l.call(r,n)||u.push(c?[n,r[n]]:r[n]);return u}}},function(t,n,r){var e=r(73),i=r(127),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(84).trim;t.exports=1/e(r(157)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(84).trim,o=r(157),u=/^[-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=function(t){try{return{e:!1,v:t()}}catch(t){return{e:!0,v:t}}}},function(t,n,r){var e=r(2),i=r(5),o=r(151);t.exports=function(t,n){if(e(t),i(n)&&n.constructor===t)return n;var r=o.f(t);return(0,r.resolve)(n),r.promise}},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var s=r(8),l=r(156),h=r(51);t.exports=function(t,n,r,e){var i=String(h(t)),o=i.length,u=void 0===r?" ":String(r),c=s(n);if(c<=o||""==u)return i;var f=c-o,a=l.call(u,Math.ceil(f/u.length));return a.length>f&&(a=a.slice(0,f)),e?a+i:i+a}},function(t,n,r){var e=r(49),i=r(8);t.exports=function(t){if(void 0===t)return 0;var n=e(t),r=i(n);if(n!==r)throw RangeError("Wrong length!");return r}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(170),i=r(80);t.exports=r(121)("Map",function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(i(this,"Map"),t);return n&&n.v},set:function(t,n){return e.def(i(this,"Map"),0===t?0:t,n)}},e,!0)},function(t,n,r){"use strict";var e=r(152);r(1)({target:"RegExp",proto:!0,forced:e!==/./.exec},{exec:e})},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(115)})},function(t,n,r){"use strict";var e=r(170),i=r(80);t.exports=r(121)("Set",function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},{add:function(t){return e.def(i(this,"Set"),t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var o,e=r(3),i=r(50)(0),u=r(27),c=r(69),f=r(182),a=r(172),s=r(5),l=r(80),h=r(80),v=!e.ActiveXObject&&"ActiveXObject"in e,p="WeakMap",d=c.getWeak,y=Object.isExtensible,g=a.ufstore,b=function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},x={get:function(t){if(s(t)){var n=d(t);return!0===n?g(l(this,p)).get(t):n?n[this._i]:void 0}},set:function(t,n){return a.def(l(this,p),t,n)}},m=t.exports=r(121)(p,b,x,a,!0,!0);h&&v&&(f((o=a.getConstructor(b,p)).prototype,x),c.NEED=!0,i(["delete","has","get","set"],function(e){var t=m.prototype,i=t[e];u(t,e,function(t,n){if(!s(t)||y(t))return i.call(this,t,n);this._f||(this._f=new o);var r=this._f[e](t,n);return"set"==e?this:r})}))},,,,function(t,n){"use strict";t.exports={init:function(){var t=document.querySelector("#page-nav");t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new&&document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")}),yiliaConfig&&yiliaConfig.toc_hide_index&&document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n,r,e,i){var o=function(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}(t),u=function(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}(t)-n;if(u-r<=i){var c=t.$newDom;c||(c=t.cloneNode(!0),(0,a.default)(t,c),(t.$newDom=c).style.position="fixed",c.style.top=(r||u)+"px",c.style.left=o+"px",c.style.zIndex=e||2,c.style.width="100%",c.style.color="#fff"),c.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var f=t.$newDom;f&&(f.style.visibility="hidden")}}function o(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");i(t,document.body.scrollTop,-63,2,0),i(n,document.body.scrollTop,1,3,0)}var f=e(r(163)),a=e((e(r(164)),r(414))),u=e(r(134)),c=e(r(204)),s=r(135);u.default.versions.mobile&&window.screen.width<800&&(function(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var i=t[r];o=n,u=i.getAttribute("href"),c=/\/|index.html/g,o.replace(c,"")===u.replace(c,"")&&(0,f.default)(i,"active")}var o,u,c}(),document.querySelector("#container").addEventListener("scroll",function(t){o()}),window.addEventListener("scroll",function(t){o()}),o()),(0,s.addLoadEvent)(function(){c.default.init()}),t.exports={}},,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object.defineProperty(t,n,{writable:!0,configurable:!0,value:r})}if(r(413),r(209),r(211),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},function(L,t){(function(t){!function(t){"use strict";function o(t,n,r,e){var o,u,c,f,i=n&&n.prototype instanceof h?n:h,a=Object.create(i.prototype),s=new p(e||[]);return a._invoke=(o=t,u=r,c=s,f=_,function(t,n){if(f===E)throw new Error("Generator is already running");if(f===M){if("throw"===t)throw n;return d()}for(c.method=t,c.arg=n;;){var r=c.delegate;if(r){var e=v(r,c);if(e){if(e===P)continue;return e}}if("next"===c.method)c.sent=c._sent=c.arg;else if("throw"===c.method){if(f===_)throw f=M,c.arg;c.dispatchException(c.arg)}else"return"===c.method&&c.abrupt("return",c.arg);f=E;var i=l(o,u,c);if("normal"===i.type){if(f=c.done?M:O,i.arg===P)continue;return{value:i.arg,done:c.done}}"throw"===i.type&&(f=M,c.method="throw",c.arg=i.arg)}}),a}function l(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function h(){}function r(){}function n(){}function e(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function u(c){function f(t,n,r,e){var i=l(c[t],c,n);if("throw"!==i.type){var o=i.arg,u=o.value;return u&&"object"==typeof u&&y.call(u,"__await")?Promise.resolve(u.__await).then(function(t){f("next",t,r,e)},function(t){f("throw",t,r,e)}):Promise.resolve(u).then(function(t){o.value=t,r(o)},e)}e(i.arg)}var n;"object"==typeof t.process&&t.process.domain&&(f=t.process.domain.bind(f)),this._invoke=function(r,e){function t(){return new Promise(function(t,n){f(r,e,t,n)})}return n=n?n.then(t,t):t()}}function v(t,n){var r=t.iterator[n.method];if(r===a){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=a,v(t,n),"throw"===n.method))return P;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return P}var e=l(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,P;var i=e.arg;return i?i.done?(n[t.resultName]=i.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=a),n.delegate=null,P):i:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,P)}function i(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function c(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(i,this),this.reset(!0)}function f(n){if(n){var t=n[b];if(t)return t.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var r=-1,e=function t(){for(;++r<n.length;)if(y.call(n,r))return t.value=n[r],t.done=!1,t;return t.value=a,t.done=!0,t};return e.next=e}}return{next:d}}function d(){return{value:a,done:!0}}var a,s=Object.prototype,y=s.hasOwnProperty,g="function"==typeof Symbol?Symbol:{},b=g.iterator||"@@iterator",x=g.asyncIterator||"@@asyncIterator",m=g.toStringTag||"@@toStringTag",S="object"==typeof L,w=t.regeneratorRuntime;if(w)S&&(L.exports=w);else{(w=t.regeneratorRuntime=S?L.exports:{}).wrap=o;var _="suspendedStart",O="suspendedYield",E="executing",M="completed",P={},j={};j[b]=function(){return this};var F=Object.getPrototypeOf,A=F&&F(F(f([])));A&&A!==s&&y.call(A,b)&&(j=A);var I=n.prototype=h.prototype=Object.create(j);r.prototype=I.constructor=n,n.constructor=r,n[m]=r.displayName="GeneratorFunction",w.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===r||"GeneratorFunction"===(n.displayName||n.name))},w.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,n):(t.__proto__=n,m in t||(t[m]="GeneratorFunction")),t.prototype=Object.create(I),t},w.awrap=function(t){return{__await:t}},e(u.prototype),u.prototype[x]=function(){return this},w.AsyncIterator=u,w.async=function(t,n,r,e){var i=new u(o(t,n,r,e));return w.isGeneratorFunction(n)?i:i.next().then(function(t){return t.done?t.value:i.next()})},e(I),I[m]="Generator",I[b]=function(){return this},I.toString=function(){return"[object Generator]"},w.keys=function(r){var e=[];for(var t in r)e.push(t);return e.reverse(),function t(){for(;e.length;){var n=e.pop();if(n in r)return t.value=n,t.done=!1,t}return t.done=!0,t}},w.values=f,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=a,this.done=!1,this.delegate=null,this.method="next",this.arg=a,this.tryEntries.forEach(c),!t)for(var n in this)"t"===n.charAt(0)&&y.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=a)},stop:function(){this.done=!0;var t=this.tryEntries[0].completion;if("throw"===t.type)throw t.arg;return this.rval},dispatchException:function(r){function t(t,n){return o.type="throw",o.arg=r,e.next=t,n&&(e.method="next",e.arg=a),!!n}if(this.done)throw r;for(var e=this,n=this.tryEntries.length-1;0<=n;--n){var i=this.tryEntries[n],o=i.completion;if("root"===i.tryLoc)return t("end");if(i.tryLoc<=this.prev){var u=y.call(i,"catchLoc"),c=y.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return t(i.catchLoc,!0);if(this.prev<i.finallyLoc)return t(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return t(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return t(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;0<=r;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&y.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,P):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),P},finish:function(t){for(var n=this.tryEntries.length-1;0<=n;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),c(r),P}},catch:function(t){for(var n=this.tryEntries.length-1;0<=n;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;c(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:f(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=a),P}}}}("object"==typeof t?t:"object"==typeof window?window:"object"==typeof self?self:this)}).call(t,function(){return this}())},,function(t,n,r){r(221),t.exports=r(46).RegExp.escape},,,,function(t,n,r){var e=r(5),i=r(123),o=r(7)("species");t.exports=function(t){var n;return i(t)&&("function"!=typeof(n=t.constructor)||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&(null===(n=n[o])&&(n=void 0))),void 0===n?Array:n}},function(t,n,r){"use strict";var e=r(4),i=Date.prototype.getTime,o=Date.prototype.toISOString,u=function(t){return 9<t?t:"0"+t};t.exports=e(function(){return"0385-07-25T07:06:39.999Z"!=o.call(new Date(-5e13-1))})||!e(function(){o.call(new Date(NaN))})?function(){if(!isFinite(i.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":9999<n?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(99<r?r:"0"+u(r))+"Z"}:o},function(t,n,r){"use strict";var e=r(2),i=r(53);t.exports=function(t){if("string"!==t&&"number"!==t&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),"number"!=t)}},function(t,n,r){var c=r(74),f=r(127),a=r(117);t.exports=function(t){var n=c(t),r=f.f;if(r)for(var e,i=r(t),o=a.f,u=0;i.length>u;)o.call(t,e=i[u++])&&n.push(e);return n}},function(t,n,r){t.exports=r(118)("native-function-to-string",Function.toString)},function(t,n){t.exports=function(n,r){var e=r===Object(r)?function(t){return r[t]}:r;return function(t){return String(t).replace(n,e)}}},function(t,n,r){var e=r(1),i=r(220)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(166)}),r(67)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(50)(4);e(e.P+e.F*!r(48)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(137)}),r(67)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(50)(2);e(e.P+e.F*!r(48)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(50)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0)}}),r(67)(o)},function(t,n,r){"use strict";var e=r(1),i=r(50)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0)}}),r(67)(o)},function(t,n,r){"use strict";var e=r(1),i=r(50)(0),o=r(48)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var h=r(47),e=r(1),v=r(17),p=r(177),d=r(145),y=r(8),g=r(139),b=r(161);e(e.S+e.F*!r(125)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,e,i,o=v(t),u="function"==typeof this?this:Array,c=arguments.length,f=1<c?arguments[1]:void 0,a=void 0!==f,s=0,l=b(o);if(a&&(f=h(f,2<c?arguments[2]:void 0,2)),null==l||u==Array&&d(l))for(r=new u(n=y(o.length));s<n;s++)g(r,s,a?f(o[s],s):o[s]);else for(i=l.call(o),r=new u;!(e=i.next()).done;s++)g(r,s,a?p(i,f,[e.value,s],!0):e.value);return r.length=s,r}})},function(t,n,r){"use strict";var e=r(1),i=r(120)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(48)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(123)})},function(t,n,r){"use strict";var e=r(1),i=r(33),o=[].join;e(e.P+e.F*(r(116)!=Object||!r(48)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(33),o=r(49),u=r(8),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(48)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(1<arguments.length&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);0<=e;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(50)(1);e(e.P+e.F*!r(48)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(139);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);t<n;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(168);e(e.P+e.F*!r(48)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(168);e(e.P+e.F*!r(48)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(143),a=r(45),s=r(78),l=r(8),h=[].slice;e(e.P+e.F*r(4)(function(){i&&h.call(i)}),"Array",{slice:function(t,n){var r=l(this.length),e=a(this);if(n=void 0===n?r:n,"Array"==e)return h.call(this,t,n);for(var i=s(t,r),o=s(n,r),u=l(o-i),c=new Array(u),f=0;f<u;f++)c[f]="String"==e?this.charAt(i+f):this[i+f];return c}})},function(t,n,r){"use strict";var e=r(1),i=r(50)(3);e(e.P+e.F*!r(48)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(21),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(48)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(77)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){var e=r(1),i=r(216);e(e.P+e.F*(Date.prototype.toISOString!==i),"Date",{toISOString:i})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(53);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(26)(i,e,r(217))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(27)(e,o,function(){var t=c.call(this);return t==t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(169)})},function(t,n,r){"use strict";var e=r(5),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=Function.prototype,o=/^\s*function ([^ (]*)/;"name"in i||r(10)&&e(i,"name",{configurable:!0,get:function(){try{return(""+this).match(o)[1]}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(180),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:94906265.62425156<t?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){var e=r(1),i=Math.asinh;e(e.S+e.F*!(i&&0<1/i(0)),"Math",{asinh:function t(n){return isFinite(n=+n)&&0!=n?n<0?-t(-n):Math.log(n+Math.sqrt(n*n+1)):n}})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(149);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(148);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1);e(e.S,"Math",{fround:r(179)})},function(t,n,r){var e=r(1),f=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,i=0,o=0,u=arguments.length,c=0;o<u;)c<(r=f(arguments[o++]))?(i=i*(e=c/r)*e+1,c=r):0<r?i+=(e=r/c)*e:i+=r;return c===1/0?1/0:c*Math.sqrt(i)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)*Math.LOG10E}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(180)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(149)})},function(t,n,r){var e=r(1),i=r(148),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(148),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(0<t?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(30),o=r(45),u=r(144),s=r(53),c=r(4),f=r(73).f,a=r(31).f,l=r(11).f,h=r(84).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(72)(y))==v,b="trim"in String.prototype,x=function(t){var n=s(t,!1);if("string"==typeof n&&2<n.length){var r,e,i,o=(n=b?n.trim():h(n,3)).charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,c=n.slice(2),f=0,a=c.length;f<a;f++)if((u=c.charCodeAt(f))<48||i<u)return NaN;return parseInt(c,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?c(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(x(n)),r,p):x(n)};for(var m,S=r(10)?f(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),w=0;S.length>w;w++)i(d,m=S[w])&&!i(p,m)&&l(p,m,a(d,m));(p.prototype=y).constructor=p,r(27)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(176)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(176),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(188);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(189);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),a=r(49),s=r(165),l=r(156),i=1..toFixed,o=Math.floor,u=[0,0,0,0,0,0],h="Number.toFixed: incorrect invocation!",v=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*u[r],u[r]=e%1e7,e=o(e/1e7)},p=function(t){for(var n=6,r=0;0<=--n;)r+=u[n],u[n]=o(r/t),r=r%t*1e7},d=function(){for(var t=6,n="";0<=--t;)if(""!==n||0===t||0!==u[t]){var r=String(u[t]);n=""===n?r:n+l.call("0",7-r.length)+r}return n},y=function(t,n,r){return 0===n?r:n%2==1?y(t,n-1,r*t):y(t*t,n/2,r)};e(e.P+e.F*(!!i&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){i.call({})})),"Number",{toFixed:function(t){var n,r,e,i,o=s(this,h),u=a(t),c="",f="0";if(u<0||20<u)throw RangeError(h);if(o!=o)return"NaN";if(o<=-1e21||1e21<=o)return String(o);if(o<0&&(c="-",o=-o),1e-21<o)if(r=(n=function(t){for(var n=0,r=t;4096<=r;)n+=12,r/=4096;for(;2<=r;)n+=1,r/=2;return n}(o*y(2,69,1))-69)<0?o*y(2,-n,1):o/y(2,n,1),r*=4503599627370496,0<(n=52-n)){for(v(0,r),e=u;7<=e;)v(1e7,0),e-=7;for(v(y(10,e,1),0),e=n-1;23<=e;)p(1<<23),e-=23;p(1<<e),v(1,1),p(2),f=d()}else v(0,r),v(1<<-n,0),f=d()+l.call("0",u);return f=0<u?c+((i=f.length)<=u?"0."+l.call("0",u-i)+f:f.slice(0,i-u)+"."+f.slice(i-u)):c+f}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(165),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(182)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(72)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(183)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(5),i=r(69).onFreeze;r(52)("freeze",function(n){return function(t){return n&&e(t)?n(i(t)):t}})},function(t,n,r){var e=r(33),i=r(31).f;r(52)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(52)("getOwnPropertyNames",function(){return r(184).f})},function(t,n,r){var e=r(17),i=r(32);r(52)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(5);r(52)("isExtensible",function(n){return function(t){return!!e(t)&&(!n||n(t))}})},function(t,n,r){var e=r(5);r(52)("isFrozen",function(n){return function(t){return!e(t)||!!n&&n(t)}})},function(t,n,r){var e=r(5);r(52)("isSealed",function(n){return function(t){return!e(t)||!!n&&n(t)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(192)})},function(t,n,r){var e=r(17),i=r(74);r(52)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(5),i=r(69).onFreeze;r(52)("preventExtensions",function(n){return function(t){return n&&e(t)?n(i(t)):t}})},function(t,n,r){var e=r(5),i=r(69).onFreeze;r(52)("seal",function(n){return function(t){return n&&e(t)?n(i(t)):t}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(153).set})},function(t,n,r){"use strict";var e=r(81),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(27)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(188);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(189);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u,c=r(68),f=r(3),a=r(47),s=r(81),l=r(1),h=r(5),v=r(21),p=r(70),d=r(71),y=r(119),g=r(158).set,b=r(150)(),x=r(151),m=r(190),S=r(133),w=r(191),_="Promise",O=f.TypeError,E=f.process,M=E&&E.versions,P=M&&M.v8||"",j=f[_],F="process"==s(E),A=function(){},I=i=x.f,L=!!function(){try{var t=j.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(A,A)};return(F||"function"==typeof PromiseRejectionEvent)&&t.then(A)instanceof n&&0!==P.indexOf("6.6")&&-1===S.indexOf("Chrome/66")}catch(t){}}(),N=function(t){var n;return!(!h(t)||"function"!=typeof(n=t.then))&&n},T=function(s,r){if(!s._n){s._n=!0;var e=s._c;b(function(){for(var f=s._v,a=1==s._s,t=0,n=function(t){var n,r,e,i=a?t.ok:t.fail,o=t.resolve,u=t.reject,c=t.domain;try{i?(a||(2==s._h&&C(s),s._h=1),!0===i?n=f:(c&&c.enter(),n=i(f),c&&(c.exit(),e=!0)),n===t.promise?u(O("Promise-chain cycle")):(r=N(n))?r.call(n,o,u):o(n)):u(f)}catch(t){c&&!e&&c.exit(),u(t)}};e.length>t;)n(e[t++]);s._c=[],s._n=!1,r&&!s._h&&k(s)})}},k=function(o){g.call(f,function(){var t,n,r,e=o._v,i=R(o);if(i&&(t=m(function(){F?E.emit("unhandledRejection",e,o):(n=f.onunhandledrejection)?n({promise:o,reason:e}):(r=f.console)&&r.error&&r.error("Unhandled promise rejection",e)}),o._h=F||R(o)?2:1),o._a=void 0,i&&t.e)throw t.v})},R=function(t){return 1!==t._h&&0===(t._a||t._c).length},C=function(n){g.call(f,function(){var t;F?E.emit("rejectionHandled",n):(t=f.onrejectionhandled)&&t({promise:n,reason:n._v})})},D=function(t){var n=this;n._d||(n._d=!0,(n=n._w||n)._v=t,n._s=2,n._a||(n._a=n._c.slice()),T(n,!0))},G=function(t){var r,e=this;if(!e._d){e._d=!0,e=e._w||e;try{if(e===t)throw O("Promise can't be resolved itself");(r=N(t))?b(function(){var n={_w:e,_d:!1};try{r.call(t,a(G,n,1),a(D,n,1))}catch(t){D.call(n,t)}}):(e._v=t,e._s=1,T(e,!1))}catch(t){D.call({_w:e,_d:!1},t)}}};L||(j=function(t){p(this,j,_,"_h"),v(t),e.call(this);try{t(a(G,this,1),a(D,this,1))}catch(t){D.call(this,t)}},(e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1}).prototype=r(76)(j.prototype,{then:function(t,n){var r=I(y(this,j));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=F?E.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&T(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),o=function(){var t=new e;this.promise=t,this.resolve=a(G,t,1),this.reject=a(D,t,1)},x.f=I=function(t){return t===j||t===u?new o(t):i(t)}),l(l.G+l.W+l.F*!L,{Promise:j}),r(83)(j,_),r(77)(_),u=r(46)[_],l(l.S+l.F*!L,_,{reject:function(t){var n=I(this);return(0,n.reject)(t),n.promise}}),l(l.S+l.F*(c||!L),_,{resolve:function(t){return w(c&&this===u?j:this,t)}}),l(l.S+l.F*!(L&&r(125)(function(t){j.all(t).catch(A)})),_,{all:function(t){var u=this,n=I(u),c=n.resolve,f=n.reject,r=m(function(){var e=[],i=0,o=1;d(t,!1,function(t){var n=i++,r=!1;e.push(void 0),o++,u.resolve(t).then(function(t){r||(r=!0,e[n]=t,--o||c(e))},f)}),--o||c(e)});return r.e&&f(r.v),n.promise},race:function(t){var n=this,r=I(n),e=r.reject,i=m(function(){d(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i.e&&e(i.v),r.promise}})},function(t,n,r){var e=r(1),o=r(21),u=r(2),c=(r(3).Reflect||{}).apply,f=Function.apply;e(e.S+e.F*!r(4)(function(){c(function(){})}),"Reflect",{apply:function(t,n,r){var e=o(t),i=u(r);return c?c(e,n,i):f.call(e,n,i)}})},function(t,n,r){var e=r(1),c=r(72),f=r(21),a=r(2),s=r(5),i=r(4),l=r(169),h=(r(3).Reflect||{}).construct,v=i(function(){function t(){}return!(h(function(){},[],t)instanceof t)}),p=!i(function(){h(function(){})});e(e.S+e.F*(v||p),"Reflect",{construct:function(t,n){f(t),a(n);var r=arguments.length<3?t:f(arguments[2]);if(p&&!v)return h(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(l.apply(t,e))}var i=r.prototype,o=c(s(i)?i:Object.prototype),u=Function.apply.call(t,o,n);return s(u)?u:o}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(53);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(146)(o,"Object",function(){var t,n=this._k;do{if(this._i>=n.length)return{value:void 0,done:!0}}while(!((t=n[this._i++])in this._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){var u=r(31),c=r(32),f=r(30),e=r(1),a=r(5),s=r(2);e(e.S,"Reflect",{get:function t(n,r){var e,i,o=arguments.length<3?n:arguments[2];return s(n)===o?n[r]:(e=u.f(n,r))?f(e,"value")?e.value:void 0!==e.get?e.get.call(o):void 0:a(i=c(n))?t(i,r,o):void 0}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(187)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(153);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){var f=r(11),a=r(31),s=r(32),l=r(30),e=r(1),h=r(75),v=r(2),p=r(5);e(e.S,"Reflect",{set:function t(n,r,e){var i,o,u=arguments.length<4?n:arguments[3],c=a.f(v(n),r);if(!c){if(p(o=s(n)))return t(o,r,e,u);c=h(0)}if(l(c,"value")){if(!1===c.writable||!p(u))return!1;if(i=a.f(u,r)){if(i.get||i.set||!1===i.writable)return!1;i.value=e,f.f(u,r,i)}else f.f(u,r,h(0,e));return!0}return void 0!==c.set&&(c.set.call(u,e),!0)}})},function(t,n,r){var e=r(3),o=r(144),i=r(11).f,u=r(73).f,c=r(124),f=r(115),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),i=void 0===n;return!r&&e&&t.constructor===a&&i?t:o(p?new s(e&&!i?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&i?f.call(t):n),r?this:l,a)};for(var d=function(n){n in a||i(a,n,{configurable:!0,get:function(){return s[n]},set:function(t){s[n]=t}})},y=u(s),g=0;y.length>g;)d(y[g++]);(l.constructor=a).prototype=l,r(27)(e,"RegExp",a)}r(77)("RegExp")},function(t,n,r){"use strict";var l=r(2),h=r(8),v=r(136),p=r(128);r(122)("match",1,function(e,i,a,s){return[function(t){var n=e(this),r=null==t?void 0:t[i];return void 0!==r?r.call(t,n):new RegExp(t)[i](String(n))},function(t){var n=s(a,t,this);if(n.done)return n.value;var r=l(t),e=String(this);if(!r.global)return p(r,e);for(var i,o=r.unicode,u=[],c=r.lastIndex=0;null!==(i=p(r,e));){var f=String(i[0]);""===(u[c]=f)&&(r.lastIndex=v(e,h(r.lastIndex),o)),c++}return 0===c?null:u}]})},function(t,n,r){"use strict";var O=r(2),e=r(17),E=r(8),M=r(49),P=r(136),j=r(128),F=Math.max,A=Math.min,h=Math.floor,v=/\$([$&`']|\d\d?|<[^>]*>)/g,p=/\$([$&`']|\d\d?)/g;r(122)("replace",2,function(i,o,S,w){function _(o,u,c,f,a,t){var s=c+o.length,l=f.length,n=p;return void 0!==a&&(a=e(a),n=v),S.call(t,n,function(t,n){var r;switch(n.charAt(0)){case"$":return"$";case"&":return o;case"`":return u.slice(0,c);case"'":return u.slice(s);case"<":r=a[n.slice(1,-1)];break;default:var e=+n;if(0===e)return t;if(l<e){var i=h(e/10);return 0===i?t:i<=l?void 0===f[i-1]?n.charAt(1):f[i-1]+n.charAt(1):t}r=f[e-1]}return void 0===r?"":r})}return[function(t,n){var r=i(this),e=null==t?void 0:t[o];return void 0!==e?e.call(t,r,n):S.call(String(r),t,n)},function(t,n){var r=w(S,t,this,n);if(r.done)return r.value;var e=O(t),i=String(this),o="function"==typeof n;o||(n=String(n));var u,c=e.global;if(c){var f=e.unicode;e.lastIndex=0}for(var a=[];;){var s=j(e,i);if(null===s)break;if(a.push(s),!c)break;""===String(s[0])&&(e.lastIndex=P(i,E(e.lastIndex),f))}for(var l="",h=0,v=0;v<a.length;v++){s=a[v];for(var p=String(s[0]),d=F(A(M(s.index),i.length),0),y=[],g=1;g<s.length;g++)y.push(void 0===(u=s[g])?u:String(u));var b=s.groups;if(o){var x=[p].concat(y,d,i);void 0!==b&&x.push(b);var m=String(n.apply(void 0,x))}else m=_(p,i,d,y,b,n);h<=d&&(l+=i.slice(h,d)+m,h=d+p.length)}return l+i.slice(h)}]})},function(t,n,r){"use strict";var f=r(2),a=r(192),s=r(128);r(122)("search",1,function(e,i,u,c){return[function(t){var n=e(this),r=null==t?void 0:t[i];return void 0!==r?r.call(t,n):new RegExp(t)[i](String(n))},function(t){var n=c(u,t,this);if(n.done)return n.value;var r=f(t),e=String(this),i=r.lastIndex;a(i,0)||(r.lastIndex=0);var o=s(r,e);return a(r.lastIndex,i)||(r.lastIndex=i),null===o?-1:o.index}]})},function(t,n,r){"use strict";var l=r(124),x=r(2),m=r(119),S=r(136),w=r(8),_=r(128),h=r(152),e=r(4),O=Math.min,v=[].push,u="split",p="length",d="lastIndex",E=4294967295,M=!e(function(){RegExp(E,"y")});r(122)("split",2,function(i,o,y,g){var b;return b="c"=="abbc"[u](/(b)*/)[1]||4!="test"[u](/(?:)/,-1)[p]||2!="ab"[u](/(?:ab)*/)[p]||4!="."[u](/(.?)(.?)/)[p]||1<"."[u](/()()/)[p]||""[u](/.?/)[p]?function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!l(t))return y.call(r,t,n);for(var e,i,o,u=[],c=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),f=0,a=void 0===n?E:n>>>0,s=new RegExp(t.source,c+"g");(e=h.call(s,r))&&!(f<(i=s[d])&&(u.push(r.slice(f,e.index)),1<e[p]&&e.index<r[p]&&v.apply(u,e.slice(1)),o=e[0][p],f=i,u[p]>=a));)s[d]===e.index&&s[d]++;return f===r[p]?!o&&s.test("")||u.push(""):u.push(r.slice(f)),u[p]>a?u.slice(0,a):u}:"0"[u](void 0,0)[p]?function(t,n){return void 0===t&&0===n?[]:y.call(this,t,n)}:y,[function(t,n){var r=i(this),e=null==t?void 0:t[o];return void 0!==e?e.call(t,r,n):b.call(String(r),t,n)},function(t,n){var r=g(b,t,this,n,b!==y);if(r.done)return r.value;var e=x(t),i=String(this),o=m(e,RegExp),u=e.unicode,c=(e.ignoreCase?"i":"")+(e.multiline?"m":"")+(e.unicode?"u":"")+(M?"y":"g"),f=new o(M?e:"^(?:"+e.source+")",c),a=void 0===n?E:n>>>0;if(0===a)return[];if(0===i.length)return null===_(f,i)?[i]:[];for(var s=0,l=0,h=[];l<i.length;){f.lastIndex=M?l:0;var v,p=_(f,M?i:i.slice(l));if(null===p||(v=O(w(f.lastIndex+(M?0:l)),i.length))===s)l=S(i,l,u);else{if(h.push(i.slice(s,l)),h.length===a)return h;for(var d=1;d<=p.length-1;d++)if(h.push(p[d]),h.length===a)return h;l=s=v}}return h.push(i.slice(s)),h}]})},function(t,n,r){"use strict";r(198);var e=r(2),i=r(115),o=r(10),u="toString",c=/./[u],f=function(t){r(27)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(28)("anchor",function(n){return function(t){return n(this,"a","name",t)}})},function(t,n,r){"use strict";r(28)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(28)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(28)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(131)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),u=r(8),c=r(155),f="endsWith",a=""[f];e(e.P+e.F*r(142)(f),"String",{endsWith:function(t){var n=c(this,t,f),r=1<arguments.length?arguments[1]:void 0,e=u(n.length),i=void 0===r?e:Math.min(u(r),e),o=String(t);return a?a.call(n,o,i):n.slice(i-o.length,i)===o}})},function(t,n,r){"use strict";r(28)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(28)("fontcolor",function(n){return function(t){return n(this,"font","color",t)}})},function(t,n,r){"use strict";r(28)("fontsize",function(n){return function(t){return n(this,"font","size",t)}})},function(t,n,r){var e=r(1),o=r(78),u=String.fromCharCode,i=String.fromCodePoint;e(e.S+e.F*(!!i&&1!=i.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,i=0;i<e;){if(n=+arguments[i++],o(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?u(n):u(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(155);e(e.P+e.F*r(142)("includes"),"String",{includes:function(t){return!!~i(this,t,"includes").indexOf(t,1<arguments.length?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(28)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(131)(!0);r(147)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(28)("link",function(n){return function(t){return n(this,"a","href",t)}})},function(t,n,r){var e=r(1),u=r(33),c=r(8);e(e.S,"String",{raw:function(t){for(var n=u(t.raw),r=c(n.length),e=arguments.length,i=[],o=0;o<r;)i.push(String(n[o++])),o<e&&i.push(String(arguments[o]));return i.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(156)})},function(t,n,r){"use strict";r(28)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(8),o=r(155),u="startsWith",c=""[u];e(e.P+e.F*r(142)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(1<arguments.length?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(28)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(28)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(28)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(84)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),u=r(30),i=r(10),o=r(1),c=r(27),f=r(69).KEY,a=r(4),s=r(118),l=r(83),h=r(79),v=r(7),p=r(195),d=r(160),y=r(218),g=r(123),b=r(2),x=r(5),m=r(17),S=r(33),w=r(53),_=r(75),O=r(72),E=r(184),M=r(31),P=r(127),j=r(11),F=r(74),A=M.f,I=j.f,L=E.f,N=e.Symbol,T=e.JSON,k=T&&T.stringify,R="prototype",C=v("_hidden"),D=v("toPrimitive"),G={}.propertyIsEnumerable,W=s("symbol-registry"),U=s("symbols"),V=s("op-symbols"),B=Object[R],q="function"==typeof N&&!!P.f,z=e.QObject,K=!z||!z[R]||!z[R].findChild,H=i&&a(function(){return 7!=O(I({},"a",{get:function(){return I(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=A(B,n);e&&delete B[n],I(t,n,r),e&&t!==B&&I(B,n,e)}:I,J=function(t){var n=U[t]=O(N[R]);return n._k=t,n},$=q&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===B&&Y(V,n,r),b(t),n=w(n,!0),b(r),u(U,n)?(r.enumerable?(u(t,C)&&t[C][n]&&(t[C][n]=!1),r=O(r,{enumerable:_(0,!1)})):(u(t,C)||I(t,C,_(1,{})),t[C][n]=!0),H(t,n,r)):I(t,n,r)},X=function(t,n){b(t);for(var r,e=y(n=S(n)),i=0,o=e.length;i<o;)Y(t,r=e[i++],n[r]);return t},Q=function(t){var n=G.call(this,t=w(t,!0));return!(this===B&&u(U,t)&&!u(V,t))&&(!(n||!u(this,t)||!u(U,t)||u(this,C)&&this[C][t])||n)},Z=function(t,n){if(t=S(t),n=w(n,!0),t!==B||!u(U,n)||u(V,n)){var r=A(t,n);return!r||!u(U,n)||u(t,C)&&t[C][n]||(r.enumerable=!0),r}},tt=function(t){for(var n,r=L(S(t)),e=[],i=0;r.length>i;)u(U,n=r[i++])||n==C||n==f||e.push(n);return e},nt=function(t){for(var n,r=t===B,e=L(r?V:S(t)),i=[],o=0;e.length>o;)!u(U,n=e[o++])||r&&!u(B,n)||i.push(U[n]);return i};q||(c((N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var n=h(0<arguments.length?arguments[0]:void 0),r=function(t){this===B&&r.call(V,t),u(this,C)&&u(this[C],n)&&(this[C][n]=!1),H(this,n,_(1,t))};return i&&K&&H(B,n,{configurable:!0,set:r}),J(n)})[R],"toString",function(){return this._k}),M.f=Z,j.f=Y,r(73).f=E.f=tt,r(117).f=Q,P.f=nt,i&&!r(68)&&c(B,"propertyIsEnumerable",Q,!0),p.f=function(t){return J(v(t))}),o(o.G+o.W+o.F*!q,{Symbol:N});for(var rt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),et=0;rt.length>et;)v(rt[et++]);for(var it=F(v.store),ot=0;it.length>ot;)d(it[ot++]);o(o.S+o.F*!q,"Symbol",{for:function(t){return u(W,t+="")?W[t]:W[t]=N(t)},keyFor:function(t){if(!$(t))throw TypeError(t+" is not a symbol!");for(var n in W)if(W[n]===t)return n},useSetter:function(){K=!0},useSimple:function(){K=!1}}),o(o.S+o.F*!q,"Object",{create:function(t,n){return void 0===n?O(t):X(O(t),n)},defineProperty:Y,defineProperties:X,getOwnPropertyDescriptor:Z,getOwnPropertyNames:tt,getOwnPropertySymbols:nt});var ut=a(function(){P.f(1)});o(o.S+o.F*ut,"Object",{getOwnPropertySymbols:function(t){return P.f(m(t))}}),T&&o(o.S+o.F*(!q||a(function(){var t=N();return"[null]"!=k([t])||"{}"!=k({a:t})||"{}"!=k(Object(t))})),"JSON",{stringify:function(t){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);if(r=n=e[1],(x(n)||void 0!==t)&&!$(t))return g(n)||(n=function(t,n){if("function"==typeof r&&(n=r.call(this,t,n)),!$(n))return n}),e[1]=n,k.apply(T,e)}}),N[R][D]||r(26)(N[R],D,N[R].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(132),o=r(159),a=r(2),s=r(78),l=r(8),u=r(5),c=r(3).ArrayBuffer,h=r(119),v=o.ArrayBuffer,p=o.DataView,f=i.ABV&&c.isView,d=v.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(c!==v),{ArrayBuffer:v}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return f&&f(t)||u(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new v(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(a(this),t);for(var r=a(this).byteLength,e=s(t,r),i=s(void 0===n?r:n,r),o=new(h(this,v))(l(i-e)),u=new p(this),c=new p(o),f=0;e<i;)c.setUint8(f++,u.getUint8(e++));return o}}),r(77)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(132).ABV,{DataView:r(159).DataView})},function(t,n,r){r(57)("Float32",4,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Float64",8,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Int16",2,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Int32",4,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Int8",1,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Uint16",2,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Uint32",4,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Uint8",1,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(57)("Uint8",1,function(e){return function(t,n,r){return e(this,t,n,r)}},!0)},function(t,n,r){"use strict";var e=r(172),i=r(80);r(121)("WeakSet",function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},{add:function(t){return e.def(i(this,"WeakSet"),t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(173),o=r(17),u=r(8),c=r(21),f=r(138);e(e.P,"Array",{flatMap:function(t){var n,r,e=o(this);return c(t),n=u(e.length),r=f(e,0),i(r,e,e,n,0,1,t,arguments[1]),r}}),r(67)("flatMap")},function(t,n,r){"use strict";var e=r(1),i=r(173),o=r(17),u=r(8),c=r(49),f=r(138);e(e.P,"Array",{flatten:function(){var t=arguments[0],n=o(this),r=u(n.length),e=f(n,0);return i(e,n,n,r,0,void 0===t?1:c(t)),e}}),r(67)("flatten")},function(t,n,r){"use strict";var e=r(1),i=r(120)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0)}}),r(67)("includes")},function(t,n,r){var e=r(1),i=r(150)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.G,{global:r(3)})},function(t,n,r){r(129)("Map")},function(t,n,r){r(130)("Map")},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(171)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{clamp:function(t,n,r){return Math.min(r,Math.max(n,t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{DEG_PER_RAD:Math.PI/180})},function(t,n,r){var e=r(1),i=180/Math.PI;e(e.S,"Math",{degrees:function(t){return t*i}})},function(t,n,r){var e=r(1),o=r(181),u=r(179);e(e.S,"Math",{fscale:function(t,n,r,e,i){return u(o(t,n,r,e,i))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=r>>>0;return(n>>>0)+(e>>>0)+((i&o|(i|o)&~(i+o>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=+t,e=+n,i=65535&r,o=65535&e,u=r>>16,c=e>>16,f=(u*o>>>0)+(i*o>>>16);return u*c+(f>>16)+((i*c>>>0)+(65535&f)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=r>>>0;return(n>>>0)-(e>>>0)-((~i&o|~(i^o)&i-o>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{RAD_PER_DEG:180/Math.PI})},function(t,n,r){var e=r(1),i=Math.PI/180;e(e.S,"Math",{radians:function(t){return t*i}})},function(t,n,r){var e=r(1);e(e.S,"Math",{scale:r(181)})},function(t,n,r){var e=r(1);e(e.S,"Math",{signbit:function(t){return(t=+t)!=t?t:0==t?1/t==1/0:0<t}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=+t,e=+n,i=65535&r,o=65535&e,u=r>>>16,c=e>>>16,f=(u*o>>>0)+(i*o>>>16);return u*c+(f>>>16)+((i*c>>>0)+(65535&f)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(21),u=r(11);r(10)&&e(e.P+r(126),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(21),u=r(11);r(10)&&e(e.P+r(126),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(186)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),f=r(187),a=r(33),s=r(31),l=r(139);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r,e=a(t),i=s.f,o=f(e),u={},c=0;o.length>c;)void 0!==(r=i(e,n=o[c++]))&&l(u,n,r);return u}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(53),u=r(32),c=r(31).f;r(10)&&e(e.P+r(126),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(53),u=r(32),c=r(31).f;r(10)&&e(e.P+r(126),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(186)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),o=r(3),u=r(46),i=r(150)(),c=r(7)("observable"),f=r(21),a=r(2),s=r(70),l=r(76),h=r(26),v=r(71),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},x=function(t,n){a(t),this._c=void 0,this._o=t,t=new m(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};x.prototype=l({},{unsubscribe:function(){b(this)}});var m=function(t){this._s=t};m.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var S=function(t){s(this,S,"Observable","_f")._f=f(t)};l(S.prototype,{subscribe:function(t){return new x(t,this._f)},forEach:function(e){var i=this;return new(u.Promise||o.Promise)(function(t,n){f(e);var r=i.subscribe({next:function(t){try{return e(t)}catch(t){n(t),r.unsubscribe()}},error:n,complete:t})})}}),l(S,{from:function(t){var n="function"==typeof this?this:S,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return i(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,e=new Array(n);t<n;)e[t]=arguments[t++];return new("function"==typeof this?this:S)(function(n){var r=!1;return i(function(){if(!r){for(var t=0;t<e.length;++t)if(n.next(e[t]),r)return;n.complete()}}),function(){r=!0}})}}),h(S.prototype,c,function(){return this}),e(e.G,{Observable:S}),r(77)("Observable")},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(3),u=r(119),c=r(191);e(e.P+e.R,"Promise",{finally:function(n){var r=u(this,i.Promise||o.Promise),t="function"==typeof n;return this.then(t?function(t){return c(r,n()).then(function(){return t})}:n,t?function(t){return c(r,n()).then(function(){throw t})}:n)}})},function(t,n,r){"use strict";var e=r(1),i=r(151),o=r(190);e(e.S,"Promise",{try:function(t){var n=i.f(this),r=o(t);return(r.e?n.reject:n.resolve)(r.v),n.promise}})},function(t,n,r){var e=r(56),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(56),o=r(2),u=e.key,c=e.map,f=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:u(arguments[2]),e=c(o(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var i=f.get(n);return i.delete(r),!!i.size||f.delete(n)}})},function(t,n,r){var o=r(199),u=r(167),e=r(56),i=r(2),c=r(32),f=e.keys,a=e.key,s=function(t,n){var r=f(t,n),e=c(t);if(null===e)return r;var i=s(e,n);return i.length?r.length?u(new o(r.concat(i))):i:r};e.exp({getMetadataKeys:function(t){return s(i(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(56),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(56),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(56),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(56),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(56),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(56),i=r(2),o=r(21),u=e.key,c=e.set;e.exp({metadata:function(r,e){return function(t,n){c(r,e,(void 0!==n?i:o)(t),u(n))}}})},function(t,n,r){r(129)("Set")},function(t,n,r){r(130)("Set")},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(171)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(131)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(51),o=r(8),u=r(124),c=r(115),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(146)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(193),o=r(133),u=/Version\/10\.\d+(\.\d+)?( Mobile\/\w+)? Safari\//.test(o);e(e.P+e.F*u,"String",{padEnd:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(193),o=r(133),u=/Version\/10\.\d+(\.\d+)?( Mobile\/\w+)? Safari\//.test(o);e(e.P+e.F*u,"String",{padStart:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(84)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(84)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(160)("asyncIterator")},function(t,n,r){r(160)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){r(129)("WeakMap")},function(t,n,r){r(130)("WeakMap")},function(t,n,r){r(129)("WeakSet")},function(t,n,r){r(130)("WeakSet")},function(t,n,r){for(var e=r(162),i=r(74),o=r(27),u=r(3),c=r(26),f=r(82),a=r(7),s=a("iterator"),l=a("toStringTag"),h=f.Array,v={CSSRuleList:!0,CSSStyleDeclaration:!1,CSSValueList:!1,ClientRectList:!1,DOMRectList:!1,DOMStringList:!1,DOMTokenList:!0,DataTransferItemList:!1,FileList:!1,HTMLAllCollection:!1,HTMLCollection:!1,HTMLFormElement:!1,HTMLSelectElement:!1,MediaList:!0,MimeTypeArray:!1,NamedNodeMap:!1,NodeList:!0,PaintRequestList:!1,Plugin:!1,PluginArray:!1,SVGLengthList:!1,SVGNumberList:!1,SVGPathSegList:!1,SVGPointList:!1,SVGStringList:!1,SVGTransformList:!1,SourceBufferList:!1,StyleSheetList:!0,TextTrackCueList:!1,TextTrackList:!1,TouchList:!1},p=i(v),d=0;d<p.length;d++){var y,g=p[d],b=v[g],x=u[g],m=x&&x.prototype;if(m&&(m[s]||c(m,s,h),m[l]||c(m,l,g),f[g]=h,b))for(y in e)m[y]||o(m,y,e[y],!0)}},function(t,n,r){var e=r(1),i=r(158);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(133),u=[].slice,c=/MSIE .\./.test(o),f=function(i){return function(t,n){var r=2<arguments.length,e=!!r&&u.call(arguments,2);return i(r?function(){("function"==typeof t?t:Function(t)).apply(this,e)}:t,n)}};i(i.G+i.B+i.F*c,{setTimeout:f(e.setTimeout),setInterval:f(e.setInterval)})},function(t,n,r){r(341),r(280),r(282),r(281),r(284),r(286),r(291),r(285),r(283),r(293),r(292),r(288),r(289),r(287),r(279),r(290),r(294),r(295),r(247),r(249),r(248),r(297),r(296),r(267),r(277),r(278),r(268),r(269),r(270),r(271),r(272),r(273),r(274),r(275),r(276),r(250),r(251),r(252),r(253),r(254),r(255),r(256),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(266),r(328),r(333),r(340),r(331),r(323),r(324),r(329),r(334),r(336),r(319),r(320),r(321),r(322),r(325),r(326),r(327),r(330),r(332),r(335),r(337),r(338),r(339),r(242),r(244),r(243),r(246),r(245),r(231),r(229),r(235),r(232),r(238),r(240),r(228),r(234),r(225),r(239),r(223),r(237),r(236),r(230),r(233),r(222),r(224),r(227),r(226),r(241),r(162),r(313),r(197),r(318),r(198),r(314),r(315),r(316),r(317),r(298),r(196),r(199),r(200),r(353),r(342),r(343),r(348),r(351),r(352),r(346),r(349),r(347),r(350),r(344),r(345),r(299),r(300),r(301),r(302),r(303),r(306),r(304),r(305),r(307),r(308),r(309),r(310),r(312),r(311),r(356),r(354),r(355),r(397),r(400),r(399),r(401),r(402),r(398),r(403),r(404),r(378),r(381),r(377),r(375),r(376),r(379),r(380),r(362),r(396),r(361),r(395),r(407),r(409),r(360),r(394),r(406),r(408),r(359),r(405),r(358),r(363),r(364),r(365),r(366),r(367),r(369),r(368),r(370),r(371),r(372),r(374),r(373),r(383),r(384),r(385),r(386),r(388),r(387),r(390),r(389),r(391),r(392),r(393),r(357),r(382),r(412),r(411),r(410),t.exports=r(46)},function(t,n){t.exports=function(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}}])</script><script src="/./main.a5fda8.js"></script><script>!function(){var e,t;e="/slider.27463f.js",t=document.createElement("script"),document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}()</script>



<!--  -->


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 50%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 50%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Server-tool</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Linux-storage</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Linux-svc</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux-base</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Linux-k8s</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Linux-db</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Windows-svc</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia-plus根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
        
          
  	  		<div class="aboutme-wrap" id="aboutme">运维技术面：<br>1. Linux 基础、脚本、服务部署、虚拟化<br>2. Linux 自动化、服务器硬件<br>3. Kubernetes、kube 服务部署 <br>4. Windows 客户端,服务端的便捷应用<br><br>微信： wnn_chat <br> <br>写博客的动力,源自于个人爱好!<br><br></div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>

  
  
<script type="text/javascript" src="/plugins/activate-power-mode/activate-power-mode.js"></script>
<script>
  POWERMODE.colorful = true; // make power mode colorful
  POWERMODE.shake = false; // turn off shake
  document.body.addEventListener('input', POWERMODE);
</script>

  
  <!-- <script async type="text/javascript" size="90" alpha="0.2" zIndex="0" src="/plugins/ribbon.js/ribbon.min.js"></script> -->
  
  
  
</body>

</html>
